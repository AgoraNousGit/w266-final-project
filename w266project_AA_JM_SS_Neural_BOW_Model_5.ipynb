{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIAR DETECTION GROUP PROJECT - Neural BOW Models  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONTENTS  \n",
    "\n",
    "Imports  \n",
    "Load ISOT data from appropriate pickle file  \n",
    "Load ISOT vocabulary from pickle file  (note: vocab contains both \"title\" and \"text\" words)  \n",
    "Train/Dev/Test split ISOT data  \n",
    "Load LIWC data for custom features  \n",
    "Load LIAR data (for evaluating models)  \n",
    "\n",
    "#### Neural BOW Models:\n",
    "- Model_1: Initial run replicating settings from Assignment 2, but with ISOT \"title\" data.  \n",
    "- Model_2: Use GloVe word embeddings rather than initializing embeddings with uniform random numbers.  \n",
    "- Model_3: Random word embeddings, but custom LIWC features concatenated into the model. \n",
    "- Model_4: Incorporate GloVe embeddings as well as LIWC features. Still training with ISOT \"title\" data. \n",
    "- Model_5: Train using LIAR/Politifact data, GloVe embeddings, but NO LIWC features; then predict on same plus ISOT \"title\" data also.  \n",
    "- Model_6: Train using LIAR/Politifact data, GloVe embeddings, and LIWC features; then predict on same plus ISOT \"title\" data also.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "from importlib import reload\n",
    "import os, sys, re, json, time, datetime, shutil\n",
    "import itertools, collections\n",
    "from functools import reduce\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.utils import shuffle\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "#assert(tf.__version__.startswith(\"1.8\"))\n",
    "\n",
    "import pickle\n",
    "import dill\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary, tf_embed_viz\n",
    "from w266_common import patched_numpy_io\n",
    "import timeit  #For timing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.10.1\n"
     ]
    }
   ],
   "source": [
    "print('TensorFlow version:', tf.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LIAR/Politifact data and vocabulary from pickle files  \n",
    "Load the COMBINED LIAR and Politifact dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23258 entries, 0 to 23257\n",
      "Data columns (total 6 columns):\n",
      "target            23258 non-null int64\n",
      "title             23258 non-null object\n",
      "title_tokcan      23258 non-null object\n",
      "title_POS         23258 non-null object\n",
      "binary_target     23258 non-null int64\n",
      "embedded_title    23258 non-null object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 19.1 MB\n"
     ]
    }
   ],
   "source": [
    "# Read LIAR/Politifact data from pickle file.\n",
    "all_data = pd.read_pickle('parsed_data/df_liarpolitifact_data_embed.pkl')  # \n",
    "\n",
    "all_data.info(memory_usage='deep', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>title</th>\n",
       "      <th>title_tokcan</th>\n",
       "      <th>title_POS</th>\n",
       "      <th>binary_target</th>\n",
       "      <th>embedded_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Says 31 percent of Texas physicians accept all...</td>\n",
       "      <td>[says, &lt;number&gt;, percent, of, texas, physician...</td>\n",
       "      <td>[V, $, N, P, ^, N, V, D, A, ^, N, ,, R, P, $, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.11797, 0.21126, 0.29075, -0.021211, 0.7819...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>''Both Democrats and Republicans are advocatin...</td>\n",
       "      <td>['', both, democrats, and, republicans, are, a...</td>\n",
       "      <td>[,, D, N, &amp;, N, V, V, P, D, N, P, N, N, V, P, ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[[0.0028594, 0.19457, -0.19449, -0.037583, 0.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>A Republican-led softening of firearms trainin...</td>\n",
       "      <td>[a, republican-led, softening, of, firearms, t...</td>\n",
       "      <td>[D, A, N, P, N, N, N, V, D, A, N, V, V, V, P, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.21705, 0.46515, -0.46757, 0.10082, 1.0135,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>The first tweet was sent from Austin.</td>\n",
       "      <td>[the, first, tweet, was, sent, from, austin, .]</td>\n",
       "      <td>[D, A, N, V, V, P, ^, ,]</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Georgia has the countrys second highest number...</td>\n",
       "      <td>[georgia, has, the, countrys, second, highest,...</td>\n",
       "      <td>[^, V, D, N, A, A, N, P, A, N, N, N, ,]</td>\n",
       "      <td>-1</td>\n",
       "      <td>[[-1.3427, 0.4592, 0.19281, 0.71305, -0.5934, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                              title  \\\n",
       "0       1  Says 31 percent of Texas physicians accept all...   \n",
       "1       2  ''Both Democrats and Republicans are advocatin...   \n",
       "2       0  A Republican-led softening of firearms trainin...   \n",
       "3       5              The first tweet was sent from Austin.   \n",
       "4       2  Georgia has the countrys second highest number...   \n",
       "\n",
       "                                        title_tokcan  \\\n",
       "0  [says, <number>, percent, of, texas, physician...   \n",
       "1  ['', both, democrats, and, republicans, are, a...   \n",
       "2  [a, republican-led, softening, of, firearms, t...   \n",
       "3    [the, first, tweet, was, sent, from, austin, .]   \n",
       "4  [georgia, has, the, countrys, second, highest,...   \n",
       "\n",
       "                                           title_POS  binary_target  \\\n",
       "0  [V, $, N, P, ^, N, V, D, A, ^, N, ,, R, P, $, ...              1   \n",
       "1  [,, D, N, &, N, V, V, P, D, N, P, N, N, V, P, ...             -1   \n",
       "2  [D, A, N, P, N, N, N, V, D, A, N, V, V, V, P, ...              1   \n",
       "3                           [D, A, N, V, V, P, ^, ,]              0   \n",
       "4            [^, V, D, N, A, A, N, P, A, N, N, N, ,]             -1   \n",
       "\n",
       "                                      embedded_title  \n",
       "0  [[0.11797, 0.21126, 0.29075, -0.021211, 0.7819...  \n",
       "1  [[0.0028594, 0.19457, -0.19449, -0.037583, 0.9...  \n",
       "2  [[0.21705, 0.46515, -0.46757, 0.10082, 1.0135,...  \n",
       "3  [[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -...  \n",
       "4  [[-1.3427, 0.4592, 0.19281, 0.71305, -0.5934, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Says 31 percent of Texas physicians accept all new Medicaid patients, down from 67 percent in 2000.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.title[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['says',\n",
       " '<number>',\n",
       " 'percent',\n",
       " 'of',\n",
       " 'texas',\n",
       " 'physicians',\n",
       " 'accept',\n",
       " 'all',\n",
       " 'new',\n",
       " 'medicaid',\n",
       " 'patients',\n",
       " ',',\n",
       " 'down',\n",
       " 'from',\n",
       " '<number>',\n",
       " 'percent',\n",
       " 'in',\n",
       " '<number>',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.title_tokcan[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target=1 (real): 8283\n",
      "target=0 (fake): 10199\n",
      "target=-1 (half true; DROP): 4776\n"
     ]
    }
   ],
   "source": [
    "print('target=1 (real):', len(all_data[all_data.binary_target == 1]))\n",
    "print('target=0 (fake):', len(all_data[all_data.binary_target == 0]))\n",
    "print('target=-1 (half true; DROP):', len(all_data[all_data.binary_target == -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read LIAR/Politifact (lp) vocab from pickle file.\n",
    "\n",
    "vocab = pd.read_pickle('parsed_data/vocab_lp.pkl')  # COMBINED LIAR and Politifact data (CMU) tokenized and POS tags added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14,460 words\n",
      "wordset:  ['<s>', '</s>', '<unk>', '.', 'the', ',', 'in', 'of', 'to', '<number>', 'a', '\"\"', 'and', '\"', 'says', 'for', '\"\"\"', 'that', 'is', 'on', 'has', 'have', 'percent', 'than', 'are', 'more', '$<number>', 'was', 'we', 'by']\n",
      "<w266_common.vocabulary2.Vocabulary2 object at 0x7fdd5e578c88>\n"
     ]
    }
   ],
   "source": [
    "print(\"{:,} words\".format(vocab.size))  # Note: this combines words from ISOT \"title\" AND \"text\" fields!\n",
    "print(\"wordset: \",vocab.ordered_words()[:30])\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LIAR and ISOT LIWC features from pickle file \n",
    "### (NOTE: will need to get new LIWC files to match vocab size of 14,460 words)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_liar = pd.read_pickle('parsed_data/liwc_liar2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>function</th>\n",
       "      <th>pronoun</th>\n",
       "      <th>ppron</th>\n",
       "      <th>i</th>\n",
       "      <th>we</th>\n",
       "      <th>you</th>\n",
       "      <th>shehe</th>\n",
       "      <th>they</th>\n",
       "      <th>ipron</th>\n",
       "      <th>article</th>\n",
       "      <th>...</th>\n",
       "      <th>money</th>\n",
       "      <th>relig</th>\n",
       "      <th>death</th>\n",
       "      <th>informal</th>\n",
       "      <th>swear</th>\n",
       "      <th>netspeak</th>\n",
       "      <th>assent</th>\n",
       "      <th>nonflu</th>\n",
       "      <th>filler</th>\n",
       "      <th>Unnamed: 74</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   function  pronoun  ppron  i  we  you  shehe  they  ipron  article  \\\n",
       "0         0        0      0  0   0    0      0     0      0        0   \n",
       "1         0        0      0  0   0    0      0     0      0        0   \n",
       "2         0        0      0  0   0    0      0     0      0        0   \n",
       "3         1        0      0  0   0    0      0     0      0        1   \n",
       "4         0        0      0  0   0    0      0     0      0        0   \n",
       "\n",
       "      ...       money  relig  death  informal  swear  netspeak  assent  \\\n",
       "0     ...           0      0      0         0      0         0       0   \n",
       "1     ...           0      0      0         0      0         0       0   \n",
       "2     ...           0      0      0         0      0         0       0   \n",
       "3     ...           0      0      0         0      0         0       0   \n",
       "4     ...           0      0      0         0      0         0       0   \n",
       "\n",
       "   nonflu  filler  Unnamed: 74  \n",
       "0       0       0            0  \n",
       "1       0       0            0  \n",
       "2       0       0            0  \n",
       "3       0       0            0  \n",
       "4       0       0            0  \n",
       "\n",
       "[5 rows x 74 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liwc_liar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(152036, 74)\n"
     ]
    }
   ],
   "source": [
    "print(liwc_liar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_isot = pd.read_pickle('parsed_data/liwc_isot2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>function</th>\n",
       "      <th>pronoun</th>\n",
       "      <th>ppron</th>\n",
       "      <th>i</th>\n",
       "      <th>we</th>\n",
       "      <th>you</th>\n",
       "      <th>shehe</th>\n",
       "      <th>they</th>\n",
       "      <th>ipron</th>\n",
       "      <th>article</th>\n",
       "      <th>...</th>\n",
       "      <th>money</th>\n",
       "      <th>relig</th>\n",
       "      <th>death</th>\n",
       "      <th>informal</th>\n",
       "      <th>swear</th>\n",
       "      <th>netspeak</th>\n",
       "      <th>assent</th>\n",
       "      <th>nonflu</th>\n",
       "      <th>filler</th>\n",
       "      <th>Unnamed: 74</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   function  pronoun  ppron  i  we  you  shehe  they  ipron  article  \\\n",
       "0         0        0      0  0   0    0      0     0      0        0   \n",
       "1         0        0      0  0   0    0      0     0      0        0   \n",
       "2         0        0      0  0   0    0      0     0      0        0   \n",
       "3         1        0      0  0   0    0      0     0      0        1   \n",
       "4         0        0      0  0   0    0      0     0      0        0   \n",
       "\n",
       "      ...       money  relig  death  informal  swear  netspeak  assent  \\\n",
       "0     ...           0      0      0         0      0         0       0   \n",
       "1     ...           0      0      0         0      0         0       0   \n",
       "2     ...           0      0      0         0      0         0       0   \n",
       "3     ...           0      0      0         0      0         0       0   \n",
       "4     ...           0      0      0         0      0         0       0   \n",
       "\n",
       "   nonflu  filler  Unnamed: 74  \n",
       "0       0       0            0  \n",
       "1       0       0            0  \n",
       "2       0       0            0  \n",
       "3       0       0            0  \n",
       "4       0       0            0  \n",
       "\n",
       "[5 rows x 74 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liwc_isot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "(152182, 74)\n"
     ]
    }
   ],
   "source": [
    "print(liwc_isot.values)\n",
    "print(liwc_isot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(liwc_isot.values[:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#liwc = tf.to_float(liwc_isot.values)\n",
    "liwc = liwc_isot.astype('float32')\n",
    "print(liwc.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(liwc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Dev / Test Split LIAR/Politifact data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, drop the data having binary_target = -1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18482, 6)\n",
      "target=1 (real): 8283\n",
      "target=0 (fake): 10199\n",
      "target=-1 (half true; DROP): 0\n"
     ]
    }
   ],
   "source": [
    "all_data = all_data[all_data.binary_target >= 0]\n",
    "print(all_data.shape)\n",
    "print('target=1 (real):', len(all_data[all_data.binary_target == 1]))\n",
    "print('target=0 (fake):', len(all_data[all_data.binary_target == 0]))\n",
    "print('target=-1 (half true; DROP):', len(all_data[all_data.binary_target == -1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split fractions add up to 1.0\n",
      "training set:  (12937, 6)\n",
      "dev set:  (2772, 6)\n",
      "test set:  (2773, 6)\n"
     ]
    }
   ],
   "source": [
    "#train/dev/train split\n",
    "#train_dev_split = 0.8\n",
    "\n",
    "train_fract = 0.70\n",
    "dev_fract = 0.15\n",
    "test_fract = 0.15\n",
    "\n",
    "if (train_fract+dev_fract+test_fract) == 1.0:\n",
    "    print('Split fractions add up to 1.0')\n",
    "else:\n",
    "    print('SPLIT FRACTIONS DO NOT ADD UP TO 1.0; PLEASE TRY AGAIN.............')\n",
    "\n",
    "#train_data = all_data[:int(len(all_data)*train_dev_split)].reset_index(drop=True)\n",
    "#dev_data = all_data[int(len(all_data)*train_dev_split):].reset_index(drop=True)\n",
    "\n",
    "train_set = all_data[ :int(len(all_data)*train_fract)].reset_index(drop=True)\n",
    "dev_set = all_data[int(len(all_data)*(train_fract)) : int(len(all_data)*(train_fract+dev_fract))].reset_index(drop=True)\n",
    "test_set = all_data[int(len(all_data)*(train_fract+dev_fract)) : ].reset_index(drop=True)\n",
    "\n",
    "print('training set: ',train_set.shape)\n",
    "print('dev set: ',dev_set.shape)\n",
    "print('test set: ',test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>title</th>\n",
       "      <th>title_tokcan</th>\n",
       "      <th>title_POS</th>\n",
       "      <th>binary_target</th>\n",
       "      <th>embedded_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Says 31 percent of Texas physicians accept all...</td>\n",
       "      <td>[says, &lt;number&gt;, percent, of, texas, physician...</td>\n",
       "      <td>[V, $, N, P, ^, N, V, D, A, ^, N, ,, R, P, $, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.11797, 0.21126, 0.29075, -0.021211, 0.7819...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>A Republican-led softening of firearms trainin...</td>\n",
       "      <td>[a, republican-led, softening, of, firearms, t...</td>\n",
       "      <td>[D, A, N, P, N, N, N, V, D, A, N, V, V, V, P, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.21705, 0.46515, -0.46757, 0.10082, 1.0135,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>The first tweet was sent from Austin.</td>\n",
       "      <td>[the, first, tweet, was, sent, from, austin, .]</td>\n",
       "      <td>[D, A, N, V, V, P, ^, ,]</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Florida has reduced its carbon emissions by 20...</td>\n",
       "      <td>[florida, has, reduced, its, carbon, emissions...</td>\n",
       "      <td>[^, V, V, L, N, N, P, $, N, P, $, ,]</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.52717, 0.16878, 0.16146, 0.93858, -0.6549...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Mt. Hood Community College is No. 1 on average...</td>\n",
       "      <td>[mt, ., hood, community, college, is, no, ., &lt;...</td>\n",
       "      <td>[^, ,, N, N, N, V, !, ,, $, P, A, &amp;, A, N, N, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.055441, 2.3025, 0.98466, -0.020482, -0.26...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                              title  \\\n",
       "0       1  Says 31 percent of Texas physicians accept all...   \n",
       "1       0  A Republican-led softening of firearms trainin...   \n",
       "2       5              The first tweet was sent from Austin.   \n",
       "3       1  Florida has reduced its carbon emissions by 20...   \n",
       "4       1  Mt. Hood Community College is No. 1 on average...   \n",
       "\n",
       "                                        title_tokcan  \\\n",
       "0  [says, <number>, percent, of, texas, physician...   \n",
       "1  [a, republican-led, softening, of, firearms, t...   \n",
       "2    [the, first, tweet, was, sent, from, austin, .]   \n",
       "3  [florida, has, reduced, its, carbon, emissions...   \n",
       "4  [mt, ., hood, community, college, is, no, ., <...   \n",
       "\n",
       "                                           title_POS  binary_target  \\\n",
       "0  [V, $, N, P, ^, N, V, D, A, ^, N, ,, R, P, $, ...              1   \n",
       "1  [D, A, N, P, N, N, N, V, D, A, N, V, V, V, P, ...              1   \n",
       "2                           [D, A, N, V, V, P, ^, ,]              0   \n",
       "3               [^, V, V, L, N, N, P, $, N, P, $, ,]              1   \n",
       "4  [^, ,, N, N, N, V, !, ,, $, P, A, &, A, N, N, ...              1   \n",
       "\n",
       "                                      embedded_title  \n",
       "0  [[0.11797, 0.21126, 0.29075, -0.021211, 0.7819...  \n",
       "1  [[0.21705, 0.46515, -0.46757, 0.10082, 1.0135,...  \n",
       "2  [[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -...  \n",
       "3  [[-0.52717, 0.16878, 0.16146, 0.93858, -0.6549...  \n",
       "4  [[-0.055441, 2.3025, 0.98466, -0.020482, -0.26...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>title</th>\n",
       "      <th>title_tokcan</th>\n",
       "      <th>title_POS</th>\n",
       "      <th>binary_target</th>\n",
       "      <th>embedded_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Under Obamacare, people who \"have a doctor the...</td>\n",
       "      <td>[\", under, obamacare, ,, people, who, \"\", have...</td>\n",
       "      <td>[,, P, ^, ,, N, O, ,, V, D, N, L, V, V, P, D, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.25769, 0.45629, -0.76974, -0.37679, 0.5927...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Even the employees union for the IRS -- the ve...</td>\n",
       "      <td>[even, the, employees, union, for, the, irs&lt;al...</td>\n",
       "      <td>[R, D, N, N, P, D, ^, ,, D, A, N, P, N, P, V, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.38336, -0.095871, 0.12229, -0.51625, 0.349...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Ch Guevara \"wrote extensively about the superi...</td>\n",
       "      <td>[\", ch, guevara, \"\", wrote, extensively, about...</td>\n",
       "      <td>[,, ^, ^, ,, V, R, P, D, N, P, A, N, P, N, P, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.25769, 0.45629, -0.76974, -0.37679, 0.5927...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The Chinese government provides their people n...</td>\n",
       "      <td>[the, chinese, government, provides, their, pe...</td>\n",
       "      <td>[D, A, N, V, D, N, D, N, P, D, N, ,]</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Agriculture has always required a significant ...</td>\n",
       "      <td>[agriculture, has, always, required, a, signif...</td>\n",
       "      <td>[N, V, R, V, D, A, N, P, R, ,]</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.43918, -0.47441, -0.78644, 0.18404, -0.11...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                              title  \\\n",
       "0       3  Under Obamacare, people who \"have a doctor the...   \n",
       "1       3  Even the employees union for the IRS -- the ve...   \n",
       "2       3  Ch Guevara \"wrote extensively about the superi...   \n",
       "3       3  The Chinese government provides their people n...   \n",
       "4       3  Agriculture has always required a significant ...   \n",
       "\n",
       "                                        title_tokcan  \\\n",
       "0  [\", under, obamacare, ,, people, who, \"\", have...   \n",
       "1  [even, the, employees, union, for, the, irs<al...   \n",
       "2  [\", ch, guevara, \"\", wrote, extensively, about...   \n",
       "3  [the, chinese, government, provides, their, pe...   \n",
       "4  [agriculture, has, always, required, a, signif...   \n",
       "\n",
       "                                           title_POS  binary_target  \\\n",
       "0  [,, P, ^, ,, N, O, ,, V, D, N, L, V, V, P, D, ...              0   \n",
       "1  [R, D, N, N, P, D, ^, ,, D, A, N, P, N, P, V, ...              0   \n",
       "2  [,, ^, ^, ,, V, R, P, D, N, P, A, N, P, N, P, ...              0   \n",
       "3               [D, A, N, V, D, N, D, N, P, D, N, ,]              0   \n",
       "4                     [N, V, R, V, D, A, N, P, R, ,]              0   \n",
       "\n",
       "                                      embedded_title  \n",
       "0  [[0.25769, 0.45629, -0.76974, -0.37679, 0.5927...  \n",
       "1  [[0.38336, -0.095871, 0.12229, -0.51625, 0.349...  \n",
       "2  [[0.25769, 0.45629, -0.76974, -0.37679, 0.5927...  \n",
       "3  [[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -...  \n",
       "4  [[-0.43918, -0.47441, -0.78644, 0.18404, -0.11...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out dev set\n",
    "#dev_set.to_csv('isot_dev_set.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>title</th>\n",
       "      <th>title_tokcan</th>\n",
       "      <th>title_POS</th>\n",
       "      <th>binary_target</th>\n",
       "      <th>embedded_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>We've lost over 5,000 Americans over there in ...</td>\n",
       "      <td>[we've, lost, over, &lt;number&gt;, americans, over,...</td>\n",
       "      <td>[L, V, P, $, N, P, R, P, ^, ,, P, ^, ,, &amp;, V, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Says he has nearly 200 delegates bound to supp...</td>\n",
       "      <td>[says, he, has, nearly, &lt;number&gt;, delegates, b...</td>\n",
       "      <td>[V, O, V, R, $, N, V, P, V, D, N, P, D, ^, ^, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.11797, 0.21126, 0.29075, -0.021211, 0.7819...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>The Fed created $15 trillion in the bailout pr...</td>\n",
       "      <td>[the, fed, created, $&lt;number&gt;, trillion, in, t...</td>\n",
       "      <td>[D, A, V, $, $, P, D, N, N, &amp;, $, $, V, A, ,]</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Says the Department of Defense changed its def...</td>\n",
       "      <td>[says, the, department, of, defense, changed, ...</td>\n",
       "      <td>[V, D, N, P, N, V, D, N, P, ^, &amp;, D, ^, V, O, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.11797, 0.21126, 0.29075, -0.021211, 0.7819...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Says Rick Perry wrote a letter \"supporting Hil...</td>\n",
       "      <td>[\", says, rick, perry, wrote, a, letter, \"\", s...</td>\n",
       "      <td>[,, V, ^, ^, V, D, N, ,, V, ^, ,, ,]</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.25769, 0.45629, -0.76974, -0.37679, 0.5927...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                              title  \\\n",
       "0       1  We've lost over 5,000 Americans over there in ...   \n",
       "1       3  Says he has nearly 200 delegates bound to supp...   \n",
       "2       3  The Fed created $15 trillion in the bailout pr...   \n",
       "3       3  Says the Department of Defense changed its def...   \n",
       "4       3  Says Rick Perry wrote a letter \"supporting Hil...   \n",
       "\n",
       "                                        title_tokcan  \\\n",
       "0  [we've, lost, over, <number>, americans, over,...   \n",
       "1  [says, he, has, nearly, <number>, delegates, b...   \n",
       "2  [the, fed, created, $<number>, trillion, in, t...   \n",
       "3  [says, the, department, of, defense, changed, ...   \n",
       "4  [\", says, rick, perry, wrote, a, letter, \"\", s...   \n",
       "\n",
       "                                           title_POS  binary_target  \\\n",
       "0  [L, V, P, $, N, P, R, P, ^, ,, P, ^, ,, &, V, ...              1   \n",
       "1  [V, O, V, R, $, N, V, P, V, D, N, P, D, ^, ^, ...              0   \n",
       "2      [D, A, V, $, $, P, D, N, N, &, $, $, V, A, ,]              0   \n",
       "3  [V, D, N, P, N, V, D, N, P, ^, &, D, ^, V, O, ...              0   \n",
       "4               [,, V, ^, ^, V, D, N, ,, V, ^, ,, ,]              0   \n",
       "\n",
       "                                      embedded_title  \n",
       "0  [[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...  \n",
       "1  [[0.11797, 0.21126, 0.29075, -0.021211, 0.7819...  \n",
       "2  [[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -...  \n",
       "3  [[0.11797, 0.21126, 0.29075, -0.021211, 0.7819...  \n",
       "4  [[0.25769, 0.45629, -0.76974, -0.37679, 0.5927...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select LIAR/Politifact features and (binary) labels for training model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape: (12937,)\n",
      "[list(['says', '<number>', 'percent', 'of', 'texas', 'physicians', 'accept', 'all', 'new', 'medicaid', 'patients', ',', 'down', 'from', '<number>', 'percent', 'in', '<number>', '.'])]\n",
      "train_labels shape: (12937,)\n",
      "[1 1 0 ... 0 0 0]\n",
      "\n",
      "dev_data shape: (2772,)\n",
      "[list(['\"', 'under', 'obamacare', ',', 'people', 'who', '\"\"', 'have', 'a', 'doctor', 'theyve', 'been', 'seeing', 'for', 'the', 'last', '<number>', 'or', '<number>', 'years', ',', 'they', 'wont', 'be', 'able', 'to', 'keep', 'going', 'to', 'that', 'doctor', '.', '\"\"\"'])]\n",
      "dev_labels shape: (2772,)\n",
      "[0 0 0 ... 1 1 1]\n",
      "\n",
      "test_data shape: (2773,)\n",
      "[list([\"we've\", 'lost', 'over', '<number>', 'americans', 'over', 'there', 'in', 'afghanistan', ',', 'in', 'iraq', ',', 'and', 'plus', 'the', 'civilians', 'killed', '.'])]\n",
      "test_labels shape: (2773,)\n",
      "[1 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels = train_set.title_tokcan.values, train_set.binary_target.values\n",
    "dev_data, dev_labels = dev_set.title_tokcan.values, dev_set.binary_target.values\n",
    "test_data, test_labels = test_set.title_tokcan.values, test_set.binary_target.values\n",
    "\n",
    "train_labels = train_labels.astype(int)\n",
    "dev_labels = dev_labels.astype(int)\n",
    "test_labels = test_labels.astype(int)\n",
    "\n",
    "#train_data.head()\n",
    "print('train_data shape:', train_data.shape)\n",
    "#print(train_data[0].shape)\n",
    "print(train_data[:1])\n",
    "print('train_labels shape:', train_labels.shape)\n",
    "print(train_labels)\n",
    "print()\n",
    "print('dev_data shape:', dev_data.shape)\n",
    "print(dev_data[:1])\n",
    "print('dev_labels shape:', dev_labels.shape)\n",
    "print(dev_labels)\n",
    "print()\n",
    "print('test_data shape:', test_data.shape)\n",
    "print(test_data[:1])\n",
    "print('test_labels shape:', test_labels.shape)\n",
    "print(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95th percentile: 36.0\n"
     ]
    }
   ],
   "source": [
    "# characterize length of documents in train_data\n",
    "\n",
    "lengths = [len(train_data[i]) for i in range(train_data.shape[0])]\n",
    "\n",
    "a = np.array(lengths)\n",
    "p = np.percentile(a, 95) # return 95th percentile\n",
    "print('95th percentile:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.1.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.1.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Bokeh for plotting.\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool\n",
    "bp.output_notebook()\n",
    "\n",
    "# Helper code for plotting histograms\n",
    "def plot_length_histogram(lengths, x_range=[0,100], bins=40, normed=True):\n",
    "    hist, bin_edges = np.histogram(a=lengths, bins=bins, normed=normed, range=x_range)\n",
    "    bin_centers = (bin_edges[1:] + bin_edges[:-1])/2\n",
    "    bin_widths =  (bin_edges[1:] - bin_edges[:-1])\n",
    "\n",
    "    hover = HoverTool(tooltips=[(\"bucket\", \"@x\"), (\"count\", \"@top\")], mode=\"vline\")\n",
    "    fig = bp.figure(plot_width=800, plot_height=400, tools=[hover])\n",
    "    fig.vbar(x=bin_centers, width=bin_widths, top=hist, hover_fill_color=\"firebrick\")\n",
    "    fig.y_range.start = 0\n",
    "    fig.x_range.start = 0\n",
    "    fig.xaxis.axis_label = \"Example length (number of tokens)\"\n",
    "    fig.yaxis.axis_label = \"Frequency\"\n",
    "    bp.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"3732e248-497f-401b-9494-61bb965df773\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"c0d91f7b-765d-48d1-be71-e1e209b7a57c\":{\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1012\",\"type\":\"LinearAxis\"}],\"left\":[{\"id\":\"1017\",\"type\":\"LinearAxis\"}],\"plot_height\":400,\"plot_width\":800,\"renderers\":[{\"id\":\"1012\",\"type\":\"LinearAxis\"},{\"id\":\"1016\",\"type\":\"Grid\"},{\"id\":\"1017\",\"type\":\"LinearAxis\"},{\"id\":\"1021\",\"type\":\"Grid\"},{\"id\":\"1028\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"1030\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"1022\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"1004\",\"type\":\"DataRange1d\"},\"x_scale\":{\"id\":\"1008\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"1006\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"1010\",\"type\":\"LinearScale\"}},\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"callback\":null,\"data\":{\"top\":{\"__ndarray__\":\"xeOSkMFSGD8VzEkHp1lKP5gFM+jbpH0/IcjI2xt7jz955BAdIm+oP8lCKtXjAqY/0NZDkD+IsD8R7oGssTCkP9Hqfg2Hbqk/K7AMM0m2mz90cT5buNihPxgMOSYxKZM/wprVnECXkj8MBU2paw2CP68KPQ1w8II/1CpuLBE+cj9F3ysmxtxxP40fjmQIeGA/3hATsORqXz+2nLf0cWdOP0yHgF5pSEU/JBMlo/ZEJD8kEyWj9kQkP4NCt7UrNxA/xeOSkMFSKD+DQre1KzcgP4NCt7UrNwA/AAAAAAAAAADF45KQwVIYPwAAAAAAAAAAg0K3tSs3ED8AAAAAAAAAAINCt7UrNxA/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=\",\"dtype\":\"float64\",\"shape\":[40]},\"width\":{\"__ndarray__\":\"AAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEA=\",\"dtype\":\"float64\",\"shape\":[40]},\"x\":{\"__ndarray__\":\"AAAAAAAA9D8AAAAAAAAOQAAAAAAAABlAAAAAAACAIUAAAAAAAIAmQAAAAAAAgCtAAAAAAABAMEAAAAAAAMAyQAAAAAAAQDVAAAAAAADAN0AAAAAAAEA6QAAAAAAAwDxAAAAAAABAP0AAAAAAAOBAQAAAAAAAIEJAAAAAAABgQ0AAAAAAAKBEQAAAAAAA4EVAAAAAAAAgR0AAAAAAAGBIQAAAAAAAoElAAAAAAADgSkAAAAAAACBMQAAAAAAAYE1AAAAAAACgTkAAAAAAAOBPQAAAAAAAkFBAAAAAAAAwUUAAAAAAANBRQAAAAAAAcFJAAAAAAAAQU0AAAAAAALBTQAAAAAAAUFRAAAAAAADwVEAAAAAAAJBVQAAAAAAAMFZAAAAAAADQVkAAAAAAAHBXQAAAAAAAEFhAAAAAAACwWEA=\",\"dtype\":\"float64\",\"shape\":[40]}},\"selected\":{\"id\":\"1036\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"1037\",\"type\":\"UnionRenderers\"}},\"id\":\"1024\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"callback\":null,\"mode\":\"vline\",\"renderers\":\"auto\",\"tooltips\":[[\"bucket\",\"@x\"],[\"count\",\"@top\"]]},\"id\":\"1002\",\"type\":\"HoverTool\"},{\"attributes\":{},\"id\":\"1032\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"1018\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1010\",\"type\":\"LinearScale\"},{\"attributes\":{\"source\":{\"id\":\"1024\",\"type\":\"ColumnDataSource\"}},\"id\":\"1029\",\"type\":\"CDSView\"},{\"attributes\":{\"axis_label\":\"Example length (number of tokens)\",\"formatter\":{\"id\":\"1032\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1013\",\"type\":\"BasicTicker\"}},\"id\":\"1012\",\"type\":\"LinearAxis\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1002\",\"type\":\"HoverTool\"}]},\"id\":\"1022\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"1013\",\"type\":\"BasicTicker\"},{\"attributes\":{\"callback\":null,\"start\":0},\"id\":\"1006\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"1037\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1034\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1018\",\"type\":\"BasicTicker\"}},\"id\":\"1021\",\"type\":\"Grid\"},{\"attributes\":{\"callback\":null,\"start\":0},\"id\":\"1004\",\"type\":\"DataRange1d\"},{\"attributes\":{\"fill_color\":{\"value\":\"#1f77b4\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"field\":\"width\"},\"x\":{\"field\":\"x\"}},\"id\":\"1025\",\"type\":\"VBar\"},{\"attributes\":{},\"id\":\"1008\",\"type\":\"LinearScale\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"field\":\"width\"},\"x\":{\"field\":\"x\"}},\"id\":\"1026\",\"type\":\"VBar\"},{\"attributes\":{\"plot\":null,\"text\":\"\"},\"id\":\"1030\",\"type\":\"Title\"},{\"attributes\":{\"axis_label\":\"Frequency\",\"formatter\":{\"id\":\"1034\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1018\",\"type\":\"BasicTicker\"}},\"id\":\"1017\",\"type\":\"LinearAxis\"},{\"attributes\":{\"plot\":{\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1013\",\"type\":\"BasicTicker\"}},\"id\":\"1016\",\"type\":\"Grid\"},{\"attributes\":{\"data_source\":{\"id\":\"1024\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"1025\",\"type\":\"VBar\"},\"hover_glyph\":{\"id\":\"1027\",\"type\":\"VBar\"},\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1026\",\"type\":\"VBar\"},\"selection_glyph\":null,\"view\":{\"id\":\"1029\",\"type\":\"CDSView\"}},\"id\":\"1028\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"1036\",\"type\":\"Selection\"},{\"attributes\":{\"fill_color\":{\"value\":\"firebrick\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"field\":\"width\"},\"x\":{\"field\":\"x\"}},\"id\":\"1027\",\"type\":\"VBar\"}],\"root_ids\":[\"1003\"]},\"title\":\"Bokeh Application\",\"version\":\"1.0.1\"}};\n",
       "  var render_items = [{\"docid\":\"c0d91f7b-765d-48d1-be71-e1e209b7a57c\",\"roots\":{\"1003\":\"3732e248-497f-401b-9494-61bb965df773\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1003"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_length_histogram(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ISOT data to evaluate various models below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44898 entries, 0 to 44897\n",
      "Data columns (total 9 columns):\n",
      "title           44898 non-null object\n",
      "text            44898 non-null object\n",
      "subject         44898 non-null object\n",
      "date            44898 non-null object\n",
      "target          44898 non-null object\n",
      "title_tokcan    44898 non-null object\n",
      "title_POS       44898 non-null object\n",
      "text_tokcan     44898 non-null object\n",
      "text_POS        44898 non-null object\n",
      "dtypes: object(9)\n",
      "memory usage: 527.8 MB\n"
     ]
    }
   ],
   "source": [
    "# Read ISOT data from pickle file.\n",
    "isot_data = pd.read_pickle('parsed_data/df_alldata2.pkl')  # ISOT data (CMU) tokenized and POS tags added\n",
    "\n",
    "isot_data.info(memory_usage='deep', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>target</th>\n",
       "      <th>title_tokcan</th>\n",
       "      <th>title_POS</th>\n",
       "      <th>text_tokcan</th>\n",
       "      <th>text_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BRAINIAC Gets Rejected After Trying To Buy BMW...</td>\n",
       "      <td>Does anyone else out there see a future BMW ca...</td>\n",
       "      <td>Government News</td>\n",
       "      <td>Mar 20, 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>[brainiac&lt;allcaps&gt;, gets, rejected, after, try...</td>\n",
       "      <td>[N, V, V, P, V, P, V, ^, P, ^, ^, ,, O, V, A, ...</td>\n",
       "      <td>[does, anyone, else, out, there, see, a, futur...</td>\n",
       "      <td>[V, N, R, P, R, V, D, A, ^, N, N, P, D, N, ,, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Windows 10 is Stealing Your Bandwidth (You Mig...</td>\n",
       "      <td>21st Century Wire says We ve heard a lot of no...</td>\n",
       "      <td>US_News</td>\n",
       "      <td>April 7, 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>[windows, &lt;number&gt;, is, stealing, your, bandwi...</td>\n",
       "      <td>[^, $, V, V, D, N, ,, O, V, V, P, V, O, ,]</td>\n",
       "      <td>[&lt;number&gt;st, century, wire, says, we, ve, hear...</td>\n",
       "      <td>[A, N, ^, V, O, V, V, D, N, P, R, R, A, N, P, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>STUNNING STORY The Media And Democrats Hid Fro...</td>\n",
       "      <td>In an email sent on April 15, 2011, our upstan...</td>\n",
       "      <td>left-news</td>\n",
       "      <td>Mar 2, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>[stunning&lt;allcaps&gt;, story&lt;allcaps&gt;, the, media...</td>\n",
       "      <td>[A, N, D, N, &amp;, N, V, P, ^, ,, R, Z, ^, ^, N, ...</td>\n",
       "      <td>[in, an, email, sent, on, april, &lt;number&gt;, ,, ...</td>\n",
       "      <td>[P, D, N, V, P, ^, $, ,, $, ,, D, A, N, A, ^, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>North Korea's Kim Jong Un fetes nuclear scient...</td>\n",
       "      <td>SEOUL (Reuters) - North Korean leader Kim Jong...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>September 10, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>[north, korea's, kim, jong, un, fetes, nuclear...</td>\n",
       "      <td>[^, Z, ^, ^, ^, V, A, N, ,, V, N, N]</td>\n",
       "      <td>[seoul&lt;allcaps&gt;, (, reuters, ), -, north, kore...</td>\n",
       "      <td>[^, ,, ^, ,, ,, ^, ^, N, ^, ^, ^, V, D, A, N, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>White House developing comprehensive biosecuri...</td>\n",
       "      <td>ASPEN, Colorado (Reuters) - The Trump administ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>July 20, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>[white, house, developing, comprehensive, bios...</td>\n",
       "      <td>[A, N, V, A, N, N, ,, A]</td>\n",
       "      <td>[aspen&lt;allcaps&gt;, ,, colorado, (, reuters, ), -...</td>\n",
       "      <td>[^, ,, ^, ,, ^, ,, ,, D, ^, N, V, V, D, A, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LOL! GEORGE LOPEZ Booed Off Stage At Children’...</td>\n",
       "      <td>George Lopez was hired to be the emcee for the...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Oct 14, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>[lol&lt;allcaps&gt;, !, george&lt;allcaps&gt;, lopez&lt;allca...</td>\n",
       "      <td>[!, ,, ^, ^, V, P, N, P, ^, ^, N, P, N, V, D, ...</td>\n",
       "      <td>[george, lopez, was, hired, to, be, the, emcee...</td>\n",
       "      <td>[^, ^, V, V, P, V, D, N, P, D, ^, G, N, N, G, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HILLARY CLINTON CRONYISM VIOLATES FEDERAL RULE...</td>\n",
       "      <td>Former Secretary of State Hillary Clinton soug...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Oct 6, 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>[hillary&lt;allcaps&gt;, clinton&lt;allcaps&gt;, cronyism&lt;...</td>\n",
       "      <td>[^, ^, N, V, A, N, ,, Z, ,, A, N, ,, V, N, P, ...</td>\n",
       "      <td>[former, secretary, of, state, hillary, clinto...</td>\n",
       "      <td>[A, N, P, ^, ^, ^, V, P, V, ^, &amp;, ^, ^, N, N, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Republican Senator Alexander to consult on bip...</td>\n",
       "      <td>WASHINGTON (Reuters) - U.S. Republican Senator...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>September 26, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>[republican, senator, alexander, to, consult, ...</td>\n",
       "      <td>[A, N, ^, P, V, P, A, N, N]</td>\n",
       "      <td>[washington&lt;allcaps&gt;, (, reuters, ), -, u.s., ...</td>\n",
       "      <td>[^, ,, ^, ,, ,, ^, ^, ^, ^, ^, V, ^, P, O, V, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kellyanne Conway Announces Trump’s HUGE ‘Than...</td>\n",
       "      <td>Kellyanne Conway accidentally announced exactl...</td>\n",
       "      <td>News</td>\n",
       "      <td>January 9, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>[kellyanne, conway, announces, trump’s, huge&lt;a...</td>\n",
       "      <td>[^, ^, V, Z, A, ,, V, O, ,, N, P, ^, ,, &amp;, L, ...</td>\n",
       "      <td>[kellyanne, conway, accidentally, announced, e...</td>\n",
       "      <td>[^, ^, R, V, R, R, ^, ^, V, P, V, ^, ^, P, D, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Zimbabwe's army seizes power, Mugabe confined ...</td>\n",
       "      <td>HARARE (Reuters) - Zimbabwe s military seized ...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>November 15, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>[\", zimbabwe's, army, seizes, power, ,, mugabe...</td>\n",
       "      <td>[,, Z, N, N, N, ,, ^, V, &amp;, ,, A, ,]</td>\n",
       "      <td>[harare&lt;allcaps&gt;, (, reuters, ), -, zimbabwe, ...</td>\n",
       "      <td>[^, ,, ^, ,, ,, ^, G, A, A, N, P, ^, V, O, V, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  BRAINIAC Gets Rejected After Trying To Buy BMW...   \n",
       "1  Windows 10 is Stealing Your Bandwidth (You Mig...   \n",
       "2  STUNNING STORY The Media And Democrats Hid Fro...   \n",
       "3  North Korea's Kim Jong Un fetes nuclear scient...   \n",
       "4  White House developing comprehensive biosecuri...   \n",
       "5  LOL! GEORGE LOPEZ Booed Off Stage At Children’...   \n",
       "6  HILLARY CLINTON CRONYISM VIOLATES FEDERAL RULE...   \n",
       "7  Republican Senator Alexander to consult on bip...   \n",
       "8   Kellyanne Conway Announces Trump’s HUGE ‘Than...   \n",
       "9  Zimbabwe's army seizes power, Mugabe confined ...   \n",
       "\n",
       "                                                text          subject  \\\n",
       "0  Does anyone else out there see a future BMW ca...  Government News   \n",
       "1  21st Century Wire says We ve heard a lot of no...          US_News   \n",
       "2  In an email sent on April 15, 2011, our upstan...        left-news   \n",
       "3  SEOUL (Reuters) - North Korean leader Kim Jong...        worldnews   \n",
       "4  ASPEN, Colorado (Reuters) - The Trump administ...     politicsNews   \n",
       "5  George Lopez was hired to be the emcee for the...         politics   \n",
       "6  Former Secretary of State Hillary Clinton soug...         politics   \n",
       "7  WASHINGTON (Reuters) - U.S. Republican Senator...     politicsNews   \n",
       "8  Kellyanne Conway accidentally announced exactl...             News   \n",
       "9  HARARE (Reuters) - Zimbabwe s military seized ...        worldnews   \n",
       "\n",
       "                  date target  \\\n",
       "0         Mar 20, 2016      0   \n",
       "1        April 7, 2016      0   \n",
       "2          Mar 2, 2017      0   \n",
       "3  September 10, 2017       1   \n",
       "4       July 20, 2017       1   \n",
       "5         Oct 14, 2017      0   \n",
       "6          Oct 6, 2016      0   \n",
       "7  September 26, 2017       1   \n",
       "8      January 9, 2017      0   \n",
       "9   November 15, 2017       1   \n",
       "\n",
       "                                        title_tokcan  \\\n",
       "0  [brainiac<allcaps>, gets, rejected, after, try...   \n",
       "1  [windows, <number>, is, stealing, your, bandwi...   \n",
       "2  [stunning<allcaps>, story<allcaps>, the, media...   \n",
       "3  [north, korea's, kim, jong, un, fetes, nuclear...   \n",
       "4  [white, house, developing, comprehensive, bios...   \n",
       "5  [lol<allcaps>, !, george<allcaps>, lopez<allca...   \n",
       "6  [hillary<allcaps>, clinton<allcaps>, cronyism<...   \n",
       "7  [republican, senator, alexander, to, consult, ...   \n",
       "8  [kellyanne, conway, announces, trump’s, huge<a...   \n",
       "9  [\", zimbabwe's, army, seizes, power, ,, mugabe...   \n",
       "\n",
       "                                           title_POS  \\\n",
       "0  [N, V, V, P, V, P, V, ^, P, ^, ^, ,, O, V, A, ...   \n",
       "1         [^, $, V, V, D, N, ,, O, V, V, P, V, O, ,]   \n",
       "2  [A, N, D, N, &, N, V, P, ^, ,, R, Z, ^, ^, N, ...   \n",
       "3               [^, Z, ^, ^, ^, V, A, N, ,, V, N, N]   \n",
       "4                           [A, N, V, A, N, N, ,, A]   \n",
       "5  [!, ,, ^, ^, V, P, N, P, ^, ^, N, P, N, V, D, ...   \n",
       "6  [^, ^, N, V, A, N, ,, Z, ,, A, N, ,, V, N, P, ...   \n",
       "7                        [A, N, ^, P, V, P, A, N, N]   \n",
       "8  [^, ^, V, Z, A, ,, V, O, ,, N, P, ^, ,, &, L, ...   \n",
       "9               [,, Z, N, N, N, ,, ^, V, &, ,, A, ,]   \n",
       "\n",
       "                                         text_tokcan  \\\n",
       "0  [does, anyone, else, out, there, see, a, futur...   \n",
       "1  [<number>st, century, wire, says, we, ve, hear...   \n",
       "2  [in, an, email, sent, on, april, <number>, ,, ...   \n",
       "3  [seoul<allcaps>, (, reuters, ), -, north, kore...   \n",
       "4  [aspen<allcaps>, ,, colorado, (, reuters, ), -...   \n",
       "5  [george, lopez, was, hired, to, be, the, emcee...   \n",
       "6  [former, secretary, of, state, hillary, clinto...   \n",
       "7  [washington<allcaps>, (, reuters, ), -, u.s., ...   \n",
       "8  [kellyanne, conway, accidentally, announced, e...   \n",
       "9  [harare<allcaps>, (, reuters, ), -, zimbabwe, ...   \n",
       "\n",
       "                                            text_POS  \n",
       "0  [V, N, R, P, R, V, D, A, ^, N, N, P, D, N, ,, ...  \n",
       "1  [A, N, ^, V, O, V, V, D, N, P, R, R, A, N, P, ...  \n",
       "2  [P, D, N, V, P, ^, $, ,, $, ,, D, A, N, A, ^, ...  \n",
       "3  [^, ,, ^, ,, ,, ^, ^, N, ^, ^, ^, V, D, A, N, ...  \n",
       "4  [^, ,, ^, ,, ^, ,, ,, D, ^, N, V, V, D, A, A, ...  \n",
       "5  [^, ^, V, V, P, V, D, N, P, D, ^, G, N, N, G, ...  \n",
       "6  [A, N, P, ^, ^, ^, V, P, V, ^, &, ^, ^, N, N, ...  \n",
       "7  [^, ,, ^, ,, ,, ^, ^, ^, ^, ^, V, ^, P, O, V, ...  \n",
       "8  [^, ^, R, V, R, R, ^, ^, V, P, V, ^, ^, P, D, ...  \n",
       "9  [^, ,, ^, ,, ,, ^, G, A, A, N, P, ^, V, O, V, ...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isot_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isot titles: [list(['brainiac<allcaps>', 'gets', 'rejected', 'after', 'trying', 'to', 'buy', 'bmw<allcaps>', 'with', 'ebt<allcaps>', 'card', '…', 'what', 'happens', 'next', 'is', 'hysterical<allcaps>', '!'])\n",
      " list(['windows', '<number>', 'is', 'stealing', 'your', 'bandwidth', '(', 'you', 'might', 'want', 'to', 'delete', 'it', ')'])\n",
      " list(['stunning<allcaps>', 'story<allcaps>', 'the', 'media', 'and', 'democrats', 'hid', 'from', 'public', ':', 'how', 'obama<allcaps>’s', 'ag<allcaps>', 'eric', 'holder', 'used', 'taxpayer<allcaps>', 'dollars', 'to', 'organize', 'street', 'mobs', 'against', 'george', 'zimmerman', ',', 'take', 'down', 'police', 'chief'])\n",
      " ...\n",
      " list(['(', 'video<allcaps>', ')', 'the<allcaps>', 'great<allcaps>', 'divider<allcaps>', ':', 'obama<allcaps>', 'pulls<allcaps>', 'out<allcaps>', 'the<allcaps>', 'straw<allcaps>', 'man<allcaps>', 'argument<allcaps>', 'at<allcaps>', 'the<allcaps>', 'poverty<allcaps>', 'summit<allcaps>'])\n",
      " list(['seasons<allcaps>', 'beatings<allcaps>', '!', '<number>-yr', 'old', 'shot<allcaps>', '…', 'mall', 'brawls', 'spills', 'outside', '…', 'topless', 'feminist', 'destroys', 'candy', 'store', '…', 'worst', 'of', '#blackfriday', 'videos<allcaps>'])\n",
      " list(['after', 'u.s.', 'visit', ',', 'south', \"sudan's\", 'kiir', 'orders', 'unhindered', 'aid', 'access'])]\n",
      "isot labels: [0 0 0 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "isot_title_tokans = isot_data.title_tokcan.values\n",
    "isot_labels = isot_data.target.values.astype(int)\n",
    "\n",
    "print('isot titles:', isot_title_tokans)\n",
    "print('isot labels:', isot_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural BOW Model 5: LIAR/Politifact data WITH GloVe embeddings but NO LIWC features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Include reference functions for viewing convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May need this info (from utils.py)\n",
    "'''\n",
    "def build_vocab(corpus, V=10000, **kw):\n",
    "    from . import vocabulary\n",
    "    if isinstance(corpus, list):\n",
    "        token_feed = (canonicalize_word(w) for w in corpus)\n",
    "        vocab = vocabulary.Vocabulary(token_feed, size=V, **kw)\n",
    "    else:\n",
    "        token_feed = (canonicalize_word(w) for w in corpus.words())\n",
    "        vocab = vocabulary.Vocabulary(token_feed, size=V, **kw)\n",
    "\n",
    "    print(\"Vocabulary: {:,} types\".format(vocab.size))\n",
    "    return vocab\n",
    "\n",
    "# Window and batch functions\n",
    "def pad_np_array(example_ids, max_len=250, pad_id=0):\n",
    "    \"\"\"Pad a list of lists of ids into a rectangular NumPy array.\n",
    "\n",
    "    Longer sequences will be truncated to max_len ids, while shorter ones will\n",
    "    be padded with pad_id.\n",
    "\n",
    "    Args:\n",
    "        example_ids: list(list(int)), sequence of ids for each example\n",
    "        max_len: maximum sequence length\n",
    "        pad_id: id to pad shorter sequences with\n",
    "\n",
    "    Returns: (x, ns)\n",
    "        x: [num_examples, max_len] NumPy array of integer ids\n",
    "        ns: [num_examples] NumPy array of sequence lengths (<= max_len)\n",
    "    \"\"\"\n",
    "    arr = np.full([len(example_ids), max_len], pad_id, dtype=np.int32)\n",
    "    ns = np.zeros([len(example_ids)], dtype=np.int32)\n",
    "    for i, ids in enumerate(example_ids):\n",
    "        cpy_len = min(len(ids), max_len)\n",
    "        arr[i,:cpy_len] = ids[:cpy_len]\n",
    "        ns[i] = cpy_len\n",
    "    return arr, ns\n",
    "\n",
    "def id_lists_to_sparse_bow(id_lists, vocab_size):\n",
    "    \"\"\"Convert a list-of-lists-of-ids to a sparse bag-of-words matrix.\n",
    "\n",
    "    Args:\n",
    "        id_lists: (list(list(int))) list of lists of word ids\n",
    "        vocab_size: (int) vocab size; must be greater than the largest word id\n",
    "            in id_lists.\n",
    "\n",
    "    Returns:\n",
    "        (scipy.sparse.csr_matrix) where each row is a sparse vector of word\n",
    "        counts for the corresponding example.\n",
    "    \"\"\"\n",
    "    from scipy import sparse\n",
    "    ii = []  # row indices (example ids)\n",
    "    jj = []  # column indices (token ids)\n",
    "    for row_id, ids in enumerate(id_lists):\n",
    "        ii.extend([row_id]*len(ids))\n",
    "        jj.extend(ids)\n",
    "    x = sparse.csr_matrix((np.ones_like(ii), (ii, jj)),\n",
    "                          shape=[len(id_lists), vocab_size])\n",
    "    return x\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are functions that were in the \"SSTDataset\" class in sst.py from A2\n",
    "'''\n",
    "def get_filtered_split(split='train', df_idxs=None, root_only=False):\n",
    "    if not hasattr(split):\n",
    "        raise ValueError(\"Invalid split name '%s'\" % name)\n",
    "    df = getattr(split)\n",
    "    if df_idxs is not None:\n",
    "        df = df.loc[df_idxs]\n",
    "    #if root_only:          # Should not need in Final Project.\n",
    "        #df = df[df.is_root]\n",
    "    return df\n",
    "\n",
    "def as_padded_array(split='train', max_len=40, pad_id=0,\n",
    "                    root_only=False, df_idxs=None):\n",
    "    \"\"\"Return the dataset as a (padded) NumPy array.\n",
    "    Longer sequences will be truncated to max_len ids, while shorter ones\n",
    "    will be padded with pad_id.\n",
    "    Args:\n",
    "      split: 'train' or 'test'\n",
    "      max_len: maximum sequence length\n",
    "      pad_id: id to pad shorter sequences with\n",
    "      root_only: if true, will only export root phrases\n",
    "      df_idxs: (optional) custom list of indices to export\n",
    "    Returns: (x, ns, y)\n",
    "      x: [num_examples, max_len] NumPy array of integer ids\n",
    "      ns: [num_examples] NumPy array of sequence lengths (<= max_len)\n",
    "      y: [num_examples] NumPy array of target ids\n",
    "    \"\"\"\n",
    "    df = get_filtered_split(split, df_idxs, root_only)\n",
    "    x, ns = utils.pad_np_array(df.ids, max_len=max_len, pad_id=pad_id)\n",
    "    return x, ns, np.array(df.label, dtype=np.int32)\n",
    "\n",
    "def as_sparse_bow(split='train', root_only=False, df_idxs=None):\n",
    "    from scipy import sparse\n",
    "    df = get_filtered_split(split, df_idxs, root_only)\n",
    "    x = utils.id_lists_to_sparse_bow(df['ids'], self.vocab.size)\n",
    "    y = np.array(df.label, dtype=np.int32)\n",
    "    return x, y\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct train, dev, test data arrays  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14, 9, 22, 7, 73, 4446, 1590, 79, 60, 306, 1257, 5, 145, 34, 9, 22, 6, 9, 3], [10, 4447, 7509, 7, 1070, 1292, 668, 766, 17, 5483, 926, 50, 51, 489, 8, 896, 446, 35, 10, 32, 1552, 3], [4, 108, 7510, 27, 545, 34, 390, 3], [124, 20, 639, 110, 1214, 1648, 29, 9, 22, 90, 9, 3], [7512, 3, 2642, 701, 253, 18, 82, 3, 9, 19, 132, 12, 291, 2643, 102, 12, 328, 3]]\n",
      "\n",
      "[[  14    9   22    7   73 4446 1590   79   60  306 1257    5  145   34\n",
      "     9   22    6    9    3    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  10 4447 7509    7 1070 1292  668  766   17 5483  926   50   51  489\n",
      "     8  896  446   35   10   32 1552    3    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "\n",
      "[19 22]\n",
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "## Training data\n",
    "\n",
    "all_train_ids=[]\n",
    "for i, tokens in enumerate(train_data):  # here, tokens are the words in a single sentence\n",
    "    sent_ids = vocab.words_to_ids(tokens)\n",
    "    all_train_ids.append(sent_ids)\n",
    "print(all_train_ids[:5])\n",
    "\n",
    "max_len = 40   # Retain this setting, since it fits the ISOT \"title\" length distribution quite well.\n",
    "train_x, train_ns = utils.pad_np_array(all_train_ids, max_len=max_len)\n",
    "print()\n",
    "print(train_x[:2])\n",
    "print()\n",
    "print(train_ns[:2])\n",
    "\n",
    "train_y = train_labels\n",
    "print(train_y[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13, 77, 125, 5, 44, 68, 11, 21, 10, 2023, 1057, 65, 1651, 15, 4, 96, 9, 67, 9, 42, 5, 66, 1354, 51, 638, 8, 351, 160, 8, 17, 2023, 3, 16], [127, 4, 267, 463, 15, 4, 691, 118, 4, 322, 44, 6, 754, 7, 4556, 59, 97, 118, 24, 4535, 8, 51, 582, 81, 34, 77, 59, 97, 3], [13, 9623, 4827, 11, 912, 5410, 69, 4, 6248, 7, 282, 6167, 62, 44, 7, 1213, 6398, 3, 16], [4, 1009, 95, 1251, 57, 44, 82, 632, 8, 4, 1208, 3], [1456, 20, 1031, 680, 10, 1104, 965, 34, 3793, 3]]\n",
      "\n",
      "[[  13   77  125    5   44   68   11   21   10 2023 1057   65 1651   15\n",
      "     4   96    9   67    9   42    5   66 1354   51  638    8  351  160\n",
      "     8   17 2023    3   16    0    0    0    0    0    0    0]\n",
      " [ 127    4  267  463   15    4  691  118    4  322   44    6  754    7\n",
      "  4556   59   97  118   24 4535    8   51  582   81   34   77   59   97\n",
      "     3    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "\n",
      "[33 29]\n",
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "## Dev data\n",
    "\n",
    "all_dev_ids=[]\n",
    "for i, tokens in enumerate(dev_data):  # here, tokens are the words in a single sentence\n",
    "    sent_ids = vocab.words_to_ids(tokens)\n",
    "    all_dev_ids.append(sent_ids)\n",
    "print(all_dev_ids[:5])\n",
    "\n",
    "max_len = 40   # Retain this setting, since it fits the ISOT \"title\" length distribution quite well.\n",
    "dev_x, dev_ns = utils.pad_np_array(all_dev_ids, max_len=max_len)\n",
    "print()\n",
    "print(dev_x[:2])\n",
    "print()\n",
    "print(dev_ns[:2])\n",
    "\n",
    "dev_y = dev_labels\n",
    "print(dev_y[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[532, 223, 62, 9, 116, 62, 86, 6, 663, 5, 6, 290, 5, 12, 1944, 4, 4932, 477, 3], [14, 45, 20, 149, 9, 2578, 6503, 8, 214, 56, 3355, 48, 4, 120, 138, 1258, 3], [4, 3139, 192, 26, 217, 6, 4, 1303, 1386, 12, 26, 217, 264, 843, 3], [14, 4, 257, 7, 501, 713, 110, 11911, 7, 11912, 12, 4, 4185, 375, 33, 198, 224, 1226, 139, 51, 11913, 1671, 35, 4, 836, 3], [13, 14, 181, 385, 912, 10, 1392, 11, 1101, 6455, 3, 16]]\n",
      "\n",
      "[[ 532  223   62    9  116   62   86    6  663    5    6  290    5   12\n",
      "  1944    4 4932  477    3    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  14   45   20  149    9 2578 6503    8  214   56 3355   48    4  120\n",
      "   138 1258    3    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "\n",
      "[19 17]\n",
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "## Test data\n",
    "\n",
    "all_test_ids=[]\n",
    "for i, tokens in enumerate(test_data):  # here, tokens are the words in a single sentence\n",
    "    sent_ids = vocab.words_to_ids(tokens)\n",
    "    all_test_ids.append(sent_ids)\n",
    "print(all_test_ids[:5])\n",
    "\n",
    "max_len = 40   # Retain this setting, since it fits the ISOT \"title\" length distribution quite well.\n",
    "test_x, test_ns = utils.pad_np_array(all_test_ids, max_len=max_len)\n",
    "print()\n",
    "print(test_x[:2])\n",
    "print()\n",
    "print(test_ns[:2])\n",
    "\n",
    "test_y = test_labels\n",
    "print(test_y[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples:\n",
      " [[  14    9   22    7   73 4446 1590   79   60  306 1257    5  145   34\n",
      "     9   22    6    9    3    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  10 4447 7509    7 1070 1292  668  766   17 5483  926   50   51  489\n",
      "     8  896  446   35   10   32 1552    3    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   4  108 7510   27  545   34  390    3    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "Original sequence lengths:  [19 22  8]\n",
      "Target labels:  [1 1 0]\n",
      "\n",
      "Padded:\n",
      " says <number> percent of texas physicians accept all new medicaid patients , down from <number> percent in <number> . <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>\n",
      "Un-padded:\n",
      " says <number> percent of texas physicians accept all new medicaid patients , down from <number> percent in <number> .\n"
     ]
    }
   ],
   "source": [
    "print(\"Examples:\\n\", train_x[:3])\n",
    "print(\"Original sequence lengths: \", train_ns[:3])\n",
    "print(\"Target labels: \", train_y[:3])\n",
    "print(\"\")\n",
    "print(\"Padded:\\n\", \" \".join(vocab.ids_to_words(train_x[0])))\n",
    "print(\"Un-padded:\\n\", \" \".join(vocab.ids_to_words(train_x[0,:train_ns[0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use tf.Estimator API along with nbow_models_x.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things to consider:  \n",
    "- Start w/ 2 epochs (20 was original)       \n",
    "- Consider use of dropouts in fully-connected layers     \n",
    "-  Use embed_dim = 300 rather than 50??   \n",
    "- xx  \n",
    "...  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 14460\n"
     ]
    }
   ],
   "source": [
    "print('vocab size:', vocab.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (14,460 words) written to '/tmp/tf_nbow_20181203-0359/metadata.tsv'\n",
      "Projector config written to /tmp/tf_nbow_20181203-0359/projector_config.pbtxt\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tf_nbow_20181203-0359', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fdd0a29ea58>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "\n",
      "To view training (once it starts), run:\n",
      "\n",
      "    tensorboard --logdir='/tmp/tf_nbow_20181203-0359' --port 6006\n",
      "\n",
      "Then in your browser, open: http://localhost:6006\n"
     ]
    }
   ],
   "source": [
    "## Setup model framework\n",
    "## (Must specify correct nbow_model_x name in this cell to use the correct nbow_model_x.py file.)\n",
    "\n",
    "import nbow_model_5; reload(nbow_model_5)\n",
    "\n",
    "# Specify model hyperparameters as used by model_fn.  Use embed_dim2=74 for all LIWC\n",
    "### ADD NEW PARAMETER: liwc_dim???\n",
    "\n",
    "model_params = dict(V=vocab.size, embed_dim=300, hidden_dims=[25], num_classes=2,\n",
    "                    encoder_type='bow',\n",
    "                    lr=0.1, optimizer='adagrad', beta=0.01)  # can set optimizer to 'adagrad' or 'adam', which is slower here\n",
    "\n",
    "checkpoint_dir = \"/tmp/tf_nbow_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "if os.path.isdir(checkpoint_dir):\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "# Write vocabulary to file, so TensorBoard can label embeddings.\n",
    "# creates checkpoint_dir/projector_config.pbtxt and checkpoint_dir/metadata.tsv\n",
    "#ds.vocab.write_projector_config(checkpoint_dir, \"Encoder/Embedding_Layer/W_embed\")\n",
    "vocab.write_projector_config(checkpoint_dir, \"Encoder/Embedding_Layer/W_embed\")\n",
    "\n",
    "model = tf.estimator.Estimator(model_fn=nbow_model_5.classifier_model_fn, \n",
    "                               params=model_params,\n",
    "                               model_dir=checkpoint_dir)\n",
    "print(\"\")\n",
    "print(\"To view training (once it starts), run:\\n\")\n",
    "print(\"    tensorboard --logdir='{:s}' --port 6006\".format(checkpoint_dir))\n",
    "print(\"\\nThen in your browser, open: http://localhost:6006\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tf_nbow_20181203-0359/model.ckpt.\n",
      "INFO:tensorflow:loss = 108.4771, step = 1\n",
      "INFO:tensorflow:global_step/sec: 151.514\n",
      "INFO:tensorflow:loss = 68.38412, step = 101 (0.662 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.833\n",
      "INFO:tensorflow:loss = 60.927498, step = 201 (0.541 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.04\n",
      "INFO:tensorflow:loss = 46.856995, step = 301 (0.529 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.349\n",
      "INFO:tensorflow:loss = 44.080967, step = 401 (0.531 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 405 into /tmp/tf_nbow_20181203-0359/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 10.575054.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-03-03:59:41\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_nbow_20181203-0359/model.ckpt-405\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-03-03:59:42\n",
      "INFO:tensorflow:Saving dict for global step 405: accuracy = 0.6020923, cross_entropy_loss = 0.6648909, global_step = 405, loss = 169.06685\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 405: /tmp/tf_nbow_20181203-0359/model.ckpt-405\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_nbow_20181203-0359/model.ckpt-405\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 405 into /tmp/tf_nbow_20181203-0359/model.ckpt.\n",
      "INFO:tensorflow:loss = 39.890133, step = 406\n",
      "INFO:tensorflow:global_step/sec: 159.675\n",
      "INFO:tensorflow:loss = 41.931137, step = 506 (0.628 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.36\n",
      "INFO:tensorflow:loss = 43.618595, step = 606 (0.519 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.067\n",
      "INFO:tensorflow:loss = 34.97306, step = 706 (0.535 sec)\n",
      "INFO:tensorflow:global_step/sec: 177.376\n",
      "INFO:tensorflow:loss = 35.3194, step = 806 (0.561 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 810 into /tmp/tf_nbow_20181203-0359/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 8.197582.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-03-03:59:52\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_nbow_20181203-0359/model.ckpt-810\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-03-03:59:52\n",
      "INFO:tensorflow:Saving dict for global step 810: accuracy = 0.61147183, cross_entropy_loss = 0.6529845, global_step = 810, loss = 132.45164\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 810: /tmp/tf_nbow_20181203-0359/model.ckpt-810\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_nbow_20181203-0359/model.ckpt-810\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 810 into /tmp/tf_nbow_20181203-0359/model.ckpt.\n",
      "INFO:tensorflow:loss = 31.076193, step = 811\n",
      "INFO:tensorflow:global_step/sec: 158.722\n",
      "INFO:tensorflow:loss = 33.118324, step = 911 (0.632 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.061\n",
      "INFO:tensorflow:loss = 36.427963, step = 1011 (0.521 sec)\n",
      "INFO:tensorflow:global_step/sec: 180.38\n",
      "INFO:tensorflow:loss = 29.385199, step = 1111 (0.556 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.14\n",
      "INFO:tensorflow:loss = 30.53867, step = 1211 (0.520 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1215 into /tmp/tf_nbow_20181203-0359/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 6.942351.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-03-04:00:03\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_nbow_20181203-0359/model.ckpt-1215\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-03-04:00:03\n",
      "INFO:tensorflow:Saving dict for global step 1215: accuracy = 0.6208514, cross_entropy_loss = 0.64673436, global_step = 1215, loss = 113.19392\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1215: /tmp/tf_nbow_20181203-0359/model.ckpt-1215\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_nbow_20181203-0359/model.ckpt-1215\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1215 into /tmp/tf_nbow_20181203-0359/model.ckpt.\n",
      "INFO:tensorflow:loss = 26.377712, step = 1216\n",
      "INFO:tensorflow:global_step/sec: 154.304\n",
      "INFO:tensorflow:loss = 28.078014, step = 1316 (0.650 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.763\n",
      "INFO:tensorflow:loss = 32.03235, step = 1416 (0.512 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.151\n",
      "INFO:tensorflow:loss = 26.474167, step = 1516 (0.538 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.08\n",
      "INFO:tensorflow:loss = 27.322685, step = 1616 (0.516 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1620 into /tmp/tf_nbow_20181203-0359/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 6.089231.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-03-04:00:12\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_nbow_20181203-0359/model.ckpt-1620\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-03-04:00:13\n",
      "INFO:tensorflow:Saving dict for global step 1620: accuracy = 0.6381674, cross_entropy_loss = 0.6382837, global_step = 1620, loss = 100.54448\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1620: /tmp/tf_nbow_20181203-0359/model.ckpt-1620\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_nbow_20181203-0359/model.ckpt-1620\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1620 into /tmp/tf_nbow_20181203-0359/model.ckpt.\n",
      "INFO:tensorflow:loss = 23.271648, step = 1621\n",
      "INFO:tensorflow:global_step/sec: 166.157\n",
      "INFO:tensorflow:loss = 24.661938, step = 1721 (0.603 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.308\n",
      "INFO:tensorflow:loss = 28.938965, step = 1821 (0.528 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.777\n",
      "INFO:tensorflow:loss = 23.388172, step = 1921 (0.538 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.213\n",
      "INFO:tensorflow:loss = 24.900608, step = 2021 (0.520 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2025 into /tmp/tf_nbow_20181203-0359/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 5.461163.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-03-04:00:23\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_nbow_20181203-0359/model.ckpt-2025\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-03-04:00:24\n",
      "INFO:tensorflow:Saving dict for global step 2025: accuracy = 0.64646465, cross_entropy_loss = 0.6304604, global_step = 2025, loss = 91.31016\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2025: /tmp/tf_nbow_20181203-0359/model.ckpt-2025\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tf_nbow_20181203-0359/model.ckpt-2025\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 2025 into /tmp/tf_nbow_20181203-0359/model.ckpt.\n",
      "INFO:tensorflow:loss = 20.98408, step = 2026\n",
      "INFO:tensorflow:global_step/sec: 160.922\n",
      "INFO:tensorflow:loss = 22.141224, step = 2126 (0.623 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.555\n",
      "INFO:tensorflow:loss = 26.584892, step = 2226 (0.527 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.923\n",
      "INFO:tensorflow:loss = 21.485044, step = 2326 (0.532 sec)\n",
      "INFO:tensorflow:global_step/sec: 199.607\n",
      "INFO:tensorflow:loss = 22.993546, step = 2426 (0.502 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2430 into /tmp/tf_nbow_20181203-0359/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4.9619527.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-03-04:00:33\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_nbow_20181203-0359/model.ckpt-2430\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-03-04:00:33\n",
      "INFO:tensorflow:Saving dict for global step 2430: accuracy = 0.6612554, cross_entropy_loss = 0.6245172, global_step = 2430, loss = 84.138664\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2430: /tmp/tf_nbow_20181203-0359/model.ckpt-2430\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_nbow_20181203-0359/model.ckpt-2430\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 2430 into /tmp/tf_nbow_20181203-0359/model.ckpt.\n",
      "INFO:tensorflow:loss = 19.209364, step = 2431\n",
      "INFO:tensorflow:global_step/sec: 173.696\n",
      "INFO:tensorflow:loss = 20.178764, step = 2531 (0.577 sec)\n",
      "INFO:tensorflow:global_step/sec: 199.157\n",
      "INFO:tensorflow:loss = 24.696207, step = 2631 (0.502 sec)\n",
      "INFO:tensorflow:global_step/sec: 200.815\n",
      "INFO:tensorflow:loss = 19.956411, step = 2731 (0.498 sec)\n",
      "INFO:tensorflow:global_step/sec: 200.652\n",
      "INFO:tensorflow:loss = 21.432089, step = 2831 (0.499 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2835 into /tmp/tf_nbow_20181203-0359/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4.5590386.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-03-04:00:43\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_nbow_20181203-0359/model.ckpt-2835\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-03-04:00:44\n",
      "INFO:tensorflow:Saving dict for global step 2835: accuracy = 0.66522366, cross_entropy_loss = 0.6204471, global_step = 2835, loss = 78.334625\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2835: /tmp/tf_nbow_20181203-0359/model.ckpt-2835\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_nbow_20181203-0359/model.ckpt-2835\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 2835 into /tmp/tf_nbow_20181203-0359/model.ckpt.\n",
      "INFO:tensorflow:loss = 17.776726, step = 2836\n",
      "INFO:tensorflow:global_step/sec: 149.358\n",
      "INFO:tensorflow:loss = 18.583584, step = 2936 (0.672 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.208\n",
      "INFO:tensorflow:loss = 23.135319, step = 3036 (0.545 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.806\n",
      "INFO:tensorflow:loss = 18.689598, step = 3136 (0.550 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.205\n",
      "INFO:tensorflow:loss = 20.11398, step = 3236 (0.542 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3240 into /tmp/tf_nbow_20181203-0359/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4.2227826.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-03-04:00:53\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_nbow_20181203-0359/model.ckpt-3240\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-03-04:00:54\n",
      "INFO:tensorflow:Saving dict for global step 3240: accuracy = 0.67496395, cross_entropy_loss = 0.61790144, global_step = 3240, loss = 73.499565\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3240: /tmp/tf_nbow_20181203-0359/model.ckpt-3240\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_nbow_20181203-0359/model.ckpt-3240\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3240 into /tmp/tf_nbow_20181203-0359/model.ckpt.\n",
      "INFO:tensorflow:loss = 16.58367, step = 3241\n",
      "INFO:tensorflow:global_step/sec: 149.198\n",
      "INFO:tensorflow:loss = 20.29335, step = 3341 (0.674 sec)\n",
      "INFO:tensorflow:global_step/sec: 197.351\n",
      "INFO:tensorflow:loss = 21.823172, step = 3441 (0.505 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.276\n",
      "INFO:tensorflow:loss = 17.60738, step = 3541 (0.512 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.473\n",
      "INFO:tensorflow:loss = 18.974861, step = 3641 (0.546 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3645 into /tmp/tf_nbow_20181203-0359/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.9338145.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-03-04:01:03\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_nbow_20181203-0359/model.ckpt-3645\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-03-04:01:04\n",
      "INFO:tensorflow:Saving dict for global step 3645: accuracy = 0.6810967, cross_entropy_loss = 0.6139424, global_step = 3645, loss = 69.38099\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3645: /tmp/tf_nbow_20181203-0359/model.ckpt-3645\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_nbow_20181203-0359/model.ckpt-3645\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3645 into /tmp/tf_nbow_20181203-0359/model.ckpt.\n",
      "INFO:tensorflow:loss = 15.564066, step = 3646\n",
      "INFO:tensorflow:global_step/sec: 150.601\n",
      "INFO:tensorflow:loss = 16.172033, step = 3746 (0.666 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.6\n",
      "INFO:tensorflow:loss = 20.667055, step = 3846 (0.542 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.77\n",
      "INFO:tensorflow:loss = 16.68536, step = 3946 (0.550 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.062\n",
      "INFO:tensorflow:loss = 17.971766, step = 4046 (0.555 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4050 into /tmp/tf_nbow_20181203-0359/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.6840975.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-03-04:01:14\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_nbow_20181203-0359/model.ckpt-4050\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-03-04:01:14\n",
      "INFO:tensorflow:Saving dict for global step 4050: accuracy = 0.6850649, cross_entropy_loss = 0.61420864, global_step = 4050, loss = 65.8235\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4050: /tmp/tf_nbow_20181203-0359/model.ckpt-4050\n"
     ]
    }
   ],
   "source": [
    "## Train model and Evaluate on Dev data\n",
    "\n",
    "# Training params, just used in this cell for the input_fn-s\n",
    "train_params = dict(batch_size=32, total_epochs=10, eval_every=1) # start with 2 epochs rather than 20; eval_every=1 (was 2)\n",
    "assert(train_params['total_epochs'] % train_params['eval_every'] == 0)\n",
    "\n",
    "# Construct and train the model, saving checkpoints to the directory above.\n",
    "# Input function for training set batches\n",
    "# Do 'eval_every' epochs at once, followed by evaluating on the dev set.\n",
    "# NOTE: use patch_numpy_io.numpy_input_fn instead of tf.estimator.inputs.numpy_input_fn\n",
    "train_input_fn = patched_numpy_io.numpy_input_fn(\n",
    "                    x={\"ids\": train_x, \"ns\": train_ns}, y=train_y,\n",
    "                    batch_size=train_params['batch_size'], \n",
    "                    num_epochs=train_params['eval_every'], shuffle=True, seed=42\n",
    "                 )\n",
    "\n",
    "# Input function for dev set batches. As above, but:\n",
    "# - Don't randomize order\n",
    "# - Iterate exactly once (one epoch)\n",
    "dev_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": dev_x, \"ns\": dev_ns}, y=dev_y,\n",
    "                    batch_size=128, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "for _ in range(train_params['total_epochs'] // train_params['eval_every']):\n",
    "    # Train for a few epochs, then evaluate on dev\n",
    "    model.train(input_fn=train_input_fn)\n",
    "    eval_metrics = model.evaluate(input_fn=dev_input_fn, name=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-03-04:01:15\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_nbow_20181203-0359/model.ckpt-4050\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-03-04:01:15\n",
      "INFO:tensorflow:Saving dict for global step 4050: accuracy = 0.69852144, cross_entropy_loss = 0.59404063, global_step = 4050, loss = 67.47291\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4050: /tmp/tf_nbow_20181203-0359/model.ckpt-4050\n",
      "Accuracy on test set: 69.85%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.69852144,\n",
       " 'cross_entropy_loss': 0.59404063,\n",
       " 'loss': 67.47291,\n",
       " 'global_step': 4050}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Evaluate model on (ISOT) Test data\n",
    "\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": test_x, \"ns\": test_ns}, y=test_y,\n",
    "                    batch_size=128, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "eval_metrics = model.evaluate(input_fn=test_input_fn, name=\"test\")\n",
    "\n",
    "print(\"Accuracy on test set: {:.02%}\".format(eval_metrics['accuracy']))\n",
    "eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_nbow_20181203-0359/model.ckpt-4050\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Accuracy on test set: 69.85%\n"
     ]
    }
   ],
   "source": [
    "## We can also evaluate the old-fashioned way, by calling model.predict(...) and working with the predicted labels directly:\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "predictions = list(model.predict(test_input_fn))  # list of dicts\n",
    "y_pred = [p['max'] for p in predictions]\n",
    "acc = accuracy_score(y_pred, test_y)\n",
    "print(\"Accuracy on test set: {:.02%}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy of 70% is better than the baseline NB result of 62%.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create padded ISOT data and apply prediction function to ISOT data.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 831, 2434, 183, 618, 8, 382, 2, 35, 2, 1234, 2, 165, 2859, 363, 18, 2, 425], [6364, 9, 18, 8701, 177, 2, 30, 61, 1364, 329, 8, 14235, 33, 31], [2, 2, 4, 977, 12, 169, 8032, 34, 117, 325, 421, 2, 2, 1120, 3443, 294, 2, 155, 8, 8419, 469, 2, 129, 289, 8421, 5, 254, 145, 374, 1433], [591, 2, 4383, 2, 12000, 2, 431, 1658, 5, 3299, 2, 2], [282, 136, 3248, 3308, 2, 3606, 325, 1466]]\n",
      "\n",
      "[[    2   831  2434   183   618     8   382     2    35     2  1234     2\n",
      "    165  2859   363    18     2   425     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]\n",
      " [ 6364     9    18  8701   177     2    30    61  1364   329     8 14235\n",
      "     33    31     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]]\n",
      "\n",
      "[18 14]\n",
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "## ISOT data padding\n",
    "\n",
    "all_isot_ids=[]\n",
    "for i, tokens in enumerate(isot_title_tokans):  # here, tokens are the words in a single sentence\n",
    "    sent_ids = vocab.words_to_ids(tokens)\n",
    "    all_isot_ids.append(sent_ids)\n",
    "print(all_isot_ids[:5])\n",
    "\n",
    "max_len = 40   # Retain this setting, since it fits the ISOT \"title\" length distribution quite well.\n",
    "isot_x, isot_ns = utils.pad_np_array(all_isot_ids, max_len=max_len)\n",
    "print()\n",
    "print(isot_x[:2])\n",
    "print()\n",
    "print(isot_ns[:2])\n",
    "\n",
    "isot_y = isot_labels\n",
    "print(isot_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: STOP!! - This requires manual intervention in the liwc_features function within the nbow_model_5.py file to set the correct LIWC type!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "\n",
    "##############################\n",
    "# PROCEED ONLY AFTER UPDATING nbow_model_5.py file!!!!!\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-03-04:34:22\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_nbow_20181203-0359/model.ckpt-4050\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-03-04:34:25\n",
      "INFO:tensorflow:Saving dict for global step 4050: accuracy = 0.55229634, cross_entropy_loss = 0.95396733, global_step = 4050, loss = 158.98058\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4050: /tmp/tf_nbow_20181203-0359/model.ckpt-4050\n",
      "Accuracy on ISOT set: 55.23%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.55229634,\n",
       " 'cross_entropy_loss': 0.95396733,\n",
       " 'loss': 158.98058,\n",
       " 'global_step': 4050}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Evaluate model on LIAR data\n",
    "\n",
    "####. S  T. O. P. !!!!  ###\n",
    "\n",
    "### NOTE: MUST SELECT WHICH LIWC FILE TO USE WITHIN nbow_model_x.py, specifically the liwc_features function.\n",
    "\n",
    "\n",
    "\n",
    "##############################\n",
    "# PROCEED ONLY AFTER UPDATING nbow_model_3.py file!!!!!\n",
    "##############################\n",
    "\n",
    "\n",
    "reload(nbow_model_5)   ### \n",
    "\n",
    "test_input_fn_isot = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": isot_x, \"ns\": isot_ns}, y=isot_y,\n",
    "                    batch_size=128, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "eval_metrics = model.evaluate(input_fn=test_input_fn_isot, name=\"test\")\n",
    "\n",
    "print(\"Accuracy on ISOT set: {:.02%}\".format(eval_metrics['accuracy']))\n",
    "eval_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction accuracy of 55% for ISOT data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
