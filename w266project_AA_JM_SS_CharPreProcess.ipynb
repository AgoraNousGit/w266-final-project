{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIAR DETECTION GROUP PROJECT\n",
    "\n",
    "Run the cell below to import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "from functools import reduce\n",
    "\n",
    "#import unittest\n",
    "from IPython.display import display, HTML\n",
    "#from sklearn.utils import shuffle\n",
    "# NLTK for NLP utils and corpora\n",
    "#import nltk\n",
    "from collections import defaultdict\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import tensorflow as tf\n",
    "import pickle\n",
    "import dill\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary, tf_embed_viz\n",
    "import timeit  #For timing\n",
    "\n",
    "#from project_files import pdio # for saving and loading dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "Loading data from Pre-Processing step Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44898 entries, 0 to 44897\n",
      "Data columns (total 5 columns):\n",
      "title      44898 non-null object\n",
      "text       44898 non-null object\n",
      "subject    44898 non-null object\n",
      "date       44898 non-null object\n",
      "target     44898 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 151.9 MB\n"
     ]
    }
   ],
   "source": [
    "#Read pkl file from part 1\n",
    "all_data = pd.read_pickle('parsed_data/df_alldata1.pkl')\n",
    "all_data.info(memory_usage='deep', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process character n-grams\n",
    "Create character n-grams for each of the news entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"North Korea's Kim Jong Un fetes nuclear scientists, holds celebration bash\", 'White House developing comprehensive biosecurity strategy: official', 'Republican Senator Alexander to consult on bipartisan healthcare plan', 'Zimbabwe\\'s army seizes power, Mugabe confined but \"safe\"', 'U.S. senator urges DOJ to reject any White House push in merger probes']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' \"saf',\n",
       " ' alex',\n",
       " ' any ',\n",
       " ' army',\n",
       " ' bash',\n",
       " ' bios',\n",
       " ' bipa',\n",
       " ' but ',\n",
       " ' cele',\n",
       " ' comp',\n",
       " ' conf',\n",
       " ' cons',\n",
       " ' deve',\n",
       " ' doj ',\n",
       " ' fete',\n",
       " ' heal',\n",
       " ' hold',\n",
       " ' hous',\n",
       " ' in ',\n",
       " ' jong',\n",
       " ' kim ',\n",
       " ' kore',\n",
       " ' merg',\n",
       " ' muga',\n",
       " ' nort',\n",
       " ' nucl',\n",
       " ' offi',\n",
       " ' on ',\n",
       " ' plan',\n",
       " ' powe',\n",
       " ' prob',\n",
       " ' push',\n",
       " ' reje',\n",
       " ' repu',\n",
       " ' scie',\n",
       " ' seiz',\n",
       " ' sena',\n",
       " ' stra',\n",
       " ' to ',\n",
       " ' u.s.',\n",
       " ' un ',\n",
       " ' urge',\n",
       " ' whit',\n",
       " ' zimb',\n",
       " '\"safe',\n",
       " \"abwe'\",\n",
       " 'afe\" ',\n",
       " 'alexa',\n",
       " 'althc',\n",
       " 'ander',\n",
       " 'army ',\n",
       " 'artis',\n",
       " 'ategy',\n",
       " 'ation',\n",
       " 'ator ',\n",
       " 'babwe',\n",
       " 'bash ',\n",
       " 'biose',\n",
       " 'bipar',\n",
       " 'blica',\n",
       " 'brati',\n",
       " \"bwe's\",\n",
       " 'care ',\n",
       " 'celeb',\n",
       " 'cial ',\n",
       " 'cient',\n",
       " 'clear',\n",
       " 'compr',\n",
       " 'confi',\n",
       " 'consu',\n",
       " 'curit',\n",
       " 'devel',\n",
       " \"ea's \",\n",
       " 'ealth',\n",
       " 'ebrat',\n",
       " 'ecuri',\n",
       " 'egy: ',\n",
       " 'ehens',\n",
       " 'eizes',\n",
       " 'eject',\n",
       " 'elebr',\n",
       " 'elopi',\n",
       " 'enato',\n",
       " 'ensiv',\n",
       " 'entis',\n",
       " 'epubl',\n",
       " 'erger',\n",
       " 'etes ',\n",
       " 'evelo',\n",
       " 'exand',\n",
       " 'fetes',\n",
       " 'ffici',\n",
       " 'ficia',\n",
       " 'fined',\n",
       " 'gabe ',\n",
       " 'hcare',\n",
       " 'healt',\n",
       " 'hensi',\n",
       " 'hite ',\n",
       " 'holds',\n",
       " 'house',\n",
       " 'ican ',\n",
       " 'icial',\n",
       " 'ienti',\n",
       " 'imbab',\n",
       " 'ined ',\n",
       " 'iosec',\n",
       " 'ipart',\n",
       " 'isan ',\n",
       " 'ists,',\n",
       " 'izes ',\n",
       " 'ject ',\n",
       " 'jong ',\n",
       " 'korea',\n",
       " 'lear ',\n",
       " 'lebra',\n",
       " 'lexan',\n",
       " 'lican',\n",
       " 'lopin',\n",
       " 'lthca',\n",
       " 'mbabw',\n",
       " 'merge',\n",
       " 'mpreh',\n",
       " 'mugab',\n",
       " 'nator',\n",
       " 'nder ',\n",
       " 'nfine',\n",
       " 'north',\n",
       " 'nsive',\n",
       " 'nsult',\n",
       " 'ntist',\n",
       " 'nucle',\n",
       " 'obes ',\n",
       " 'offic',\n",
       " 'olds ',\n",
       " 'ompre',\n",
       " 'onfin',\n",
       " 'onsul',\n",
       " 'oping',\n",
       " \"orea'\",\n",
       " 'orth ',\n",
       " 'osecu',\n",
       " 'ouse ',\n",
       " 'ower,',\n",
       " 'parti',\n",
       " 'ping ',\n",
       " 'plan ',\n",
       " 'power',\n",
       " 'prehe',\n",
       " 'probe',\n",
       " 'publi',\n",
       " 'push ',\n",
       " 'rateg',\n",
       " 'ratio',\n",
       " \"rea's\",\n",
       " 'rehen',\n",
       " 'rejec',\n",
       " 'repub',\n",
       " 'rger ',\n",
       " 'rges ',\n",
       " 'rity ',\n",
       " 'robes',\n",
       " 'rtisa',\n",
       " 'safe\"',\n",
       " 'scien',\n",
       " 'secur',\n",
       " 'seize',\n",
       " 'senat',\n",
       " 'sive ',\n",
       " 'strat',\n",
       " 'sts, ',\n",
       " 'sult ',\n",
       " 'tegy:',\n",
       " 'thcar',\n",
       " 'tion ',\n",
       " 'tisan',\n",
       " 'tists',\n",
       " 'trate',\n",
       " 'u.s. ',\n",
       " 'ublic',\n",
       " 'uclea',\n",
       " 'ugabe',\n",
       " 'urges',\n",
       " 'urity',\n",
       " 'velop',\n",
       " \"we's \",\n",
       " 'wer, ',\n",
       " 'white',\n",
       " 'xande',\n",
       " 'zimba']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_char_ngram_vectorizer(text_list, ngram_len):\n",
    "    ngrams = collections.Counter()\n",
    "    for item in text_list:\n",
    "        # remove extra white space\n",
    "        text_tokens = item.split()\n",
    "        new_item = ' '.join(text_tokens)\n",
    "\n",
    "        # generate character ngrams\n",
    "        char_ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(ngram_len, ngram_len))\n",
    "        char_ngram_vectorizer.fit_transform(text_list)\n",
    "    return char_ngram_vectorizer\n",
    "\n",
    "true_titles = list(all_data.loc[all_data['target']=='1']['title'][:5])\n",
    "print(true_titles)\n",
    "\n",
    "my_ngram_vec = get_char_ngram_vectorizer(true_titles, 5)\n",
    "my_ngram_vec.get_feature_names()\n",
    "\n",
    "# len(my_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"North Korea's Kim Jong Un fetes nuclear scientists, holds celebration bash\", 'White House developing comprehensive biosecurity strategy: official', 'Republican Senator Alexander to consult on bipartisan healthcare plan', 'Zimbabwe\\'s army seizes power, Mugabe confined but \"safe\"', 'U.S. senator urges DOJ to reject any White House push in merger probes']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' \"saf',\n",
       " ' alex',\n",
       " ' any ',\n",
       " ' army',\n",
       " ' bash',\n",
       " ' bios',\n",
       " ' bipa',\n",
       " ' but ',\n",
       " ' cele',\n",
       " ' comp',\n",
       " ' conf',\n",
       " ' cons',\n",
       " ' deve',\n",
       " ' doj ',\n",
       " ' fete',\n",
       " ' heal',\n",
       " ' hold',\n",
       " ' hous',\n",
       " ' in ',\n",
       " ' jong',\n",
       " ' kim ',\n",
       " ' kore',\n",
       " ' merg',\n",
       " ' muga',\n",
       " ' nort',\n",
       " ' nucl',\n",
       " ' offi',\n",
       " ' on ',\n",
       " ' plan',\n",
       " ' powe',\n",
       " ' prob',\n",
       " ' push',\n",
       " ' reje',\n",
       " ' repu',\n",
       " ' scie',\n",
       " ' seiz',\n",
       " ' sena',\n",
       " ' stra',\n",
       " ' to ',\n",
       " ' u.s.',\n",
       " ' un ',\n",
       " ' urge',\n",
       " ' whit',\n",
       " ' zimb',\n",
       " '\"safe',\n",
       " \"abwe'\",\n",
       " 'afe\" ',\n",
       " 'alexa',\n",
       " 'althc',\n",
       " 'ander',\n",
       " 'army ',\n",
       " 'artis',\n",
       " 'ategy',\n",
       " 'ation',\n",
       " 'ator ',\n",
       " 'babwe',\n",
       " 'bash ',\n",
       " 'biose',\n",
       " 'bipar',\n",
       " 'blica',\n",
       " 'brati',\n",
       " \"bwe's\",\n",
       " 'care ',\n",
       " 'celeb',\n",
       " 'cial ',\n",
       " 'cient',\n",
       " 'clear',\n",
       " 'compr',\n",
       " 'confi',\n",
       " 'consu',\n",
       " 'curit',\n",
       " 'devel',\n",
       " \"ea's \",\n",
       " 'ealth',\n",
       " 'ebrat',\n",
       " 'ecuri',\n",
       " 'egy: ',\n",
       " 'ehens',\n",
       " 'eizes',\n",
       " 'eject',\n",
       " 'elebr',\n",
       " 'elopi',\n",
       " 'enato',\n",
       " 'ensiv',\n",
       " 'entis',\n",
       " 'epubl',\n",
       " 'erger',\n",
       " 'etes ',\n",
       " 'evelo',\n",
       " 'exand',\n",
       " 'fetes',\n",
       " 'ffici',\n",
       " 'ficia',\n",
       " 'fined',\n",
       " 'gabe ',\n",
       " 'hcare',\n",
       " 'healt',\n",
       " 'hensi',\n",
       " 'hite ',\n",
       " 'holds',\n",
       " 'house',\n",
       " 'ican ',\n",
       " 'icial',\n",
       " 'ienti',\n",
       " 'imbab',\n",
       " 'ined ',\n",
       " 'iosec',\n",
       " 'ipart',\n",
       " 'isan ',\n",
       " 'ists,',\n",
       " 'izes ',\n",
       " 'ject ',\n",
       " 'jong ',\n",
       " 'korea',\n",
       " 'lear ',\n",
       " 'lebra',\n",
       " 'lexan',\n",
       " 'lican',\n",
       " 'lopin',\n",
       " 'lthca',\n",
       " 'mbabw',\n",
       " 'merge',\n",
       " 'mpreh',\n",
       " 'mugab',\n",
       " 'nator',\n",
       " 'nder ',\n",
       " 'nfine',\n",
       " 'north',\n",
       " 'nsive',\n",
       " 'nsult',\n",
       " 'ntist',\n",
       " 'nucle',\n",
       " 'obes ',\n",
       " 'offic',\n",
       " 'olds ',\n",
       " 'ompre',\n",
       " 'onfin',\n",
       " 'onsul',\n",
       " 'oping',\n",
       " \"orea'\",\n",
       " 'orth ',\n",
       " 'osecu',\n",
       " 'ouse ',\n",
       " 'ower,',\n",
       " 'parti',\n",
       " 'ping ',\n",
       " 'plan ',\n",
       " 'power',\n",
       " 'prehe',\n",
       " 'probe',\n",
       " 'publi',\n",
       " 'push ',\n",
       " 'rateg',\n",
       " 'ratio',\n",
       " \"rea's\",\n",
       " 'rehen',\n",
       " 'rejec',\n",
       " 'repub',\n",
       " 'rger ',\n",
       " 'rges ',\n",
       " 'rity ',\n",
       " 'robes',\n",
       " 'rtisa',\n",
       " 'safe\"',\n",
       " 'scien',\n",
       " 'secur',\n",
       " 'seize',\n",
       " 'senat',\n",
       " 'sive ',\n",
       " 'strat',\n",
       " 'sts, ',\n",
       " 'sult ',\n",
       " 'tegy:',\n",
       " 'thcar',\n",
       " 'tion ',\n",
       " 'tisan',\n",
       " 'tists',\n",
       " 'trate',\n",
       " 'u.s. ',\n",
       " 'ublic',\n",
       " 'uclea',\n",
       " 'ugabe',\n",
       " 'urges',\n",
       " 'urity',\n",
       " 'velop',\n",
       " \"we's \",\n",
       " 'wer, ',\n",
       " 'white',\n",
       " 'xande',\n",
       " 'zimba']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_char_ngram_vectorizer(text_list, ngram_len):\n",
    "    ngrams = collections.Counter()\n",
    "    for item in text_list:\n",
    "        # remove extra white space\n",
    "        text_tokens = item.split()\n",
    "        new_item = ' '.join(text_tokens)\n",
    "\n",
    "        # generate character ngrams\n",
    "        char_ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(ngram_len, ngram_len))\n",
    "        char_ngram_vectorizer.fit_transform(text_list)\n",
    "    return char_ngram_vectorizer\n",
    "\n",
    "true_titles = list(all_data.loc[all_data['target']=='1']['title'][:5])\n",
    "print(true_titles)\n",
    "\n",
    "my_ngram_vec = get_char_ngram_vectorizer(true_titles, 5)\n",
    "my_ngram_vec.get_feature_names()\n",
    "\n",
    "# len(my_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"North Korea's Kim Jong Un fetes nuclear scientists, holds celebration bash\", 'White House developing comprehensive biosecurity strategy: official', 'Republican Senator Alexander to consult on bipartisan healthcare plan', 'Zimbabwe\\'s army seizes power, Mugabe confined but \"safe\"', 'U.S. senator urges DOJ to reject any White House push in merger probes']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' \"saf',\n",
       " ' alex',\n",
       " ' any ',\n",
       " ' army',\n",
       " ' bash',\n",
       " ' bios',\n",
       " ' bipa',\n",
       " ' but ',\n",
       " ' cele',\n",
       " ' comp',\n",
       " ' conf',\n",
       " ' cons',\n",
       " ' deve',\n",
       " ' doj ',\n",
       " ' fete',\n",
       " ' heal',\n",
       " ' hold',\n",
       " ' hous',\n",
       " ' in ',\n",
       " ' jong',\n",
       " ' kim ',\n",
       " ' kore',\n",
       " ' merg',\n",
       " ' muga',\n",
       " ' nort',\n",
       " ' nucl',\n",
       " ' offi',\n",
       " ' on ',\n",
       " ' plan',\n",
       " ' powe',\n",
       " ' prob',\n",
       " ' push',\n",
       " ' reje',\n",
       " ' repu',\n",
       " ' scie',\n",
       " ' seiz',\n",
       " ' sena',\n",
       " ' stra',\n",
       " ' to ',\n",
       " ' u.s.',\n",
       " ' un ',\n",
       " ' urge',\n",
       " ' whit',\n",
       " ' zimb',\n",
       " '\"safe',\n",
       " \"abwe'\",\n",
       " 'afe\" ',\n",
       " 'alexa',\n",
       " 'althc',\n",
       " 'ander',\n",
       " 'army ',\n",
       " 'artis',\n",
       " 'ategy',\n",
       " 'ation',\n",
       " 'ator ',\n",
       " 'babwe',\n",
       " 'bash ',\n",
       " 'biose',\n",
       " 'bipar',\n",
       " 'blica',\n",
       " 'brati',\n",
       " \"bwe's\",\n",
       " 'care ',\n",
       " 'celeb',\n",
       " 'cial ',\n",
       " 'cient',\n",
       " 'clear',\n",
       " 'compr',\n",
       " 'confi',\n",
       " 'consu',\n",
       " 'curit',\n",
       " 'devel',\n",
       " \"ea's \",\n",
       " 'ealth',\n",
       " 'ebrat',\n",
       " 'ecuri',\n",
       " 'egy: ',\n",
       " 'ehens',\n",
       " 'eizes',\n",
       " 'eject',\n",
       " 'elebr',\n",
       " 'elopi',\n",
       " 'enato',\n",
       " 'ensiv',\n",
       " 'entis',\n",
       " 'epubl',\n",
       " 'erger',\n",
       " 'etes ',\n",
       " 'evelo',\n",
       " 'exand',\n",
       " 'fetes',\n",
       " 'ffici',\n",
       " 'ficia',\n",
       " 'fined',\n",
       " 'gabe ',\n",
       " 'hcare',\n",
       " 'healt',\n",
       " 'hensi',\n",
       " 'hite ',\n",
       " 'holds',\n",
       " 'house',\n",
       " 'ican ',\n",
       " 'icial',\n",
       " 'ienti',\n",
       " 'imbab',\n",
       " 'ined ',\n",
       " 'iosec',\n",
       " 'ipart',\n",
       " 'isan ',\n",
       " 'ists,',\n",
       " 'izes ',\n",
       " 'ject ',\n",
       " 'jong ',\n",
       " 'korea',\n",
       " 'lear ',\n",
       " 'lebra',\n",
       " 'lexan',\n",
       " 'lican',\n",
       " 'lopin',\n",
       " 'lthca',\n",
       " 'mbabw',\n",
       " 'merge',\n",
       " 'mpreh',\n",
       " 'mugab',\n",
       " 'nator',\n",
       " 'nder ',\n",
       " 'nfine',\n",
       " 'north',\n",
       " 'nsive',\n",
       " 'nsult',\n",
       " 'ntist',\n",
       " 'nucle',\n",
       " 'obes ',\n",
       " 'offic',\n",
       " 'olds ',\n",
       " 'ompre',\n",
       " 'onfin',\n",
       " 'onsul',\n",
       " 'oping',\n",
       " \"orea'\",\n",
       " 'orth ',\n",
       " 'osecu',\n",
       " 'ouse ',\n",
       " 'ower,',\n",
       " 'parti',\n",
       " 'ping ',\n",
       " 'plan ',\n",
       " 'power',\n",
       " 'prehe',\n",
       " 'probe',\n",
       " 'publi',\n",
       " 'push ',\n",
       " 'rateg',\n",
       " 'ratio',\n",
       " \"rea's\",\n",
       " 'rehen',\n",
       " 'rejec',\n",
       " 'repub',\n",
       " 'rger ',\n",
       " 'rges ',\n",
       " 'rity ',\n",
       " 'robes',\n",
       " 'rtisa',\n",
       " 'safe\"',\n",
       " 'scien',\n",
       " 'secur',\n",
       " 'seize',\n",
       " 'senat',\n",
       " 'sive ',\n",
       " 'strat',\n",
       " 'sts, ',\n",
       " 'sult ',\n",
       " 'tegy:',\n",
       " 'thcar',\n",
       " 'tion ',\n",
       " 'tisan',\n",
       " 'tists',\n",
       " 'trate',\n",
       " 'u.s. ',\n",
       " 'ublic',\n",
       " 'uclea',\n",
       " 'ugabe',\n",
       " 'urges',\n",
       " 'urity',\n",
       " 'velop',\n",
       " \"we's \",\n",
       " 'wer, ',\n",
       " 'white',\n",
       " 'xande',\n",
       " 'zimba']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_char_ngram_vectorizer(text_list, ngram_len):\n",
    "    ngrams = collections.Counter()\n",
    "    for item in text_list:\n",
    "        # remove extra white space\n",
    "        text_tokens = item.split()\n",
    "        new_item = ' '.join(text_tokens)\n",
    "\n",
    "        # generate character ngrams\n",
    "        char_ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(ngram_len, ngram_len))\n",
    "        char_ngram_vectorizer.fit_transform(text_list)\n",
    "    return char_ngram_vectorizer\n",
    "\n",
    "true_titles = list(all_data.loc[all_data['target']=='1']['title'][:5])\n",
    "print(true_titles)\n",
    "\n",
    "my_ngram_vec = get_char_ngram_vectorizer(true_titles, 5)\n",
    "my_ngram_vec.get_feature_names()\n",
    "\n",
    "# len(my_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pickle file for the vocabulary\n",
    "def pikle_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "def dill_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        dill.dump(obj, output)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pickle the Vocabulary\n",
    "pikle_object(vocab, 'parsed_data/vocab.pkl')\n",
    "\n",
    "#Save the unknown sequence for analysis\n",
    "np.save('parsed_data/GloVe_Unknown_50.npy', glove_dd['unk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdio.save_pandas(r\"parsed_data/df_alldata_embed.pdy\", all_data) ## does not work for objects\n",
    "#all_data.to_msgpack('parsed_data/df_alldata_embed.msg') ## error\n",
    "\n",
    "#store = pd.HDFStore('parsed_data/pdata.h5')\n",
    "#store['df_alldata_embed'] = all_data   ## cant save complex objects\n",
    "\n",
    "#all_data1.to_hdf('parsed_data/pdata.h5' , 'df_alldata_embed', format = 'table', mode = 'w' ) #kernel restarts\n",
    "\n",
    "#all_data.to_parquet('parsed_data/df_alldata_embed.parquet') #embed column cannot be parquet\n",
    "\n",
    "#all_data.to_json('parsed_data/df_alldata_embed.json')  #kernel restarts\n",
    "#all_data.to_feather('parsed_data/df_alldata_embed.feather')  #cant use on embeddings\n",
    "#all_data.to_csv('parsed_data/df_alldata_embed.csv', chunksize =5000 ) #breaks embeddings\n",
    "\n",
    "\n",
    "#### Thanks Mike Mckerns for creating dill, the hammer of pickle!!!!\n",
    "\n",
    "dill_object(all_data, 'parsed_data/df_alldata_embed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.9G\r\n",
      "drwxrwxr-x 2 w266project w266project 4.0K Nov 17 05:16 .\r\n",
      "drwxrwxr-x 9 w266project w266project 4.0K Nov 17 05:18 ..\r\n",
      "-rw-rw-r-- 1 w266project w266project  528 Nov 17 02:04 GloVe_Unknown_50.npy\r\n",
      "-rw-rw-r-- 1 w266project w266project  95M Nov 12 06:53 df_alldata1.pkl\r\n",
      "-rw-rw-r-- 1 w266project w266project 368M Nov 17 02:01 df_alldata2.pkl\r\n",
      "-rw-rw-r-- 1 w266project w266project 4.5G Nov 17 05:19 df_alldata_embed.pkl\r\n",
      "-rw-rw-r-- 1 w266project w266project 2.3K Nov 17 04:28 pdata.h5\r\n",
      "-rw-rw-r-- 1 w266project w266project  34M Nov 17 02:04 vocab.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./parsed_data -lah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
