{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIAR DETECTION GROUP PROJECT\n",
    "\n",
    "Run the cell below to import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "from functools import reduce\n",
    "\n",
    "#import unittest\n",
    "from IPython.display import display, HTML\n",
    "#from sklearn.utils import shuffle\n",
    "# NLTK for NLP utils and corpora\n",
    "#import nltk\n",
    "from collections import defaultdict\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import tensorflow as tf\n",
    "import pickle\n",
    "import dill\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary, tf_embed_viz\n",
    "import timeit  #For timing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from project_files import pdio # for saving and loading dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "Loading data from Pre-Processing step Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23258 entries, 0 to 23257\n",
      "Data columns (total 6 columns):\n",
      "target            23258 non-null int64\n",
      "title             23258 non-null object\n",
      "title_tokcan      23258 non-null object\n",
      "title_POS         23258 non-null object\n",
      "binary_target     23258 non-null int64\n",
      "embedded_title    23258 non-null object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 19.1 MB\n"
     ]
    }
   ],
   "source": [
    "#Read pkl file from part 1\n",
    "# all_data = pd.read_pickle('parsed_data/df_alldata1.pkl')\n",
    "all_data = pd.read_pickle('parsed_data/df_liarpolitifact_data_embed.pkl')\n",
    "all_data.info(memory_usage='deep', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>title</th>\n",
       "      <th>title_tokcan</th>\n",
       "      <th>title_POS</th>\n",
       "      <th>binary_target</th>\n",
       "      <th>embedded_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Says 31 percent of Texas physicians accept all...</td>\n",
       "      <td>[says, &lt;number&gt;, percent, of, texas, physician...</td>\n",
       "      <td>[V, $, N, P, ^, N, V, D, A, ^, N, ,, R, P, $, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.11797, 0.21126, 0.29075, -0.021211, 0.7819...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>''Both Democrats and Republicans are advocatin...</td>\n",
       "      <td>['', both, democrats, and, republicans, are, a...</td>\n",
       "      <td>[,, D, N, &amp;, N, V, V, P, D, N, P, N, N, V, P, ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[[0.0028594, 0.19457, -0.19449, -0.037583, 0.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>A Republican-led softening of firearms trainin...</td>\n",
       "      <td>[a, republican-led, softening, of, firearms, t...</td>\n",
       "      <td>[D, A, N, P, N, N, N, V, D, A, N, V, V, V, P, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.21705, 0.46515, -0.46757, 0.10082, 1.0135,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>The first tweet was sent from Austin.</td>\n",
       "      <td>[the, first, tweet, was, sent, from, austin, .]</td>\n",
       "      <td>[D, A, N, V, V, P, ^, ,]</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Georgia has the countrys second highest number...</td>\n",
       "      <td>[georgia, has, the, countrys, second, highest,...</td>\n",
       "      <td>[^, V, D, N, A, A, N, P, A, N, N, N, ,]</td>\n",
       "      <td>-1</td>\n",
       "      <td>[[-1.3427, 0.4592, 0.19281, 0.71305, -0.5934, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                              title  \\\n",
       "0       1  Says 31 percent of Texas physicians accept all...   \n",
       "1       2  ''Both Democrats and Republicans are advocatin...   \n",
       "2       0  A Republican-led softening of firearms trainin...   \n",
       "3       5              The first tweet was sent from Austin.   \n",
       "4       2  Georgia has the countrys second highest number...   \n",
       "\n",
       "                                        title_tokcan  \\\n",
       "0  [says, <number>, percent, of, texas, physician...   \n",
       "1  ['', both, democrats, and, republicans, are, a...   \n",
       "2  [a, republican-led, softening, of, firearms, t...   \n",
       "3    [the, first, tweet, was, sent, from, austin, .]   \n",
       "4  [georgia, has, the, countrys, second, highest,...   \n",
       "\n",
       "                                           title_POS  binary_target  \\\n",
       "0  [V, $, N, P, ^, N, V, D, A, ^, N, ,, R, P, $, ...              1   \n",
       "1  [,, D, N, &, N, V, V, P, D, N, P, N, N, V, P, ...             -1   \n",
       "2  [D, A, N, P, N, N, N, V, D, A, N, V, V, V, P, ...              1   \n",
       "3                           [D, A, N, V, V, P, ^, ,]              0   \n",
       "4            [^, V, D, N, A, A, N, P, A, N, N, N, ,]             -1   \n",
       "\n",
       "                                      embedded_title  \n",
       "0  [[0.11797, 0.21126, 0.29075, -0.021211, 0.7819...  \n",
       "1  [[0.0028594, 0.19457, -0.19449, -0.037583, 0.9...  \n",
       "2  [[0.21705, 0.46515, -0.46757, 0.10082, 1.0135,...  \n",
       "3  [[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -...  \n",
       "4  [[-1.3427, 0.4592, 0.19281, 0.71305, -0.5934, ...  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char pre-process\n",
    "Make it easier to select different sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select column to train on \n",
    "# mini_data = all_data[:10]\n",
    "\n",
    "tok_char_n = lambda a, ngram_len: [a[i:i+ngram_len] for i in range(len(a))][:-ngram_len]\n",
    "\n",
    "n_gram_len = 2\n",
    "field_name='title'\n",
    "new_field_name = field_name+'_pad'\n",
    "tok_field_name = field_name+'_tok'\n",
    "tok_int_field_name = field_name+'_tok_int'\n",
    "\n",
    "# find 95% quantile\n",
    "text_lens=[len(i) for i in all_data[field_name]]\n",
    "pad_len = int(np.percentile(text_lens, 95))\n",
    "\n",
    "# truncate/pad new field to 95th percentile\n",
    "all_data[new_field_name] = [(\" \".join(i.lower().split()) + pad_len * ' ')[:pad_len] for i in all_data[field_name]]\n",
    "\n",
    "# tokenize\n",
    "all_data[tok_field_name] = [tok_char_n(i,n_gram_len) for i in all_data[new_field_name]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define helper functions\n",
    "Helper functions for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [br, ra, ai, in, ni, ia, ac, c ,  g, ge, et, t...\n",
       "1    [wi, in, nd, do, ow, ws, s ,  1, 10, 0 ,  i, i...\n",
       "2    [st, tu, un, nn, ni, in, ng, g ,  s, st, to, o...\n",
       "3    [no, or, rt, th, h ,  k, ko, or, re, ea, a', '...\n",
       "4    [wh, hi, it, te, e ,  h, ho, ou, us, se, e ,  ...\n",
       "Name: title_tok, dtype: object"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pulled from a StackOverflow post\n",
    "\n",
    "def train_validate_test_split(df, train_percent=.6, validate_percent=.2, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df.index)\n",
    "    train_end = int(train_percent * m)\n",
    "    validate_end = int(validate_percent * m) + train_end\n",
    "    train = df.ix[perm[:train_end]]\n",
    "    validate = df.ix[perm[train_end:validate_end]]\n",
    "    test = df.ix[perm[validate_end:]]\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process character n-grams\n",
    "Create character n-grams for each of the news entries. This gives me the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [793, 1441, 740, 1099, 1295, 1086, 734, 804, 3...\n",
       "1    [1660, 1099, 1290, 875, 1358, 1668, 1473, 12, ...\n",
       "2    [1512, 1559, 1595, 1300, 1295, 1099, 1293, 984...\n",
       "3    [1301, 1353, 1460, 1546, 1032, 36, 1168, 1353,...\n",
       "4    [1659, 1056, 1105, 1543, 893, 33, 1061, 1356, ...\n",
       "5    [1209, 1347, 1183, 64, 32, 1007, 922, 1353, 14...\n",
       "6    [1056, 1097, 1206, 1195, 749, 1465, 1709, 28, ...\n",
       "7    [1445, 923, 1399, 1583, 787, 1203, 1088, 817, ...\n",
       "8    [1159, 919, 1206, 1218, 1725, 745, 1300, 1291,...\n",
       "9    [1770, 1098, 1243, 777, 733, 798, 1656, 896, 1...\n",
       "Name: title_tok_int, dtype: object"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = []\n",
    "ngram = []\n",
    "start_ngram = 2\n",
    "vocab = []\n",
    "for ngram_len in range(start_ngram,2):\n",
    "    i = ngram_len-2\n",
    "    vec.append(CountVectorizer(analyzer='char', binary=True, ngram_range=(n_gram_len, n_gram_len)))\n",
    "    ngram.append(vec[i].fit_transform(all_data[field_name])\n",
    "    vocab.append(vec[i].vocabulary_)\n",
    "    vocab[i]['  '] = len(vocab)\n",
    "    vocab[i]['oov'] = len(vocab)\n",
    "\n",
    "    tok = lambda voc, toks: [voc[i].get(t,default) for t in toks]\n",
    "\n",
    "    all_data[tok_int_field_name] = [tok(vocab[i], j) for j in all_data[tok_field_name]]\n",
    "\n",
    "    all_data[tok_int_field_name][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and run Keras LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  import sys\n",
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \n",
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:  (31428, 131)\n",
      "y_train:  (31428, 1)\n",
      "x_test:  (6734, 131)\n",
      "y_test:  (6734, 1)\n",
      "Epoch 1/2\n",
      "31428/31428 [==============================] - 334s 11ms/step - loss: 0.2326 - acc: 0.9011\n",
      "Epoch 2/2\n",
      "31428/31428 [==============================] - 331s 11ms/step - loss: 0.1178 - acc: 0.9626\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 6734 arrays: [array([[1659],\n       [1056],\n       [1097],\n       [1199],\n       [ 893],\n       [  45],\n       [1546],\n       [1052],\n       [ 893],\n       [  48],\n       [1665],\n       [1353],\n       [1452],\n    ...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-8ac4315dd570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 6734 arrays: [array([[1659],\n       [1056],\n       [1097],\n       [1199],\n       [ 893],\n       [  45],\n       [1546],\n       [1052],\n       [ 893],\n       [  48],\n       [1665],\n       [1353],\n       [1452],\n    ..."
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "all_data_train, all_data_dev, all_data_test = train_validate_test_split(all_data, train_percent = .7, validate_percent=.15)\n",
    "\n",
    "x_train = np.array(list(all_data_train[tok_int_field_name]))\n",
    "print(\"x_train: \", x_train.shape)\n",
    "y_train = np.reshape(list(all_data_train['binary_target']),[-1,1])\n",
    "print(\"y_train: \", y_train.shape)\n",
    "x_test = np.array(list(all_data_dev[tok_int_field_name]))\n",
    "print(\"x_test: \", x_test.shape)\n",
    "y_test = np.reshape(list(all_data_dev['binart_target']),[-1,1])\n",
    "print(\"y_test: \", y_test.shape)\n",
    "\n",
    "max_features = len(vocab)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, output_dim=256))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=16, epochs=5)\n",
    "score = model.evaluate(list(x_test), list(y_test), batch_size=16)\n",
    "\n",
    "\n",
    "# define model\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(75, input_shape=(X.shape[1], X.shape[2])))\n",
    "# model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1921,\n",
       "  1351,\n",
       "  1385,\n",
       "  921,\n",
       "  1270,\n",
       "  38,\n",
       "  1250,\n",
       "  1099,\n",
       "  1290,\n",
       "  865,\n",
       "  911,\n",
       "  890,\n",
       "  1935,\n",
       "  37,\n",
       "  1203,\n",
       "  1087,\n",
       "  781,\n",
       "  925,\n",
       "  1441,\n",
       "  743,\n",
       "  1213,\n",
       "  1473,\n",
       "  27,\n",
       "  777,\n",
       "  745,\n",
       "  1270,\n",
       "  45,\n",
       "  1556,\n",
       "  1461,\n",
       "  1594,\n",
       "  1256,\n",
       "  1374,\n",
       "  312,\n",
       "  1513,\n",
       "  1597,\n",
       "  1395,\n",
       "  1394,\n",
       "  1353,\n",
       "  1460,\n",
       "  1547,\n",
       "  1099,\n",
       "  1293,\n",
       "  984,\n",
       "  26,\n",
       "  747,\n",
       "  1381,\n",
       "  749,\n",
       "  1460,\n",
       "  1551,\n",
       "  1246,\n",
       "  921,\n",
       "  1306,\n",
       "  1524,\n",
       "  33,\n",
       "  1067,\n",
       "  1595,\n",
       "  1306,\n",
       "  1543,\n",
       "  925,\n",
       "  1459,\n",
       "  1473,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015],\n",
       " [1061,\n",
       "  1358,\n",
       "  1641,\n",
       "  45,\n",
       "  1546,\n",
       "  1052,\n",
       "  893,\n",
       "  28,\n",
       "  828,\n",
       "  1203,\n",
       "  1099,\n",
       "  1306,\n",
       "  1553,\n",
       "  1349,\n",
       "  1270,\n",
       "  31,\n",
       "  968,\n",
       "  1356,\n",
       "  1595,\n",
       "  1290,\n",
       "  861,\n",
       "  751,\n",
       "  1547,\n",
       "  1100,\n",
       "  1349,\n",
       "  1270,\n",
       "  43,\n",
       "  1449,\n",
       "  1101,\n",
       "  1395,\n",
       "  1385,\n",
       "  911,\n",
       "  846,\n",
       "  40,\n",
       "  1341,\n",
       "  961,\n",
       "  941,\n",
       "  33,\n",
       "  1048,\n",
       "  740,\n",
       "  1105,\n",
       "  1547,\n",
       "  1118,\n",
       "  1999,\n",
       "  1512,\n",
       "  1559,\n",
       "  1595,\n",
       "  1300,\n",
       "  1295,\n",
       "  1099,\n",
       "  1293,\n",
       "  984,\n",
       "  47,\n",
       "  1625,\n",
       "  1089,\n",
       "  865,\n",
       "  922,\n",
       "  1321,\n",
       "  45,\n",
       "  1546,\n",
       "  1048,\n",
       "  751,\n",
       "  1524,\n",
       "  30,\n",
       "  931,\n",
       "  1698,\n",
       "  1394,\n",
       "  1354,\n",
       "  1497,\n",
       "  926,\n",
       "  1473,\n",
       "  26,\n",
       "  744,\n",
       "  1246,\n",
       "  925,\n",
       "  1449,\n",
       "  1088,\n",
       "  817,\n",
       "  760,\n",
       "  1892,\n",
       "  1473,\n",
       "  38,\n",
       "  1255,\n",
       "  1354,\n",
       "  1512,\n",
       "  1524,\n",
       "  44,\n",
       "  1497,\n",
       "  919,\n",
       "  1200,\n",
       "  963,\n",
       "  1104,\n",
       "  1500,\n",
       "  1032,\n",
       "  28,\n",
       "  831,\n",
       "  1356,\n",
       "  1597,\n",
       "  1391,\n",
       "  1199,\n",
       "  893,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015],\n",
       " [865,\n",
       "  926,\n",
       "  1508,\n",
       "  1389,\n",
       "  1105,\n",
       "  1543,\n",
       "  893,\n",
       "  29,\n",
       "  865,\n",
       "  925,\n",
       "  1449,\n",
       "  1104,\n",
       "  1501,\n",
       "  1100,\n",
       "  1349,\n",
       "  1277,\n",
       "  262,\n",
       "  27,\n",
       "  793,\n",
       "  1449,\n",
       "  1105,\n",
       "  1539,\n",
       "  740,\n",
       "  1099,\n",
       "  1273,\n",
       "  185,\n",
       "  1473,\n",
       "  41,\n",
       "  1392,\n",
       "  1227,\n",
       "  38,\n",
       "  1242,\n",
       "  756,\n",
       "  1709,\n",
       "  38,\n",
       "  1250,\n",
       "  1092,\n",
       "  1010,\n",
       "  1066,\n",
       "  1524,\n",
       "  48,\n",
       "  1656,\n",
       "  919,\n",
       "  1206,\n",
       "  1182,\n",
       "  27,\n",
       "  781,\n",
       "  893,\n",
       "  26,\n",
       "  733,\n",
       "  787,\n",
       "  1199,\n",
       "  893,\n",
       "  45,\n",
       "  1553,\n",
       "  1321,\n",
       "  28,\n",
       "  817,\n",
       "  749,\n",
       "  1458,\n",
       "  1465,\n",
       "  1709,\n",
       "  40,\n",
       "  1349,\n",
       "  1279,\n",
       "  325,\n",
       "  325,\n",
       "  320,\n",
       "  31,\n",
       "  968,\n",
       "  1353,\n",
       "  1424,\n",
       "  39,\n",
       "  1301,\n",
       "  1358,\n",
       "  1641,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015],\n",
       " [793,\n",
       "  1449,\n",
       "  1104,\n",
       "  1512,\n",
       "  1553,\n",
       "  1347,\n",
       "  1182,\n",
       "  41,\n",
       "  1381,\n",
       "  743,\n",
       "  1203,\n",
       "  1099,\n",
       "  1270,\n",
       "  28,\n",
       "  824,\n",
       "  1052,\n",
       "  912,\n",
       "  925,\n",
       "  1459,\n",
       "  1473,\n",
       "  31,\n",
       "  968,\n",
       "  1353,\n",
       "  1424,\n",
       "  28,\n",
       "  824,\n",
       "  1064,\n",
       "  1449,\n",
       "  1104,\n",
       "  1512,\n",
       "  1547,\n",
       "  1086,\n",
       "  745,\n",
       "  1270,\n",
       "  44,\n",
       "  1512,\n",
       "  1559,\n",
       "  1585,\n",
       "  865,\n",
       "  921,\n",
       "  1306,\n",
       "  1557,\n",
       "  1473,\n",
       "  31,\n",
       "  968,\n",
       "  1353,\n",
       "  1443,\n",
       "  825,\n",
       "  1099,\n",
       "  1293,\n",
       "  984,\n",
       "  45,\n",
       "  1546,\n",
       "  1052,\n",
       "  916,\n",
       "  1103,\n",
       "  1424,\n",
       "  43,\n",
       "  1445,\n",
       "  919,\n",
       "  1203,\n",
       "  1092,\n",
       "  1011,\n",
       "  1100,\n",
       "  1349,\n",
       "  1270,\n",
       "  46,\n",
       "  1597,\n",
       "  1394,\n",
       "  1349,\n",
       "  1270,\n",
       "  26,\n",
       "  751,\n",
       "  1546,\n",
       "  1052,\n",
       "  916,\n",
       "  1104,\n",
       "  1512,\n",
       "  1557,\n",
       "  1473,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015],\n",
       " [828,\n",
       "  1203,\n",
       "  1099,\n",
       "  1306,\n",
       "  1553,\n",
       "  1349,\n",
       "  1277,\n",
       "  262,\n",
       "  34,\n",
       "  1099,\n",
       "  1270,\n",
       "  27,\n",
       "  790,\n",
       "  1350,\n",
       "  1346,\n",
       "  1149,\n",
       "  262,\n",
       "  44,\n",
       "  1493,\n",
       "  756,\n",
       "  1741,\n",
       "  1473,\n",
       "  45,\n",
       "  1556,\n",
       "  1461,\n",
       "  1594,\n",
       "  1256,\n",
       "  1370,\n",
       "  185,\n",
       "  1473,\n",
       "  29,\n",
       "  865,\n",
       "  909,\n",
       "  777,\n",
       "  751,\n",
       "  1543,\n",
       "  893,\n",
       "  44,\n",
       "  1512,\n",
       "  1539,\n",
       "  743,\n",
       "  1205,\n",
       "  1163,\n",
       "  1099,\n",
       "  1293,\n",
       "  984,\n",
       "  38,\n",
       "  1242,\n",
       "  735,\n",
       "  865,\n",
       "  893,\n",
       "  33,\n",
       "  1052,\n",
       "  925,\n",
       "  1424,\n",
       "  44,\n",
       "  1503,\n",
       "  1163,\n",
       "  1099,\n",
       "  1270,\n",
       "  28,\n",
       "  834,\n",
       "  1441,\n",
       "  754,\n",
       "  1662,\n",
       "  1182,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015],\n",
       " [1556,\n",
       "  1461,\n",
       "  1594,\n",
       "  1256,\n",
       "  1367,\n",
       "  45,\n",
       "  1556,\n",
       "  1445,\n",
       "  908,\n",
       "  750,\n",
       "  1513,\n",
       "  1599,\n",
       "  1465,\n",
       "  1709,\n",
       "  39,\n",
       "  1301,\n",
       "  1348,\n",
       "  1250,\n",
       "  1099,\n",
       "  1291,\n",
       "  912,\n",
       "  893,\n",
       "  48,\n",
       "  1652,\n",
       "  745,\n",
       "  1306,\n",
       "  1557,\n",
       "  1473,\n",
       "  45,\n",
       "  1553,\n",
       "  1321,\n",
       "  37,\n",
       "  1209,\n",
       "  1350,\n",
       "  1354,\n",
       "  1497,\n",
       "  921,\n",
       "  1270,\n",
       "  37,\n",
       "  1203,\n",
       "  1098,\n",
       "  1250,\n",
       "  1105,\n",
       "  1557,\n",
       "  1473,\n",
       "  46,\n",
       "  1595,\n",
       "  1290,\n",
       "  865,\n",
       "  925,\n",
       "  1424,\n",
       "  47,\n",
       "  1628,\n",
       "  1347,\n",
       "  1197,\n",
       "  827,\n",
       "  1159,\n",
       "  925,\n",
       "  1424,\n",
       "  43,\n",
       "  1461,\n",
       "  1593,\n",
       "  1199,\n",
       "  904,\n",
       "  649,\n",
       "  29,\n",
       "  875,\n",
       "  1338,\n",
       "  837,\n",
       "  1594,\n",
       "  1246,\n",
       "  921,\n",
       "  1306,\n",
       "  1524,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015],\n",
       " [1017,\n",
       "  1347,\n",
       "  1200,\n",
       "  941,\n",
       "  26,\n",
       "  735,\n",
       "  864,\n",
       "  869,\n",
       "  1088,\n",
       "  836,\n",
       "  1543,\n",
       "  911,\n",
       "  846,\n",
       "  18,\n",
       "  590,\n",
       "  398,\n",
       "  318,\n",
       "  1729,\n",
       "  908,\n",
       "  749,\n",
       "  1432,\n",
       "  308,\n",
       "  1347,\n",
       "  1198,\n",
       "  846,\n",
       "  38,\n",
       "  1242,\n",
       "  745,\n",
       "  1270,\n",
       "  44,\n",
       "  1493,\n",
       "  756,\n",
       "  1741,\n",
       "  1473,\n",
       "  33,\n",
       "  1052,\n",
       "  893,\n",
       "  57,\n",
       "  1844,\n",
       "  831,\n",
       "  1356,\n",
       "  1593,\n",
       "  1198,\n",
       "  874,\n",
       "  1317,\n",
       "  1893,\n",
       "  1524,\n",
       "  28,\n",
       "  817,\n",
       "  749,\n",
       "  1445,\n",
       "  893,\n",
       "  37,\n",
       "  1199,\n",
       "  926,\n",
       "  1511,\n",
       "  1473,\n",
       "  26,\n",
       "  733,\n",
       "  790,\n",
       "  1356,\n",
       "  1601,\n",
       "  1524,\n",
       "  32,\n",
       "  1017,\n",
       "  1347,\n",
       "  1200,\n",
       "  981,\n",
       "  1867,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015],\n",
       " [1513,\n",
       "  1590,\n",
       "  1088,\n",
       "  825,\n",
       "  1089,\n",
       "  865,\n",
       "  893,\n",
       "  26,\n",
       "  751,\n",
       "  1558,\n",
       "  1539,\n",
       "  734,\n",
       "  827,\n",
       "  1143,\n",
       "  40,\n",
       "  1349,\n",
       "  1270,\n",
       "  36,\n",
       "  1155,\n",
       "  733,\n",
       "  796,\n",
       "  1593,\n",
       "  1182,\n",
       "  44,\n",
       "  1500,\n",
       "  1056,\n",
       "  1076,\n",
       "  175,\n",
       "  1105,\n",
       "  1543,\n",
       "  893,\n",
       "  38,\n",
       "  1255,\n",
       "  1354,\n",
       "  1509,\n",
       "  1420,\n",
       "  1586,\n",
       "  893,\n",
       "  36,\n",
       "  1163,\n",
       "  1097,\n",
       "  1206,\n",
       "  1213,\n",
       "  1473,\n",
       "  26,\n",
       "  751,\n",
       "  1524,\n",
       "  37,\n",
       "  1199,\n",
       "  908,\n",
       "  750,\n",
       "  1512,\n",
       "  1524,\n",
       "  14,\n",
       "  483,\n",
       "  391,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015],\n",
       " [1556,\n",
       "  1461,\n",
       "  1594,\n",
       "  1256,\n",
       "  1367,\n",
       "  43,\n",
       "  1445,\n",
       "  908,\n",
       "  734,\n",
       "  824,\n",
       "  1052,\n",
       "  926,\n",
       "  1473,\n",
       "  40,\n",
       "  1356,\n",
       "  1601,\n",
       "  1524,\n",
       "  45,\n",
       "  1553,\n",
       "  1321,\n",
       "  37,\n",
       "  1195,\n",
       "  754,\n",
       "  1663,\n",
       "  1242,\n",
       "  742,\n",
       "  1159,\n",
       "  925,\n",
       "  1459,\n",
       "  1473,\n",
       "  40,\n",
       "  1349,\n",
       "  1270,\n",
       "  33,\n",
       "  1052,\n",
       "  908,\n",
       "  743,\n",
       "  1214,\n",
       "  1546,\n",
       "  1050,\n",
       "  817,\n",
       "  749,\n",
       "  1445,\n",
       "  893,\n",
       "  26,\n",
       "  750,\n",
       "  1473,\n",
       "  26,\n",
       "  745,\n",
       "  1301,\n",
       "  1355,\n",
       "  1546,\n",
       "  1052,\n",
       "  925,\n",
       "  1424,\n",
       "  44,\n",
       "  1493,\n",
       "  756,\n",
       "  1741,\n",
       "  1473,\n",
       "  4,\n",
       "  180,\n",
       "  1301,\n",
       "  1324,\n",
       "  159,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015],\n",
       " [1056,\n",
       "  1097,\n",
       "  1195,\n",
       "  749,\n",
       "  1449,\n",
       "  1100,\n",
       "  1356,\n",
       "  1600,\n",
       "  1474,\n",
       "  64,\n",
       "  37,\n",
       "  1209,\n",
       "  1350,\n",
       "  1346,\n",
       "  1143,\n",
       "  48,\n",
       "  1659,\n",
       "  1061,\n",
       "  1321,\n",
       "  37,\n",
       "  1203,\n",
       "  1087,\n",
       "  781,\n",
       "  925,\n",
       "  1441,\n",
       "  743,\n",
       "  1182,\n",
       "  38,\n",
       "  1250,\n",
       "  1089,\n",
       "  864,\n",
       "  872,\n",
       "  1199,\n",
       "  909,\n",
       "  796,\n",
       "  1599,\n",
       "  1465,\n",
       "  1709,\n",
       "  41,\n",
       "  1396,\n",
       "  1455,\n",
       "  1341,\n",
       "  960,\n",
       "  926,\n",
       "  1511,\n",
       "  1507,\n",
       "  1353,\n",
       "  1424,\n",
       "  34,\n",
       "  1104,\n",
       "  1473,\n",
       "  27,\n",
       "  787,\n",
       "  1195,\n",
       "  744,\n",
       "  1250,\n",
       "  1099,\n",
       "  1293,\n",
       "  984,\n",
       "  26,\n",
       "  737,\n",
       "  972,\n",
       "  1543,\n",
       "  925,\n",
       "  1424,\n",
       "  44,\n",
       "  1500,\n",
       "  1052,\n",
       "  893,\n",
       "  48,\n",
       "  1652,\n",
       "  750,\n",
       "  1473,\n",
       "  44,\n",
       "  1497,\n",
       "  921,\n",
       "  1306,\n",
       "  1524,\n",
       "  45,\n",
       "  1553,\n",
       "  1321,\n",
       "  33,\n",
       "  1061,\n",
       "  1354,\n",
       "  1508,\n",
       "  1389,\n",
       "  1105,\n",
       "  1539,\n",
       "  743,\n",
       "  1182,\n",
       "  27,\n",
       "  799,\n",
       "  1709,\n",
       "  26,\n",
       "  745,\n",
       "  1293,\n",
       "  1020,\n",
       "  1465,\n",
       "  1709,\n",
       "  38,\n",
       "  1255,\n",
       "  1337,\n",
       "  763,\n",
       "  40,\n",
       "  1341,\n",
       "  941,\n",
       "  37,\n",
       "  1199,\n",
       "  913,\n",
       "  972,\n",
       "  1547,\n",
       "  1104,\n",
       "  1512,\n",
       "  1524,\n",
       "  44,\n",
       "  1512,\n",
       "  1559,\n",
       "  1585,\n",
       "  865,\n",
       "  921,\n",
       "  1306,\n",
       "  1557,\n",
       "  1473,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015,\n",
       "  2015]]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(all_data_dev[tok_int_field_name][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6734/6734 [==============================] - 15s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# x_train = np.array(list(all_data_train[tok_int_field_name]))\n",
    "# print(\"x_train: \", x_train.shape)\n",
    "# y_train = np.array(list(all_data_train['target']))\n",
    "# print(\"y_train: \", y_train.shape)\n",
    "# x_test = np.array(list(all_data_dev[tok_int_field_name]))\n",
    "# print(\"x_test: \", x_test.shape)\n",
    "# y_test = np.reshape(list(all_data_dev['target']),[1,-1])\n",
    "# print(\"y_test: \", y_test.shape)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.10409376217936916, 0.9642114642291668]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
