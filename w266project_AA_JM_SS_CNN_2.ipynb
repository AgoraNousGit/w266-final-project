{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIAR DETECTION GROUP PROJECT - CNN Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONTENTS  \n",
    "\n",
    "Imports  \n",
    "Load ISOT data from appropriate pickle file  \n",
    "Load ISOT vocabulary from pickle file  (note: vocab contains both \"title\" and \"text\" words)  \n",
    "Train/Dev/Test split ISOT data  \n",
    "Load LIAR data (for evaluating models)  \n",
    "\n",
    "#### Neural BOW Models:\n",
    "- Model_1: Initial run replicating settings from Assignment 2, but with ISOT \"title\" data.  \n",
    "- Model_2: Use GloVe word embeddings rather than initializing embeddings with uniform random numbers.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "from importlib import reload\n",
    "import os, sys, re, json, time, datetime, shutil\n",
    "import itertools, collections\n",
    "from functools import reduce\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.utils import shuffle\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "#assert(tf.__version__.startswith(\"1.8\"))\n",
    "\n",
    "import pickle\n",
    "import dill\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary, tf_embed_viz\n",
    "from w266_common import patched_numpy_io\n",
    "import timeit  #For timing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.10.1\n"
     ]
    }
   ],
   "source": [
    "print('TensorFlow version:', tf.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ISOT data and vocabulary from pickle files  \n",
    "Loading the dataset from the Information security and object technology (ISOT) Research lab at the University of Victoria School of Engineering.\n",
    "\n",
    "The ISOT Fake News Dataset is a compilation of several thousands fake news and truthful articles, obtained from different legitimate news sites and sites flagged as unreliable by politifact.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read ISOT data from pickle file.\n",
    "#all_data = pd.read_pickle('parsed_data/df_alldata2.pkl')  # ISOT data (CMU) tokenized and POS tags added\n",
    "\n",
    "# NOTE: for models 2+, read in the pickle file that includes the GloVe embeddings\n",
    "all_data = pd.read_pickle('parsed_data/df_alldata_embed.pkl')  # GloVe embeddings for ISOT title and text tokens\n",
    "\n",
    "#all_data.info(memory_usage='deep', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>target</th>\n",
       "      <th>title_tokcan</th>\n",
       "      <th>title_POS</th>\n",
       "      <th>text_tokcan</th>\n",
       "      <th>text_POS</th>\n",
       "      <th>embedded_title</th>\n",
       "      <th>embedded_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BRAINIAC Gets Rejected After Trying To Buy BMW...</td>\n",
       "      <td>Does anyone else out there see a future BMW ca...</td>\n",
       "      <td>Government News</td>\n",
       "      <td>Mar 20, 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>[brainiac&lt;allcaps&gt;, gets, rejected, after, try...</td>\n",
       "      <td>[N, V, V, P, V, P, V, ^, P, ^, ^, ,, O, V, A, ...</td>\n",
       "      <td>[does, anyone, else, out, there, see, a, futur...</td>\n",
       "      <td>[V, N, R, P, R, V, D, A, ^, N, N, P, D, N, ,, ...</td>\n",
       "      <td>[[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...</td>\n",
       "      <td>[[0.2293, 0.34231, 0.059817, 0.083003, 0.57685...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Windows 10 is Stealing Your Bandwidth (You Mig...</td>\n",
       "      <td>21st Century Wire says We ve heard a lot of no...</td>\n",
       "      <td>US_News</td>\n",
       "      <td>April 7, 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>[windows, &lt;number&gt;, is, stealing, your, bandwi...</td>\n",
       "      <td>[^, $, V, V, D, N, ,, O, V, V, P, V, O, ,]</td>\n",
       "      <td>[&lt;number&gt;st, century, wire, says, we, ve, hear...</td>\n",
       "      <td>[A, N, ^, V, O, V, V, D, N, P, R, R, A, N, P, ...</td>\n",
       "      <td>[[0.80482, -0.39569, 1.4741, 0.39562, 0.025004...</td>\n",
       "      <td>[[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>STUNNING STORY The Media And Democrats Hid Fro...</td>\n",
       "      <td>In an email sent on April 15, 2011, our upstan...</td>\n",
       "      <td>left-news</td>\n",
       "      <td>Mar 2, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>[stunning&lt;allcaps&gt;, story&lt;allcaps&gt;, the, media...</td>\n",
       "      <td>[A, N, D, N, &amp;, N, V, P, ^, ,, R, Z, ^, ^, N, ...</td>\n",
       "      <td>[in, an, email, sent, on, april, &lt;number&gt;, ,, ...</td>\n",
       "      <td>[P, D, N, V, P, ^, $, ,, $, ,, D, A, N, A, ^, ...</td>\n",
       "      <td>[[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...</td>\n",
       "      <td>[[0.33042, 0.24995, -0.60874, 0.10923, 0.03637...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>North Korea's Kim Jong Un fetes nuclear scient...</td>\n",
       "      <td>SEOUL (Reuters) - North Korean leader Kim Jong...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>September 10, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>[north, korea's, kim, jong, un, fetes, nuclear...</td>\n",
       "      <td>[^, Z, ^, ^, ^, V, A, N, ,, V, N, N]</td>\n",
       "      <td>[seoul&lt;allcaps&gt;, (, reuters, ), -, north, kore...</td>\n",
       "      <td>[^, ,, ^, ,, ,, ^, ^, N, ^, ^, ^, V, D, A, N, ...</td>\n",
       "      <td>[[0.30059, 0.55598, -0.040589, 0.020289, -0.57...</td>\n",
       "      <td>[[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>White House developing comprehensive biosecuri...</td>\n",
       "      <td>ASPEN, Colorado (Reuters) - The Trump administ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>July 20, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>[white, house, developing, comprehensive, bios...</td>\n",
       "      <td>[A, N, V, A, N, N, ,, A]</td>\n",
       "      <td>[aspen&lt;allcaps&gt;, ,, colorado, (, reuters, ), -...</td>\n",
       "      <td>[^, ,, ^, ,, ^, ,, ,, D, ^, N, V, V, D, A, A, ...</td>\n",
       "      <td>[[-0.68652, 0.80125, -0.6124, -0.1512, 0.997, ...</td>\n",
       "      <td>[[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  BRAINIAC Gets Rejected After Trying To Buy BMW...   \n",
       "1  Windows 10 is Stealing Your Bandwidth (You Mig...   \n",
       "2  STUNNING STORY The Media And Democrats Hid Fro...   \n",
       "3  North Korea's Kim Jong Un fetes nuclear scient...   \n",
       "4  White House developing comprehensive biosecuri...   \n",
       "\n",
       "                                                text          subject  \\\n",
       "0  Does anyone else out there see a future BMW ca...  Government News   \n",
       "1  21st Century Wire says We ve heard a lot of no...          US_News   \n",
       "2  In an email sent on April 15, 2011, our upstan...        left-news   \n",
       "3  SEOUL (Reuters) - North Korean leader Kim Jong...        worldnews   \n",
       "4  ASPEN, Colorado (Reuters) - The Trump administ...     politicsNews   \n",
       "\n",
       "                  date target  \\\n",
       "0         Mar 20, 2016      0   \n",
       "1        April 7, 2016      0   \n",
       "2          Mar 2, 2017      0   \n",
       "3  September 10, 2017       1   \n",
       "4       July 20, 2017       1   \n",
       "\n",
       "                                        title_tokcan  \\\n",
       "0  [brainiac<allcaps>, gets, rejected, after, try...   \n",
       "1  [windows, <number>, is, stealing, your, bandwi...   \n",
       "2  [stunning<allcaps>, story<allcaps>, the, media...   \n",
       "3  [north, korea's, kim, jong, un, fetes, nuclear...   \n",
       "4  [white, house, developing, comprehensive, bios...   \n",
       "\n",
       "                                           title_POS  \\\n",
       "0  [N, V, V, P, V, P, V, ^, P, ^, ^, ,, O, V, A, ...   \n",
       "1         [^, $, V, V, D, N, ,, O, V, V, P, V, O, ,]   \n",
       "2  [A, N, D, N, &, N, V, P, ^, ,, R, Z, ^, ^, N, ...   \n",
       "3               [^, Z, ^, ^, ^, V, A, N, ,, V, N, N]   \n",
       "4                           [A, N, V, A, N, N, ,, A]   \n",
       "\n",
       "                                         text_tokcan  \\\n",
       "0  [does, anyone, else, out, there, see, a, futur...   \n",
       "1  [<number>st, century, wire, says, we, ve, hear...   \n",
       "2  [in, an, email, sent, on, april, <number>, ,, ...   \n",
       "3  [seoul<allcaps>, (, reuters, ), -, north, kore...   \n",
       "4  [aspen<allcaps>, ,, colorado, (, reuters, ), -...   \n",
       "\n",
       "                                            text_POS  \\\n",
       "0  [V, N, R, P, R, V, D, A, ^, N, N, P, D, N, ,, ...   \n",
       "1  [A, N, ^, V, O, V, V, D, N, P, R, R, A, N, P, ...   \n",
       "2  [P, D, N, V, P, ^, $, ,, $, ,, D, A, N, A, ^, ...   \n",
       "3  [^, ,, ^, ,, ,, ^, ^, N, ^, ^, ^, V, D, A, N, ...   \n",
       "4  [^, ,, ^, ,, ^, ,, ,, D, ^, N, V, V, D, A, A, ...   \n",
       "\n",
       "                                      embedded_title  \\\n",
       "0  [[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...   \n",
       "1  [[0.80482, -0.39569, 1.4741, 0.39562, 0.025004...   \n",
       "2  [[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...   \n",
       "3  [[0.30059, 0.55598, -0.040589, 0.020289, -0.57...   \n",
       "4  [[-0.68652, 0.80125, -0.6124, -0.1512, 0.997, ...   \n",
       "\n",
       "                                       embedded_text  \n",
       "0  [[0.2293, 0.34231, 0.059817, 0.083003, 0.57685...  \n",
       "1  [[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...  \n",
       "2  [[0.33042, 0.24995, -0.60874, 0.10923, 0.03637...  \n",
       "3  [[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...  \n",
       "4  [[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BRAINIAC Gets Rejected After Trying To Buy BMW With EBT Card…What Happens Next Is HYSTERICAL!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.title[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brainiac<allcaps>',\n",
       " 'gets',\n",
       " 'rejected',\n",
       " 'after',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'bmw<allcaps>',\n",
       " 'with',\n",
       " 'ebt<allcaps>',\n",
       " 'card',\n",
       " '…',\n",
       " 'what',\n",
       " 'happens',\n",
       " 'next',\n",
       " 'is',\n",
       " 'hysterical<allcaps>',\n",
       " '!']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.title_tokcan[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of all_data: 44898\n",
      "shape of all_data: (44898, 11)\n",
      "length of embedded title: 44898\n",
      "shape of embedded title: (44898,)\n",
      "dimension of single embedded title: 18\n",
      "dimension of single embedded title: 14\n",
      "dimension of single embedded title: 50\n",
      "example of a title embedding:\n",
      " [[-7.9149e-01  8.6617e-01  1.1998e-01  9.2287e-04  2.7760e-01 -4.9185e-01\n",
      "   5.0195e-01  6.0792e-04 -2.5845e-01  1.7865e-01  2.5350e-01  7.6572e-01\n",
      "   5.0664e-01  4.0250e-01 -2.1388e-03 -2.8397e-01 -5.0324e-01  3.0449e-01\n",
      "   5.1779e-01  1.5090e-02 -3.5031e-01 -1.1278e+00  3.3253e-01 -3.5250e-01\n",
      "   4.1326e-02  1.0863e+00  3.3910e-02  3.3564e-01  4.9745e-01 -7.0131e-02\n",
      "  -1.2192e+00 -4.8512e-01 -3.8512e-02 -1.3554e-01 -1.6380e-01  5.2321e-01\n",
      "  -3.1318e-01 -1.6550e-01  1.1909e-01 -1.5115e-01 -1.5621e-01 -6.2655e-01\n",
      "  -6.2336e-01 -4.2150e-01  4.1873e-01 -9.2472e-01  1.1049e+00 -2.9996e-01\n",
      "  -6.3003e-03  3.9540e-01]\n",
      " [-7.2202e-02 -3.8312e-02  3.5678e-01 -1.8762e-01  6.1088e-01  2.3631e-01\n",
      "  -4.3833e-01  4.1290e-01  2.5682e-01  3.3728e-01 -1.5096e-01  2.5277e-01\n",
      "  -2.2161e-01  1.9157e-01  1.1020e+00  2.5557e-01  1.2585e-01  6.4586e-01\n",
      "  -1.1285e-01 -4.6857e-01 -2.4584e-01  5.1331e-01  2.6720e-01  4.7869e-01\n",
      "   5.7446e-01 -1.9802e+00  5.0118e-02  7.3693e-01  5.4434e-01 -3.9787e-01\n",
      "   2.4670e+00  4.5542e-01  1.8973e-02  2.8066e-01 -6.2605e-02  7.1148e-02\n",
      "  -1.0552e-01  2.3076e-01  4.7146e-01 -1.2256e+00 -8.7966e-02  5.7726e-01\n",
      "  -2.4806e-01  9.8880e-02 -3.8841e-01 -1.5556e-01  2.3919e-01 -3.0419e-01\n",
      "   1.2098e-01  5.9627e-01]\n",
      " [ 2.2426e-01 -7.4140e-01 -2.3476e-01  4.6580e-02  2.1730e-02  6.1725e-01\n",
      "  -3.1005e-01  3.1925e-01 -7.5937e-01  1.8388e-01  7.7406e-03 -1.2780e-01\n",
      "  -2.6909e-01 -4.5236e-01  2.7808e-01  5.0401e-01  1.1270e-01 -8.9587e-01\n",
      "   5.6290e-01 -3.2498e-01  3.6629e-01  3.0937e-01 -7.0191e-02 -2.9065e-01\n",
      "  -2.6496e-01 -1.9891e+00  4.2419e-01  2.5204e-02 -1.0215e+00  1.0292e-01\n",
      "   2.0998e+00 -8.3131e-01 -1.2163e+00 -4.0155e-01 -9.0138e-02 -1.2048e+00\n",
      "   5.6457e-01 -2.7167e-01 -7.2691e-01 -1.9167e-01  1.4795e-01  1.3526e-01\n",
      "   9.6795e-02  6.6091e-02 -3.7050e-01  7.3678e-02 -1.2934e+00  1.2381e+00\n",
      "   3.3281e-02 -2.5800e-01]\n",
      " [ 3.8315e-01 -3.5610e-01 -1.2830e-01 -1.9527e-01  4.7629e-02  2.1468e-01\n",
      "  -9.8765e-01  8.2962e-01 -4.2782e-01 -2.2879e-01  1.0712e-01 -3.0870e-01\n",
      "  -1.2069e+00 -1.7713e-01  8.8841e-01  5.6658e-03 -7.7305e-01 -6.6913e-01\n",
      "  -1.3384e+00  3.4676e-01  5.0440e-01  5.1250e-01  2.6826e-01 -6.5313e-01\n",
      "  -8.1516e-02 -2.1658e+00  5.7974e-01  3.6345e-02  9.0949e-03  2.5772e-01\n",
      "   3.4402e+00  2.0732e-01 -5.2028e-01  2.6453e-02  1.7895e-01 -1.7802e-02\n",
      "   3.6605e-01  3.4539e-01  4.1357e-01 -2.4970e-01 -4.9227e-01  1.7745e-01\n",
      "  -4.3764e-01 -3.4840e-01 -5.7061e-02 -3.9578e-02 -1.3517e-01 -4.2580e-01\n",
      "   1.3681e-01 -7.7731e-01]\n",
      " [ 4.4349e-01 -7.1870e-01  7.4168e-01 -3.0663e-01  3.2147e-01  9.8841e-03\n",
      "  -5.7295e-01  7.1186e-01 -1.3162e-01  2.5001e-02 -3.3982e-01  2.4820e-01\n",
      "  -8.1255e-01  4.7738e-01  1.1965e-01  4.6931e-01  7.7249e-01 -3.2196e-01\n",
      "   2.9724e-01 -7.6281e-01  9.4952e-02  6.9615e-01  3.1000e-02  3.4311e-01\n",
      "   3.7160e-01 -2.4612e+00 -1.8062e-01 -4.4132e-01  9.1056e-01 -9.3270e-01\n",
      "   2.9290e+00  7.9259e-01 -1.1483e+00 -4.3050e-01 -3.3168e-02  2.2947e-01\n",
      "  -8.0059e-01 -9.0278e-02  2.0351e-01 -4.9679e-01 -3.6519e-01  2.2556e-01\n",
      "   3.7469e-01  1.7841e-01  6.1943e-01 -3.5550e-02 -6.7799e-02 -8.8973e-02\n",
      "   2.2376e-02 -1.9262e-01]\n",
      " [ 6.8047e-01 -3.9263e-02  3.0186e-01 -1.7792e-01  4.2962e-01  3.2246e-02\n",
      "  -4.1376e-01  1.3228e-01 -2.9847e-01 -8.5253e-02  1.7118e-01  2.2419e-01\n",
      "  -1.0046e-01 -4.3653e-01  3.3418e-01  6.7846e-01  5.7204e-02 -3.4448e-01\n",
      "  -4.2785e-01 -4.3275e-01  5.5963e-01  1.0032e-01  1.8677e-01 -2.6854e-01\n",
      "   3.7334e-02 -2.0932e+00  2.2171e-01 -3.9868e-01  2.0912e-01 -5.5725e-01\n",
      "   3.8826e+00  4.7466e-01 -9.5658e-01 -3.7788e-01  2.0869e-01 -3.2752e-01\n",
      "   1.2751e-01  8.8359e-02  1.6351e-01 -2.1634e-01 -9.4375e-02  1.8324e-02\n",
      "   2.1048e-01 -3.0880e-02 -1.9722e-01  8.2279e-02 -9.4340e-02 -7.3297e-02\n",
      "  -6.4699e-02 -2.6044e-01]\n",
      " [ 7.6552e-01 -1.2379e-01  1.1440e+00  7.1116e-02  6.9537e-01 -2.5318e-01\n",
      "  -1.4303e+00 -6.4846e-01 -1.7595e-01  6.0268e-01  1.0464e-01  9.6283e-01\n",
      "   3.5706e-01 -4.5109e-01  5.5292e-01  1.1588e+00 -1.3179e-01  3.4544e-01\n",
      "   1.8801e-02 -1.1188e+00  1.3580e+00 -1.1799e-01 -3.9478e-01 -1.1678e-01\n",
      "  -9.2234e-01 -1.4277e+00 -3.8724e-01 -1.5941e-01  7.8802e-01 -9.0019e-01\n",
      "   2.5643e+00  7.0629e-01 -1.3151e-01  1.1043e+00  5.1428e-01 -6.0614e-01\n",
      "  -5.1230e-01 -3.4909e-02  4.5906e-01 -9.7082e-01  7.6554e-01  5.2619e-03\n",
      "   1.0155e+00  2.7476e-01  2.8309e-01  2.3979e-01 -8.5404e-01  3.6266e-01\n",
      "  -1.7013e-01  6.4426e-01]\n",
      " [-7.9149e-01  8.6617e-01  1.1998e-01  9.2287e-04  2.7760e-01 -4.9185e-01\n",
      "   5.0195e-01  6.0792e-04 -2.5845e-01  1.7865e-01  2.5350e-01  7.6572e-01\n",
      "   5.0664e-01  4.0250e-01 -2.1388e-03 -2.8397e-01 -5.0324e-01  3.0449e-01\n",
      "   5.1779e-01  1.5090e-02 -3.5031e-01 -1.1278e+00  3.3253e-01 -3.5250e-01\n",
      "   4.1326e-02  1.0863e+00  3.3910e-02  3.3564e-01  4.9745e-01 -7.0131e-02\n",
      "  -1.2192e+00 -4.8512e-01 -3.8512e-02 -1.3554e-01 -1.6380e-01  5.2321e-01\n",
      "  -3.1318e-01 -1.6550e-01  1.1909e-01 -1.5115e-01 -1.5621e-01 -6.2655e-01\n",
      "  -6.2336e-01 -4.2150e-01  4.1873e-01 -9.2472e-01  1.1049e+00 -2.9996e-01\n",
      "  -6.3003e-03  3.9540e-01]\n",
      " [ 2.5616e-01  4.3694e-01 -1.1889e-01  2.0345e-01  4.1959e-01  8.5863e-01\n",
      "  -6.0344e-01 -3.1835e-01 -6.7180e-01  3.9840e-03 -7.5159e-02  1.1043e-01\n",
      "  -7.3534e-01  2.7436e-01  5.4015e-02 -2.3828e-01 -1.3767e-01  1.1573e-02\n",
      "  -4.6623e-01 -5.5233e-01  8.3317e-02  5.5938e-01  5.1903e-01 -2.7065e-01\n",
      "  -2.8211e-01 -1.3918e+00  1.7498e-01  2.6586e-01  6.1449e-02 -2.7300e-01\n",
      "   3.9032e+00  3.8169e-01 -5.6009e-02 -4.4250e-03  2.4033e-01  3.0675e-01\n",
      "  -1.2638e-01  3.3436e-01  7.5485e-02 -3.6218e-02  1.3691e-01  3.7762e-01\n",
      "  -1.2159e-01 -1.3808e-01  1.9505e-01  2.2793e-01 -1.7304e-01 -7.5730e-02\n",
      "  -2.5868e-01 -3.9339e-01]\n",
      " [-7.9149e-01  8.6617e-01  1.1998e-01  9.2287e-04  2.7760e-01 -4.9185e-01\n",
      "   5.0195e-01  6.0792e-04 -2.5845e-01  1.7865e-01  2.5350e-01  7.6572e-01\n",
      "   5.0664e-01  4.0250e-01 -2.1388e-03 -2.8397e-01 -5.0324e-01  3.0449e-01\n",
      "   5.1779e-01  1.5090e-02 -3.5031e-01 -1.1278e+00  3.3253e-01 -3.5250e-01\n",
      "   4.1326e-02  1.0863e+00  3.3910e-02  3.3564e-01  4.9745e-01 -7.0131e-02\n",
      "  -1.2192e+00 -4.8512e-01 -3.8512e-02 -1.3554e-01 -1.6380e-01  5.2321e-01\n",
      "  -3.1318e-01 -1.6550e-01  1.1909e-01 -1.5115e-01 -1.5621e-01 -6.2655e-01\n",
      "  -6.2336e-01 -4.2150e-01  4.1873e-01 -9.2472e-01  1.1049e+00 -2.9996e-01\n",
      "  -6.3003e-03  3.9540e-01]\n",
      " [-4.9282e-01  3.4231e-01  9.3662e-01  8.6695e-01  6.2261e-01  1.4615e-02\n",
      "  -6.2331e-01  2.1383e-02  2.5933e-01  2.3695e-01 -4.3837e-02  6.5615e-01\n",
      "  -1.2180e-01  1.8390e-01  7.2801e-01 -5.7085e-01 -9.9428e-01 -7.6187e-01\n",
      "   4.4481e-02 -6.6478e-01  3.5339e-01 -1.3720e+00 -7.0328e-01  3.4372e-01\n",
      "  -5.5958e-01 -1.5715e+00 -7.3208e-02 -6.2584e-01  2.0277e-01 -1.1796e+00\n",
      "   2.4812e+00  5.5142e-01  1.8541e-02  2.8690e-01  1.1410e-01  6.0408e-01\n",
      "   5.5863e-01  1.3701e-01 -3.3045e-01 -1.3547e+00  1.0943e+00  7.6710e-02\n",
      "  -2.5892e-01  7.9890e-01 -1.4411e-01 -3.4701e-01  3.1202e-01  1.8600e-01\n",
      "   8.5489e-01  2.5597e-02]\n",
      " [-2.9317e-01  4.0853e-01 -3.3864e-01 -5.1048e-01  7.0926e-01 -1.9915e-01\n",
      "   8.5861e-01 -5.7151e-02 -3.8268e-01  3.7696e-01  7.3189e-02  5.7895e-01\n",
      "  -1.7146e-01 -5.3171e-01  2.4612e-01 -2.6859e-01  4.9583e-01  3.8142e-01\n",
      "   9.8846e-02  7.3221e-02 -2.8310e-01  3.2308e-01  7.3795e-01  2.4775e-01\n",
      "   9.9444e-01 -1.6140e-01 -1.1411e+00 -2.7577e-01  7.7427e-01 -3.4591e-01\n",
      "   1.7546e+00 -1.0290e-03 -5.6660e-02 -2.6130e-01 -3.9687e-01  7.9826e-02\n",
      "   4.9462e-01 -1.2352e-01  6.2210e-02  1.1853e-01  3.8208e-01 -9.1746e-01\n",
      "  -9.9595e-03  1.6010e-01 -4.1623e-01  2.4994e-01  3.8409e-02 -3.3172e-01\n",
      "   1.9551e-01  3.5758e-01]\n",
      " [ 4.5323e-01  5.9811e-02 -1.0577e-01 -3.3300e-01  7.2359e-01 -8.7170e-02\n",
      "  -6.1053e-01 -3.7695e-02 -3.0945e-01  2.1805e-01 -4.3605e-01  4.7318e-01\n",
      "  -7.6866e-01 -2.7130e-01  1.1042e+00  5.9141e-01  5.6962e-01 -1.8678e-01\n",
      "   1.4867e-01 -6.7292e-01 -3.4672e-01  5.2284e-01  2.2959e-01 -7.2014e-02\n",
      "   9.3967e-01 -2.3985e+00 -1.3238e+00  2.8698e-01  7.5509e-01 -7.6522e-01\n",
      "   3.3425e+00  1.7233e-01 -5.1803e-01 -8.2970e-01 -2.9333e-01 -5.0076e-01\n",
      "  -1.5228e-01  9.8973e-02  1.8146e-01 -1.7420e-01 -4.0666e-01  2.0348e-01\n",
      "  -1.1788e-02  4.8252e-01  2.4598e-02  3.4064e-01 -8.4724e-02  5.3240e-01\n",
      "  -2.5103e-01  6.2546e-01]\n",
      " [ 4.0825e-01  1.3589e-01 -9.2040e-02  7.7916e-02  8.0876e-01 -1.1682e-01\n",
      "   1.7651e-01  5.7259e-01 -3.4809e-02  1.8086e-01 -1.3495e-01  2.3056e-01\n",
      "  -3.2223e-01  1.9001e-01  1.3714e+00  8.9084e-01  5.3573e-01  2.0547e-01\n",
      "  -1.1459e-01 -4.3327e-01 -1.0507e-01  5.9712e-02  2.1579e-01  7.5578e-01\n",
      "   1.1642e+00 -1.4020e+00 -8.0244e-01  2.9944e-01  1.2265e+00 -3.7954e-01\n",
      "   2.1982e+00  4.5418e-01 -3.4887e-01 -6.9931e-01 -3.3260e-01 -2.8976e-01\n",
      "  -5.9243e-02 -3.9974e-02  4.2430e-01 -2.7954e-01 -7.2751e-01  1.0190e-01\n",
      "  -2.4600e-01  4.8189e-01 -1.1966e-01  2.0752e-01  5.0721e-01 -8.1430e-03\n",
      "   1.6891e-02  9.7260e-01]\n",
      " [ 3.9119e-01  3.4992e-01  2.2498e-01  1.1417e-01  3.1087e-01 -5.2257e-01\n",
      "  -9.2955e-01  2.5335e-01 -8.9138e-02 -4.2545e-01 -3.1354e-01 -4.5698e-01\n",
      "  -6.4173e-01  3.1926e-01  1.0812e+00  6.1861e-01  2.8539e-02 -3.4201e-01\n",
      "  -6.8273e-01 -2.5298e-01  4.3399e-01 -2.6450e-01 -5.7452e-02  8.2656e-03\n",
      "   1.5807e-02 -1.5390e+00  2.8407e-01  1.4952e-01 -7.4397e-02 -1.8124e-01\n",
      "   3.5567e+00  6.3548e-01 -7.6356e-01  2.1495e-01  2.3488e-01 -3.0534e-01\n",
      "   4.6327e-01  6.4019e-01  7.7317e-02 -8.8417e-01 -7.1713e-01 -1.6503e-01\n",
      "  -2.1991e-01 -3.8886e-01 -2.6111e-01  5.8742e-01  2.9305e-01 -2.0466e-01\n",
      "  -5.9552e-03  1.7460e-01]\n",
      " [ 6.1850e-01  6.4254e-01 -4.6552e-01  3.7570e-01  7.4838e-01  5.3739e-01\n",
      "   2.2239e-03 -6.0577e-01  2.6408e-01  1.1703e-01  4.3722e-01  2.0092e-01\n",
      "  -5.7859e-02 -3.4589e-01  2.1664e-01  5.8573e-01  5.3919e-01  6.9490e-01\n",
      "  -1.5618e-01  5.5830e-02 -6.0515e-01 -2.8997e-01 -2.5594e-02  5.5593e-01\n",
      "   2.5356e-01 -1.9612e+00 -5.1381e-01  6.9096e-01  6.6246e-02 -5.4224e-02\n",
      "   3.7871e+00 -7.7403e-01 -1.2689e-01 -5.1465e-01  6.6705e-02 -3.2933e-01\n",
      "   1.3483e-01  1.9049e-01  1.3812e-01 -2.1503e-01 -1.6573e-02  3.1200e-01\n",
      "  -3.3189e-01 -2.6001e-02 -3.8203e-01  1.9403e-01 -1.2466e-01 -2.7557e-01\n",
      "   3.0899e-01  4.8497e-01]\n",
      " [-7.9149e-01  8.6617e-01  1.1998e-01  9.2287e-04  2.7760e-01 -4.9185e-01\n",
      "   5.0195e-01  6.0792e-04 -2.5845e-01  1.7865e-01  2.5350e-01  7.6572e-01\n",
      "   5.0664e-01  4.0250e-01 -2.1388e-03 -2.8397e-01 -5.0324e-01  3.0449e-01\n",
      "   5.1779e-01  1.5090e-02 -3.5031e-01 -1.1278e+00  3.3253e-01 -3.5250e-01\n",
      "   4.1326e-02  1.0863e+00  3.3910e-02  3.3564e-01  4.9745e-01 -7.0131e-02\n",
      "  -1.2192e+00 -4.8512e-01 -3.8512e-02 -1.3554e-01 -1.6380e-01  5.2321e-01\n",
      "  -3.1318e-01 -1.6550e-01  1.1909e-01 -1.5115e-01 -1.5621e-01 -6.2655e-01\n",
      "  -6.2336e-01 -4.2150e-01  4.1873e-01 -9.2472e-01  1.1049e+00 -2.9996e-01\n",
      "  -6.3003e-03  3.9540e-01]\n",
      " [-5.8402e-01  3.9031e-01  6.5282e-01 -3.4030e-01  1.9493e-01 -8.3489e-01\n",
      "   1.1929e-01 -5.7291e-01 -5.6844e-01  7.2989e-01 -5.6975e-01  5.3436e-01\n",
      "  -3.8034e-01  2.2471e-01  9.8031e-01 -2.9660e-01  1.2600e-01  5.5222e-01\n",
      "  -6.2737e-01 -8.2242e-02 -8.5359e-02  3.1515e-01  9.6077e-01  3.1986e-01\n",
      "   8.7878e-01 -1.5189e+00 -1.7831e+00  3.5639e-01  9.6740e-01 -1.5497e+00\n",
      "   2.3350e+00  8.4940e-01 -1.2371e+00  1.0623e+00 -1.4267e+00 -4.9056e-01\n",
      "   8.5465e-01 -1.2878e+00  6.0204e-01 -3.5963e-01  2.8586e-01 -5.2162e-02\n",
      "  -5.0818e-01 -6.3459e-01  3.3889e-01  2.8416e-01 -2.0340e-01 -1.2338e+00\n",
      "   4.6715e-01  7.8858e-01]]\n"
     ]
    }
   ],
   "source": [
    "print('length of all_data:', len(all_data))  # 44898\n",
    "print('shape of all_data:', all_data.shape)  # (44898, 11)\n",
    "print('length of embedded title:', len(all_data.embedded_title))  # 44898\n",
    "print('shape of embedded title:', all_data.embedded_title.shape)  # (44898,)\n",
    "print('dimension of single embedded title:', len(all_data.embedded_title[0]))  # 18\n",
    "print('dimension of single embedded title:', len(all_data.embedded_title[1]))  # 14\n",
    "print('dimension of single embedded title:', len(all_data.embedded_title[0][0]))  # 50\n",
    "print('example of a title embedding:\\n', all_data.embedded_title[0])\n",
    "#print('dimension of single embedded text:', len(all_data.embedded_text[0][0]))  # 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isot_title_embed shape: (44898,)\n",
      "isot_title_embed shape of first element: (18, 50)\n",
      "isot_title_embed example:\n",
      " [[-7.9149e-01  8.6617e-01  1.1998e-01  9.2287e-04  2.7760e-01 -4.9185e-01\n",
      "   5.0195e-01  6.0792e-04 -2.5845e-01  1.7865e-01  2.5350e-01  7.6572e-01\n",
      "   5.0664e-01  4.0250e-01 -2.1388e-03 -2.8397e-01 -5.0324e-01  3.0449e-01\n",
      "   5.1779e-01  1.5090e-02 -3.5031e-01 -1.1278e+00  3.3253e-01 -3.5250e-01\n",
      "   4.1326e-02  1.0863e+00  3.3910e-02  3.3564e-01  4.9745e-01 -7.0131e-02\n",
      "  -1.2192e+00 -4.8512e-01 -3.8512e-02 -1.3554e-01 -1.6380e-01  5.2321e-01\n",
      "  -3.1318e-01 -1.6550e-01  1.1909e-01 -1.5115e-01 -1.5621e-01 -6.2655e-01\n",
      "  -6.2336e-01 -4.2150e-01  4.1873e-01 -9.2472e-01  1.1049e+00 -2.9996e-01\n",
      "  -6.3003e-03  3.9540e-01]\n",
      " [-7.2202e-02 -3.8312e-02  3.5678e-01 -1.8762e-01  6.1088e-01  2.3631e-01\n",
      "  -4.3833e-01  4.1290e-01  2.5682e-01  3.3728e-01 -1.5096e-01  2.5277e-01\n",
      "  -2.2161e-01  1.9157e-01  1.1020e+00  2.5557e-01  1.2585e-01  6.4586e-01\n",
      "  -1.1285e-01 -4.6857e-01 -2.4584e-01  5.1331e-01  2.6720e-01  4.7869e-01\n",
      "   5.7446e-01 -1.9802e+00  5.0118e-02  7.3693e-01  5.4434e-01 -3.9787e-01\n",
      "   2.4670e+00  4.5542e-01  1.8973e-02  2.8066e-01 -6.2605e-02  7.1148e-02\n",
      "  -1.0552e-01  2.3076e-01  4.7146e-01 -1.2256e+00 -8.7966e-02  5.7726e-01\n",
      "  -2.4806e-01  9.8880e-02 -3.8841e-01 -1.5556e-01  2.3919e-01 -3.0419e-01\n",
      "   1.2098e-01  5.9627e-01]\n",
      " [ 2.2426e-01 -7.4140e-01 -2.3476e-01  4.6580e-02  2.1730e-02  6.1725e-01\n",
      "  -3.1005e-01  3.1925e-01 -7.5937e-01  1.8388e-01  7.7406e-03 -1.2780e-01\n",
      "  -2.6909e-01 -4.5236e-01  2.7808e-01  5.0401e-01  1.1270e-01 -8.9587e-01\n",
      "   5.6290e-01 -3.2498e-01  3.6629e-01  3.0937e-01 -7.0191e-02 -2.9065e-01\n",
      "  -2.6496e-01 -1.9891e+00  4.2419e-01  2.5204e-02 -1.0215e+00  1.0292e-01\n",
      "   2.0998e+00 -8.3131e-01 -1.2163e+00 -4.0155e-01 -9.0138e-02 -1.2048e+00\n",
      "   5.6457e-01 -2.7167e-01 -7.2691e-01 -1.9167e-01  1.4795e-01  1.3526e-01\n",
      "   9.6795e-02  6.6091e-02 -3.7050e-01  7.3678e-02 -1.2934e+00  1.2381e+00\n",
      "   3.3281e-02 -2.5800e-01]\n",
      " [ 3.8315e-01 -3.5610e-01 -1.2830e-01 -1.9527e-01  4.7629e-02  2.1468e-01\n",
      "  -9.8765e-01  8.2962e-01 -4.2782e-01 -2.2879e-01  1.0712e-01 -3.0870e-01\n",
      "  -1.2069e+00 -1.7713e-01  8.8841e-01  5.6658e-03 -7.7305e-01 -6.6913e-01\n",
      "  -1.3384e+00  3.4676e-01  5.0440e-01  5.1250e-01  2.6826e-01 -6.5313e-01\n",
      "  -8.1516e-02 -2.1658e+00  5.7974e-01  3.6345e-02  9.0949e-03  2.5772e-01\n",
      "   3.4402e+00  2.0732e-01 -5.2028e-01  2.6453e-02  1.7895e-01 -1.7802e-02\n",
      "   3.6605e-01  3.4539e-01  4.1357e-01 -2.4970e-01 -4.9227e-01  1.7745e-01\n",
      "  -4.3764e-01 -3.4840e-01 -5.7061e-02 -3.9578e-02 -1.3517e-01 -4.2580e-01\n",
      "   1.3681e-01 -7.7731e-01]\n",
      " [ 4.4349e-01 -7.1870e-01  7.4168e-01 -3.0663e-01  3.2147e-01  9.8841e-03\n",
      "  -5.7295e-01  7.1186e-01 -1.3162e-01  2.5001e-02 -3.3982e-01  2.4820e-01\n",
      "  -8.1255e-01  4.7738e-01  1.1965e-01  4.6931e-01  7.7249e-01 -3.2196e-01\n",
      "   2.9724e-01 -7.6281e-01  9.4952e-02  6.9615e-01  3.1000e-02  3.4311e-01\n",
      "   3.7160e-01 -2.4612e+00 -1.8062e-01 -4.4132e-01  9.1056e-01 -9.3270e-01\n",
      "   2.9290e+00  7.9259e-01 -1.1483e+00 -4.3050e-01 -3.3168e-02  2.2947e-01\n",
      "  -8.0059e-01 -9.0278e-02  2.0351e-01 -4.9679e-01 -3.6519e-01  2.2556e-01\n",
      "   3.7469e-01  1.7841e-01  6.1943e-01 -3.5550e-02 -6.7799e-02 -8.8973e-02\n",
      "   2.2376e-02 -1.9262e-01]\n",
      " [ 6.8047e-01 -3.9263e-02  3.0186e-01 -1.7792e-01  4.2962e-01  3.2246e-02\n",
      "  -4.1376e-01  1.3228e-01 -2.9847e-01 -8.5253e-02  1.7118e-01  2.2419e-01\n",
      "  -1.0046e-01 -4.3653e-01  3.3418e-01  6.7846e-01  5.7204e-02 -3.4448e-01\n",
      "  -4.2785e-01 -4.3275e-01  5.5963e-01  1.0032e-01  1.8677e-01 -2.6854e-01\n",
      "   3.7334e-02 -2.0932e+00  2.2171e-01 -3.9868e-01  2.0912e-01 -5.5725e-01\n",
      "   3.8826e+00  4.7466e-01 -9.5658e-01 -3.7788e-01  2.0869e-01 -3.2752e-01\n",
      "   1.2751e-01  8.8359e-02  1.6351e-01 -2.1634e-01 -9.4375e-02  1.8324e-02\n",
      "   2.1048e-01 -3.0880e-02 -1.9722e-01  8.2279e-02 -9.4340e-02 -7.3297e-02\n",
      "  -6.4699e-02 -2.6044e-01]\n",
      " [ 7.6552e-01 -1.2379e-01  1.1440e+00  7.1116e-02  6.9537e-01 -2.5318e-01\n",
      "  -1.4303e+00 -6.4846e-01 -1.7595e-01  6.0268e-01  1.0464e-01  9.6283e-01\n",
      "   3.5706e-01 -4.5109e-01  5.5292e-01  1.1588e+00 -1.3179e-01  3.4544e-01\n",
      "   1.8801e-02 -1.1188e+00  1.3580e+00 -1.1799e-01 -3.9478e-01 -1.1678e-01\n",
      "  -9.2234e-01 -1.4277e+00 -3.8724e-01 -1.5941e-01  7.8802e-01 -9.0019e-01\n",
      "   2.5643e+00  7.0629e-01 -1.3151e-01  1.1043e+00  5.1428e-01 -6.0614e-01\n",
      "  -5.1230e-01 -3.4909e-02  4.5906e-01 -9.7082e-01  7.6554e-01  5.2619e-03\n",
      "   1.0155e+00  2.7476e-01  2.8309e-01  2.3979e-01 -8.5404e-01  3.6266e-01\n",
      "  -1.7013e-01  6.4426e-01]\n",
      " [-7.9149e-01  8.6617e-01  1.1998e-01  9.2287e-04  2.7760e-01 -4.9185e-01\n",
      "   5.0195e-01  6.0792e-04 -2.5845e-01  1.7865e-01  2.5350e-01  7.6572e-01\n",
      "   5.0664e-01  4.0250e-01 -2.1388e-03 -2.8397e-01 -5.0324e-01  3.0449e-01\n",
      "   5.1779e-01  1.5090e-02 -3.5031e-01 -1.1278e+00  3.3253e-01 -3.5250e-01\n",
      "   4.1326e-02  1.0863e+00  3.3910e-02  3.3564e-01  4.9745e-01 -7.0131e-02\n",
      "  -1.2192e+00 -4.8512e-01 -3.8512e-02 -1.3554e-01 -1.6380e-01  5.2321e-01\n",
      "  -3.1318e-01 -1.6550e-01  1.1909e-01 -1.5115e-01 -1.5621e-01 -6.2655e-01\n",
      "  -6.2336e-01 -4.2150e-01  4.1873e-01 -9.2472e-01  1.1049e+00 -2.9996e-01\n",
      "  -6.3003e-03  3.9540e-01]\n",
      " [ 2.5616e-01  4.3694e-01 -1.1889e-01  2.0345e-01  4.1959e-01  8.5863e-01\n",
      "  -6.0344e-01 -3.1835e-01 -6.7180e-01  3.9840e-03 -7.5159e-02  1.1043e-01\n",
      "  -7.3534e-01  2.7436e-01  5.4015e-02 -2.3828e-01 -1.3767e-01  1.1573e-02\n",
      "  -4.6623e-01 -5.5233e-01  8.3317e-02  5.5938e-01  5.1903e-01 -2.7065e-01\n",
      "  -2.8211e-01 -1.3918e+00  1.7498e-01  2.6586e-01  6.1449e-02 -2.7300e-01\n",
      "   3.9032e+00  3.8169e-01 -5.6009e-02 -4.4250e-03  2.4033e-01  3.0675e-01\n",
      "  -1.2638e-01  3.3436e-01  7.5485e-02 -3.6218e-02  1.3691e-01  3.7762e-01\n",
      "  -1.2159e-01 -1.3808e-01  1.9505e-01  2.2793e-01 -1.7304e-01 -7.5730e-02\n",
      "  -2.5868e-01 -3.9339e-01]\n",
      " [-7.9149e-01  8.6617e-01  1.1998e-01  9.2287e-04  2.7760e-01 -4.9185e-01\n",
      "   5.0195e-01  6.0792e-04 -2.5845e-01  1.7865e-01  2.5350e-01  7.6572e-01\n",
      "   5.0664e-01  4.0250e-01 -2.1388e-03 -2.8397e-01 -5.0324e-01  3.0449e-01\n",
      "   5.1779e-01  1.5090e-02 -3.5031e-01 -1.1278e+00  3.3253e-01 -3.5250e-01\n",
      "   4.1326e-02  1.0863e+00  3.3910e-02  3.3564e-01  4.9745e-01 -7.0131e-02\n",
      "  -1.2192e+00 -4.8512e-01 -3.8512e-02 -1.3554e-01 -1.6380e-01  5.2321e-01\n",
      "  -3.1318e-01 -1.6550e-01  1.1909e-01 -1.5115e-01 -1.5621e-01 -6.2655e-01\n",
      "  -6.2336e-01 -4.2150e-01  4.1873e-01 -9.2472e-01  1.1049e+00 -2.9996e-01\n",
      "  -6.3003e-03  3.9540e-01]\n",
      " [-4.9282e-01  3.4231e-01  9.3662e-01  8.6695e-01  6.2261e-01  1.4615e-02\n",
      "  -6.2331e-01  2.1383e-02  2.5933e-01  2.3695e-01 -4.3837e-02  6.5615e-01\n",
      "  -1.2180e-01  1.8390e-01  7.2801e-01 -5.7085e-01 -9.9428e-01 -7.6187e-01\n",
      "   4.4481e-02 -6.6478e-01  3.5339e-01 -1.3720e+00 -7.0328e-01  3.4372e-01\n",
      "  -5.5958e-01 -1.5715e+00 -7.3208e-02 -6.2584e-01  2.0277e-01 -1.1796e+00\n",
      "   2.4812e+00  5.5142e-01  1.8541e-02  2.8690e-01  1.1410e-01  6.0408e-01\n",
      "   5.5863e-01  1.3701e-01 -3.3045e-01 -1.3547e+00  1.0943e+00  7.6710e-02\n",
      "  -2.5892e-01  7.9890e-01 -1.4411e-01 -3.4701e-01  3.1202e-01  1.8600e-01\n",
      "   8.5489e-01  2.5597e-02]\n",
      " [-2.9317e-01  4.0853e-01 -3.3864e-01 -5.1048e-01  7.0926e-01 -1.9915e-01\n",
      "   8.5861e-01 -5.7151e-02 -3.8268e-01  3.7696e-01  7.3189e-02  5.7895e-01\n",
      "  -1.7146e-01 -5.3171e-01  2.4612e-01 -2.6859e-01  4.9583e-01  3.8142e-01\n",
      "   9.8846e-02  7.3221e-02 -2.8310e-01  3.2308e-01  7.3795e-01  2.4775e-01\n",
      "   9.9444e-01 -1.6140e-01 -1.1411e+00 -2.7577e-01  7.7427e-01 -3.4591e-01\n",
      "   1.7546e+00 -1.0290e-03 -5.6660e-02 -2.6130e-01 -3.9687e-01  7.9826e-02\n",
      "   4.9462e-01 -1.2352e-01  6.2210e-02  1.1853e-01  3.8208e-01 -9.1746e-01\n",
      "  -9.9595e-03  1.6010e-01 -4.1623e-01  2.4994e-01  3.8409e-02 -3.3172e-01\n",
      "   1.9551e-01  3.5758e-01]\n",
      " [ 4.5323e-01  5.9811e-02 -1.0577e-01 -3.3300e-01  7.2359e-01 -8.7170e-02\n",
      "  -6.1053e-01 -3.7695e-02 -3.0945e-01  2.1805e-01 -4.3605e-01  4.7318e-01\n",
      "  -7.6866e-01 -2.7130e-01  1.1042e+00  5.9141e-01  5.6962e-01 -1.8678e-01\n",
      "   1.4867e-01 -6.7292e-01 -3.4672e-01  5.2284e-01  2.2959e-01 -7.2014e-02\n",
      "   9.3967e-01 -2.3985e+00 -1.3238e+00  2.8698e-01  7.5509e-01 -7.6522e-01\n",
      "   3.3425e+00  1.7233e-01 -5.1803e-01 -8.2970e-01 -2.9333e-01 -5.0076e-01\n",
      "  -1.5228e-01  9.8973e-02  1.8146e-01 -1.7420e-01 -4.0666e-01  2.0348e-01\n",
      "  -1.1788e-02  4.8252e-01  2.4598e-02  3.4064e-01 -8.4724e-02  5.3240e-01\n",
      "  -2.5103e-01  6.2546e-01]\n",
      " [ 4.0825e-01  1.3589e-01 -9.2040e-02  7.7916e-02  8.0876e-01 -1.1682e-01\n",
      "   1.7651e-01  5.7259e-01 -3.4809e-02  1.8086e-01 -1.3495e-01  2.3056e-01\n",
      "  -3.2223e-01  1.9001e-01  1.3714e+00  8.9084e-01  5.3573e-01  2.0547e-01\n",
      "  -1.1459e-01 -4.3327e-01 -1.0507e-01  5.9712e-02  2.1579e-01  7.5578e-01\n",
      "   1.1642e+00 -1.4020e+00 -8.0244e-01  2.9944e-01  1.2265e+00 -3.7954e-01\n",
      "   2.1982e+00  4.5418e-01 -3.4887e-01 -6.9931e-01 -3.3260e-01 -2.8976e-01\n",
      "  -5.9243e-02 -3.9974e-02  4.2430e-01 -2.7954e-01 -7.2751e-01  1.0190e-01\n",
      "  -2.4600e-01  4.8189e-01 -1.1966e-01  2.0752e-01  5.0721e-01 -8.1430e-03\n",
      "   1.6891e-02  9.7260e-01]\n",
      " [ 3.9119e-01  3.4992e-01  2.2498e-01  1.1417e-01  3.1087e-01 -5.2257e-01\n",
      "  -9.2955e-01  2.5335e-01 -8.9138e-02 -4.2545e-01 -3.1354e-01 -4.5698e-01\n",
      "  -6.4173e-01  3.1926e-01  1.0812e+00  6.1861e-01  2.8539e-02 -3.4201e-01\n",
      "  -6.8273e-01 -2.5298e-01  4.3399e-01 -2.6450e-01 -5.7452e-02  8.2656e-03\n",
      "   1.5807e-02 -1.5390e+00  2.8407e-01  1.4952e-01 -7.4397e-02 -1.8124e-01\n",
      "   3.5567e+00  6.3548e-01 -7.6356e-01  2.1495e-01  2.3488e-01 -3.0534e-01\n",
      "   4.6327e-01  6.4019e-01  7.7317e-02 -8.8417e-01 -7.1713e-01 -1.6503e-01\n",
      "  -2.1991e-01 -3.8886e-01 -2.6111e-01  5.8742e-01  2.9305e-01 -2.0466e-01\n",
      "  -5.9552e-03  1.7460e-01]\n",
      " [ 6.1850e-01  6.4254e-01 -4.6552e-01  3.7570e-01  7.4838e-01  5.3739e-01\n",
      "   2.2239e-03 -6.0577e-01  2.6408e-01  1.1703e-01  4.3722e-01  2.0092e-01\n",
      "  -5.7859e-02 -3.4589e-01  2.1664e-01  5.8573e-01  5.3919e-01  6.9490e-01\n",
      "  -1.5618e-01  5.5830e-02 -6.0515e-01 -2.8997e-01 -2.5594e-02  5.5593e-01\n",
      "   2.5356e-01 -1.9612e+00 -5.1381e-01  6.9096e-01  6.6246e-02 -5.4224e-02\n",
      "   3.7871e+00 -7.7403e-01 -1.2689e-01 -5.1465e-01  6.6705e-02 -3.2933e-01\n",
      "   1.3483e-01  1.9049e-01  1.3812e-01 -2.1503e-01 -1.6573e-02  3.1200e-01\n",
      "  -3.3189e-01 -2.6001e-02 -3.8203e-01  1.9403e-01 -1.2466e-01 -2.7557e-01\n",
      "   3.0899e-01  4.8497e-01]\n",
      " [-7.9149e-01  8.6617e-01  1.1998e-01  9.2287e-04  2.7760e-01 -4.9185e-01\n",
      "   5.0195e-01  6.0792e-04 -2.5845e-01  1.7865e-01  2.5350e-01  7.6572e-01\n",
      "   5.0664e-01  4.0250e-01 -2.1388e-03 -2.8397e-01 -5.0324e-01  3.0449e-01\n",
      "   5.1779e-01  1.5090e-02 -3.5031e-01 -1.1278e+00  3.3253e-01 -3.5250e-01\n",
      "   4.1326e-02  1.0863e+00  3.3910e-02  3.3564e-01  4.9745e-01 -7.0131e-02\n",
      "  -1.2192e+00 -4.8512e-01 -3.8512e-02 -1.3554e-01 -1.6380e-01  5.2321e-01\n",
      "  -3.1318e-01 -1.6550e-01  1.1909e-01 -1.5115e-01 -1.5621e-01 -6.2655e-01\n",
      "  -6.2336e-01 -4.2150e-01  4.1873e-01 -9.2472e-01  1.1049e+00 -2.9996e-01\n",
      "  -6.3003e-03  3.9540e-01]\n",
      " [-5.8402e-01  3.9031e-01  6.5282e-01 -3.4030e-01  1.9493e-01 -8.3489e-01\n",
      "   1.1929e-01 -5.7291e-01 -5.6844e-01  7.2989e-01 -5.6975e-01  5.3436e-01\n",
      "  -3.8034e-01  2.2471e-01  9.8031e-01 -2.9660e-01  1.2600e-01  5.5222e-01\n",
      "  -6.2737e-01 -8.2242e-02 -8.5359e-02  3.1515e-01  9.6077e-01  3.1986e-01\n",
      "   8.7878e-01 -1.5189e+00 -1.7831e+00  3.5639e-01  9.6740e-01 -1.5497e+00\n",
      "   2.3350e+00  8.4940e-01 -1.2371e+00  1.0623e+00 -1.4267e+00 -4.9056e-01\n",
      "   8.5465e-01 -1.2878e+00  6.0204e-01 -3.5963e-01  2.8586e-01 -5.2162e-02\n",
      "  -5.0818e-01 -6.3459e-01  3.3889e-01  2.8416e-01 -2.0340e-01 -1.2338e+00\n",
      "   4.6715e-01  7.8858e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Look at file for GloVe embedding of titles\n",
    "isot_title_embed = all_data.embedded_title.values\n",
    "\n",
    "print('isot_title_embed shape:', isot_title_embed.shape)  #(44898,)\n",
    "print('isot_title_embed shape of first element:', isot_title_embed[0].shape)  # (18, 50)\n",
    "print('isot_title_embed example:\\n', isot_title_embed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = np.reshape(isot_title_embed, (-1, 50)) \n",
    "#print(text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read ISOT vocab from pickle file.\n",
    "\n",
    "vocab = pd.read_pickle('parsed_data/vocab.pkl')  # ISOT data (CMU) tokenized and POS tags added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152,182 words\n",
      "wordset:  ['<s>', '</s>', '<unk>', 'the', ',', '.', 'to', 'of', 'a', 'and', 'in', 'that', 'on', '<number>', 'for', 's', 'is', 'he', 'said', 'trump', 'it', 'with', 'was', 'as', 'his', 'by', 'has', 'be', 'have', 'not']\n",
      "<w266_common.vocabulary.Vocabulary object at 0x7fd9b41e1278>\n"
     ]
    }
   ],
   "source": [
    "print(\"{:,} words\".format(vocab.size))  # Note: this combines words from ISOT \"title\" AND \"text\" fields!\n",
    "print(\"wordset: \",vocab.ordered_words()[:30])\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISOT ALL target=real: 21417\n",
      "ISOT ALL target=fake: 23481\n"
     ]
    }
   ],
   "source": [
    "print('ISOT ALL target=real:', len(all_data.target[all_data.target == '1']))\n",
    "print('ISOT ALL target=fake:', len(all_data.target[all_data.target == '0']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Dev / Test Split ISOT data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split fractions add up to 1.0\n",
      "training set:  (31428, 11)\n",
      "dev set:  (6735, 11)\n",
      "test set:  (6735, 11)\n"
     ]
    }
   ],
   "source": [
    "#train/dev/train split\n",
    "#train_dev_split = 0.8\n",
    "\n",
    "train_fract = 0.70\n",
    "dev_fract = 0.15\n",
    "test_fract = 0.15\n",
    "\n",
    "if (train_fract+dev_fract+test_fract) == 1.0:\n",
    "    print('Split fractions add up to 1.0')\n",
    "else:\n",
    "    print('SPLIT FRACTIONS DO NOT ADD UP TO 1.0; PLEASE TRY AGAIN.............')\n",
    "\n",
    "#train_data = all_data[:int(len(all_data)*train_dev_split)].reset_index(drop=True)\n",
    "#dev_data = all_data[int(len(all_data)*train_dev_split):].reset_index(drop=True)\n",
    "\n",
    "train_set = all_data[ :int(len(all_data)*train_fract)].reset_index(drop=True)\n",
    "dev_set = all_data[int(len(all_data)*(train_fract)) : int(len(all_data)*(train_fract+dev_fract))].reset_index(drop=True)\n",
    "test_set = all_data[int(len(all_data)*(train_fract+dev_fract)) : ].reset_index(drop=True)\n",
    "\n",
    "print('training set: ',train_set.shape)\n",
    "print('dev set: ',dev_set.shape)\n",
    "print('test set: ',test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>target</th>\n",
       "      <th>title_tokcan</th>\n",
       "      <th>title_POS</th>\n",
       "      <th>text_tokcan</th>\n",
       "      <th>text_POS</th>\n",
       "      <th>embedded_title</th>\n",
       "      <th>embedded_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BRAINIAC Gets Rejected After Trying To Buy BMW...</td>\n",
       "      <td>Does anyone else out there see a future BMW ca...</td>\n",
       "      <td>Government News</td>\n",
       "      <td>Mar 20, 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>[brainiac&lt;allcaps&gt;, gets, rejected, after, try...</td>\n",
       "      <td>[N, V, V, P, V, P, V, ^, P, ^, ^, ,, O, V, A, ...</td>\n",
       "      <td>[does, anyone, else, out, there, see, a, futur...</td>\n",
       "      <td>[V, N, R, P, R, V, D, A, ^, N, N, P, D, N, ,, ...</td>\n",
       "      <td>[[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...</td>\n",
       "      <td>[[0.2293, 0.34231, 0.059817, 0.083003, 0.57685...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Windows 10 is Stealing Your Bandwidth (You Mig...</td>\n",
       "      <td>21st Century Wire says We ve heard a lot of no...</td>\n",
       "      <td>US_News</td>\n",
       "      <td>April 7, 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>[windows, &lt;number&gt;, is, stealing, your, bandwi...</td>\n",
       "      <td>[^, $, V, V, D, N, ,, O, V, V, P, V, O, ,]</td>\n",
       "      <td>[&lt;number&gt;st, century, wire, says, we, ve, hear...</td>\n",
       "      <td>[A, N, ^, V, O, V, V, D, N, P, R, R, A, N, P, ...</td>\n",
       "      <td>[[0.80482, -0.39569, 1.4741, 0.39562, 0.025004...</td>\n",
       "      <td>[[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>STUNNING STORY The Media And Democrats Hid Fro...</td>\n",
       "      <td>In an email sent on April 15, 2011, our upstan...</td>\n",
       "      <td>left-news</td>\n",
       "      <td>Mar 2, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>[stunning&lt;allcaps&gt;, story&lt;allcaps&gt;, the, media...</td>\n",
       "      <td>[A, N, D, N, &amp;, N, V, P, ^, ,, R, Z, ^, ^, N, ...</td>\n",
       "      <td>[in, an, email, sent, on, april, &lt;number&gt;, ,, ...</td>\n",
       "      <td>[P, D, N, V, P, ^, $, ,, $, ,, D, A, N, A, ^, ...</td>\n",
       "      <td>[[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...</td>\n",
       "      <td>[[0.33042, 0.24995, -0.60874, 0.10923, 0.03637...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>North Korea's Kim Jong Un fetes nuclear scient...</td>\n",
       "      <td>SEOUL (Reuters) - North Korean leader Kim Jong...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>September 10, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>[north, korea's, kim, jong, un, fetes, nuclear...</td>\n",
       "      <td>[^, Z, ^, ^, ^, V, A, N, ,, V, N, N]</td>\n",
       "      <td>[seoul&lt;allcaps&gt;, (, reuters, ), -, north, kore...</td>\n",
       "      <td>[^, ,, ^, ,, ,, ^, ^, N, ^, ^, ^, V, D, A, N, ...</td>\n",
       "      <td>[[0.30059, 0.55598, -0.040589, 0.020289, -0.57...</td>\n",
       "      <td>[[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>White House developing comprehensive biosecuri...</td>\n",
       "      <td>ASPEN, Colorado (Reuters) - The Trump administ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>July 20, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>[white, house, developing, comprehensive, bios...</td>\n",
       "      <td>[A, N, V, A, N, N, ,, A]</td>\n",
       "      <td>[aspen&lt;allcaps&gt;, ,, colorado, (, reuters, ), -...</td>\n",
       "      <td>[^, ,, ^, ,, ^, ,, ,, D, ^, N, V, V, D, A, A, ...</td>\n",
       "      <td>[[-0.68652, 0.80125, -0.6124, -0.1512, 0.997, ...</td>\n",
       "      <td>[[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  BRAINIAC Gets Rejected After Trying To Buy BMW...   \n",
       "1  Windows 10 is Stealing Your Bandwidth (You Mig...   \n",
       "2  STUNNING STORY The Media And Democrats Hid Fro...   \n",
       "3  North Korea's Kim Jong Un fetes nuclear scient...   \n",
       "4  White House developing comprehensive biosecuri...   \n",
       "\n",
       "                                                text          subject  \\\n",
       "0  Does anyone else out there see a future BMW ca...  Government News   \n",
       "1  21st Century Wire says We ve heard a lot of no...          US_News   \n",
       "2  In an email sent on April 15, 2011, our upstan...        left-news   \n",
       "3  SEOUL (Reuters) - North Korean leader Kim Jong...        worldnews   \n",
       "4  ASPEN, Colorado (Reuters) - The Trump administ...     politicsNews   \n",
       "\n",
       "                  date target  \\\n",
       "0         Mar 20, 2016      0   \n",
       "1        April 7, 2016      0   \n",
       "2          Mar 2, 2017      0   \n",
       "3  September 10, 2017       1   \n",
       "4       July 20, 2017       1   \n",
       "\n",
       "                                        title_tokcan  \\\n",
       "0  [brainiac<allcaps>, gets, rejected, after, try...   \n",
       "1  [windows, <number>, is, stealing, your, bandwi...   \n",
       "2  [stunning<allcaps>, story<allcaps>, the, media...   \n",
       "3  [north, korea's, kim, jong, un, fetes, nuclear...   \n",
       "4  [white, house, developing, comprehensive, bios...   \n",
       "\n",
       "                                           title_POS  \\\n",
       "0  [N, V, V, P, V, P, V, ^, P, ^, ^, ,, O, V, A, ...   \n",
       "1         [^, $, V, V, D, N, ,, O, V, V, P, V, O, ,]   \n",
       "2  [A, N, D, N, &, N, V, P, ^, ,, R, Z, ^, ^, N, ...   \n",
       "3               [^, Z, ^, ^, ^, V, A, N, ,, V, N, N]   \n",
       "4                           [A, N, V, A, N, N, ,, A]   \n",
       "\n",
       "                                         text_tokcan  \\\n",
       "0  [does, anyone, else, out, there, see, a, futur...   \n",
       "1  [<number>st, century, wire, says, we, ve, hear...   \n",
       "2  [in, an, email, sent, on, april, <number>, ,, ...   \n",
       "3  [seoul<allcaps>, (, reuters, ), -, north, kore...   \n",
       "4  [aspen<allcaps>, ,, colorado, (, reuters, ), -...   \n",
       "\n",
       "                                            text_POS  \\\n",
       "0  [V, N, R, P, R, V, D, A, ^, N, N, P, D, N, ,, ...   \n",
       "1  [A, N, ^, V, O, V, V, D, N, P, R, R, A, N, P, ...   \n",
       "2  [P, D, N, V, P, ^, $, ,, $, ,, D, A, N, A, ^, ...   \n",
       "3  [^, ,, ^, ,, ,, ^, ^, N, ^, ^, ^, V, D, A, N, ...   \n",
       "4  [^, ,, ^, ,, ^, ,, ,, D, ^, N, V, V, D, A, A, ...   \n",
       "\n",
       "                                      embedded_title  \\\n",
       "0  [[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...   \n",
       "1  [[0.80482, -0.39569, 1.4741, 0.39562, 0.025004...   \n",
       "2  [[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...   \n",
       "3  [[0.30059, 0.55598, -0.040589, 0.020289, -0.57...   \n",
       "4  [[-0.68652, 0.80125, -0.6124, -0.1512, 0.997, ...   \n",
       "\n",
       "                                       embedded_text  \n",
       "0  [[0.2293, 0.34231, 0.059817, 0.083003, 0.57685...  \n",
       "1  [[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...  \n",
       "2  [[0.33042, 0.24995, -0.60874, 0.10923, 0.03637...  \n",
       "3  [[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...  \n",
       "4  [[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>target</th>\n",
       "      <th>title_tokcan</th>\n",
       "      <th>title_POS</th>\n",
       "      <th>text_tokcan</th>\n",
       "      <th>text_POS</th>\n",
       "      <th>embedded_title</th>\n",
       "      <th>embedded_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Turkey condemns U.S. move on Jerusalem as 'irr...</td>\n",
       "      <td>ISTANBUL (Reuters) - Turkey s foreign ministry...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>December 6, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>[turkey, condemns, u.s., move, on, jerusalem, ...</td>\n",
       "      <td>[N, V, ^, N, P, ^, P, ,, A, ,]</td>\n",
       "      <td>[istanbul&lt;allcaps&gt;, (, reuters, ), -, turkey, ...</td>\n",
       "      <td>[^, ,, ^, ,, ,, N, G, A, N, P, ^, V, D, N, P, ...</td>\n",
       "      <td>[[0.38135, -0.20444, -0.57329, 0.039556, 0.566...</td>\n",
       "      <td>[[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UK finance minister's future questioned by PM ...</td>\n",
       "      <td>LONDON (Reuters) - Britain s finance minister ...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>October 14, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>[uk&lt;allcaps&gt;, finance, minister's, future, que...</td>\n",
       "      <td>[^, N, S, N, V, P, ^, Z, N, P, N, V]</td>\n",
       "      <td>[london&lt;allcaps&gt;, (, reuters, ), -, britain, s...</td>\n",
       "      <td>[^, ,, ^, ,, ,, ^, G, N, N, ^, ^, P, ^, V, V, ...</td>\n",
       "      <td>[[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...</td>\n",
       "      <td>[[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Canada government facing resistance from Senat...</td>\n",
       "      <td>OTTAWA (Reuters) - The Canadian government s p...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>November 3, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>[canada, government, facing, resistance, from,...</td>\n",
       "      <td>[^, N, V, N, P, ^, P, N, N]</td>\n",
       "      <td>[ottawa&lt;allcaps&gt;, (, reuters, ), -, the, canad...</td>\n",
       "      <td>[^, ,, ^, ,, ,, D, ^, ^, G, V, P, V, A, N, P, ...</td>\n",
       "      <td>[[-0.72491, 0.40524, -0.35895, 0.65977, -0.275...</td>\n",
       "      <td>[[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tillerson says would support maintaining Russi...</td>\n",
       "      <td>WASHINGTON (Reuters) - President-elect Donald ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>January 11, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>[tillerson, says, would, support, maintaining,...</td>\n",
       "      <td>[^, V, V, V, V, ^, N, P, R]</td>\n",
       "      <td>[washington&lt;allcaps&gt;, (, reuters, ), -, presid...</td>\n",
       "      <td>[^, ,, ^, ,, ,, ^, ^, ^, N, P, ^, N, P, ^, ,, ...</td>\n",
       "      <td>[[0.80122, -0.68464, 0.52739, 0.5973, 0.45633,...</td>\n",
       "      <td>[[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DEPLORABLE! HILLARY’S Campaign Is In PANIC Mod...</td>\n",
       "      <td>What happens when Hillary s poll numbers take ...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Sep 16, 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>[deplorable&lt;allcaps&gt;, !, hillary&lt;allcaps&gt;’s, c...</td>\n",
       "      <td>[A, ,, Z, N, V, P, N, N, ,, D, A, ,, A, N, ,, ...</td>\n",
       "      <td>[what, happens, when, hillary, s, poll, number...</td>\n",
       "      <td>[O, V, R, ^, G, N, N, V, D, N, P, O, V, V, V, ...</td>\n",
       "      <td>[[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...</td>\n",
       "      <td>[[0.45323, 0.059811, -0.10577, -0.333, 0.72359...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Turkey condemns U.S. move on Jerusalem as 'irr...   \n",
       "1  UK finance minister's future questioned by PM ...   \n",
       "2  Canada government facing resistance from Senat...   \n",
       "3  Tillerson says would support maintaining Russi...   \n",
       "4  DEPLORABLE! HILLARY’S Campaign Is In PANIC Mod...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  ISTANBUL (Reuters) - Turkey s foreign ministry...     worldnews   \n",
       "1  LONDON (Reuters) - Britain s finance minister ...     worldnews   \n",
       "2  OTTAWA (Reuters) - The Canadian government s p...     worldnews   \n",
       "3  WASHINGTON (Reuters) - President-elect Donald ...  politicsNews   \n",
       "4  What happens when Hillary s poll numbers take ...      politics   \n",
       "\n",
       "                date target  \\\n",
       "0  December 6, 2017       1   \n",
       "1  October 14, 2017       1   \n",
       "2  November 3, 2017       1   \n",
       "3  January 11, 2017       1   \n",
       "4       Sep 16, 2016      0   \n",
       "\n",
       "                                        title_tokcan  \\\n",
       "0  [turkey, condemns, u.s., move, on, jerusalem, ...   \n",
       "1  [uk<allcaps>, finance, minister's, future, que...   \n",
       "2  [canada, government, facing, resistance, from,...   \n",
       "3  [tillerson, says, would, support, maintaining,...   \n",
       "4  [deplorable<allcaps>, !, hillary<allcaps>’s, c...   \n",
       "\n",
       "                                           title_POS  \\\n",
       "0                     [N, V, ^, N, P, ^, P, ,, A, ,]   \n",
       "1               [^, N, S, N, V, P, ^, Z, N, P, N, V]   \n",
       "2                        [^, N, V, N, P, ^, P, N, N]   \n",
       "3                        [^, V, V, V, V, ^, N, P, R]   \n",
       "4  [A, ,, Z, N, V, P, N, N, ,, D, A, ,, A, N, ,, ...   \n",
       "\n",
       "                                         text_tokcan  \\\n",
       "0  [istanbul<allcaps>, (, reuters, ), -, turkey, ...   \n",
       "1  [london<allcaps>, (, reuters, ), -, britain, s...   \n",
       "2  [ottawa<allcaps>, (, reuters, ), -, the, canad...   \n",
       "3  [washington<allcaps>, (, reuters, ), -, presid...   \n",
       "4  [what, happens, when, hillary, s, poll, number...   \n",
       "\n",
       "                                            text_POS  \\\n",
       "0  [^, ,, ^, ,, ,, N, G, A, N, P, ^, V, D, N, P, ...   \n",
       "1  [^, ,, ^, ,, ,, ^, G, N, N, ^, ^, P, ^, V, V, ...   \n",
       "2  [^, ,, ^, ,, ,, D, ^, ^, G, V, P, V, A, N, P, ...   \n",
       "3  [^, ,, ^, ,, ,, ^, ^, ^, N, P, ^, N, P, ^, ,, ...   \n",
       "4  [O, V, R, ^, G, N, N, V, D, N, P, O, V, V, V, ...   \n",
       "\n",
       "                                      embedded_title  \\\n",
       "0  [[0.38135, -0.20444, -0.57329, 0.039556, 0.566...   \n",
       "1  [[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...   \n",
       "2  [[-0.72491, 0.40524, -0.35895, 0.65977, -0.275...   \n",
       "3  [[0.80122, -0.68464, 0.52739, 0.5973, 0.45633,...   \n",
       "4  [[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...   \n",
       "\n",
       "                                       embedded_text  \n",
       "0  [[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...  \n",
       "1  [[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...  \n",
       "2  [[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...  \n",
       "3  [[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...  \n",
       "4  [[0.45323, 0.059811, -0.10577, -0.333, 0.72359...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>target</th>\n",
       "      <th>title_tokcan</th>\n",
       "      <th>title_POS</th>\n",
       "      <th>text_tokcan</th>\n",
       "      <th>text_POS</th>\n",
       "      <th>embedded_title</th>\n",
       "      <th>embedded_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEW LAW WILL PUNISH MUSLIM Migrants…Assimilate...</td>\n",
       "      <td>Is this common sense law even practical given ...</td>\n",
       "      <td>left-news</td>\n",
       "      <td>Apr 23, 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>[new&lt;allcaps&gt;, law&lt;allcaps&gt;, will&lt;allcaps&gt;, pu...</td>\n",
       "      <td>[A, N, V, V, A, N, ,, V, &amp;, V, T, ,]</td>\n",
       "      <td>[is, this, common, sense, law, even, practical...</td>\n",
       "      <td>[V, D, N, N, N, R, A, V, R, A, N, N, P, N, P, ...</td>\n",
       "      <td>[[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...</td>\n",
       "      <td>[[0.6185, 0.64254, -0.46552, 0.3757, 0.74838, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STUNNING: Hillary’s Own Numbers Show Her Tax H...</td>\n",
       "      <td>Wow! If I were Hillary, I d stop sending peopl...</td>\n",
       "      <td>left-news</td>\n",
       "      <td>Oct 9, 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>[stunning&lt;allcaps&gt;, :, hillary’s, own, numbers...</td>\n",
       "      <td>[A, ,, Z, A, N, V, D, N, N, N, V, V, A, N, A, ...</td>\n",
       "      <td>[wow, !, if, i, were, hillary, ,, i, d, stop, ...</td>\n",
       "      <td>[!, ,, P, O, V, ^, ,, O, V, V, V, N, P, D, N, ...</td>\n",
       "      <td>[[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...</td>\n",
       "      <td>[[-0.36362, 0.26759, 0.5763, -0.37784, 0.19068...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How Is Panama’s “Migrant Crisis” Giving A FREE...</td>\n",
       "      <td>Let this sink in The word has been out for som...</td>\n",
       "      <td>Government News</td>\n",
       "      <td>Aug 10, 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>[how, is, panama’s, “, migrant, crisis, ”, giv...</td>\n",
       "      <td>[R, V, Z, ,, A, N, ,, V, D, A, N, P, D, ^, P, ...</td>\n",
       "      <td>[let, this, sink, in, the, word, has, been, ou...</td>\n",
       "      <td>[V, O, V, P, D, N, V, V, T, P, D, N, R, P, D, ...</td>\n",
       "      <td>[[0.68938, -0.10644, 0.17083, -0.37583, 0.7517...</td>\n",
       "      <td>[[0.067025, -0.010427, 0.61778, -0.29952, 0.68...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THE LIST OF OBAMA’S HISTORIC FIRSTS AKA HOW CH...</td>\n",
       "      <td>Wow! What a list of accomplishments! The probl...</td>\n",
       "      <td>left-news</td>\n",
       "      <td>Apr 14, 2015</td>\n",
       "      <td>0</td>\n",
       "      <td>[the&lt;allcaps&gt;, list&lt;allcaps&gt;, of&lt;allcaps&gt;, oba...</td>\n",
       "      <td>[D, N, P, Z, A, N, G, R, ^, N, V, ^, R, A]</td>\n",
       "      <td>[wow, !, what, a, list, of, accomplishments, !...</td>\n",
       "      <td>[!, ,, O, D, N, P, N, ,, D, N, V, P, D, N, V, ...</td>\n",
       "      <td>[[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...</td>\n",
       "      <td>[[-0.36362, 0.26759, 0.5763, -0.37784, 0.19068...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Top tech executives to attend Trump summit on ...</td>\n",
       "      <td>NEW YORK (Reuters) - Top executives from Alpha...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 11, 2016</td>\n",
       "      <td>1</td>\n",
       "      <td>[top, tech, executives, to, attend, trump, sum...</td>\n",
       "      <td>[A, N, N, P, V, ^, N, P, ^, ,, ^]</td>\n",
       "      <td>[new&lt;allcaps&gt;, york&lt;allcaps&gt;, (, reuters, ), -...</td>\n",
       "      <td>[A, ^, ,, ^, ,, ,, A, N, P, ^, ^, ,, ^, ^, &amp;, ...</td>\n",
       "      <td>[[-0.66508, 0.78362, 0.74452, 0.68549, 0.648, ...</td>\n",
       "      <td>[[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  NEW LAW WILL PUNISH MUSLIM Migrants…Assimilate...   \n",
       "1  STUNNING: Hillary’s Own Numbers Show Her Tax H...   \n",
       "2  How Is Panama’s “Migrant Crisis” Giving A FREE...   \n",
       "3  THE LIST OF OBAMA’S HISTORIC FIRSTS AKA HOW CH...   \n",
       "4  Top tech executives to attend Trump summit on ...   \n",
       "\n",
       "                                                text          subject  \\\n",
       "0  Is this common sense law even practical given ...        left-news   \n",
       "1  Wow! If I were Hillary, I d stop sending peopl...        left-news   \n",
       "2  Let this sink in The word has been out for som...  Government News   \n",
       "3  Wow! What a list of accomplishments! The probl...        left-news   \n",
       "4  NEW YORK (Reuters) - Top executives from Alpha...     politicsNews   \n",
       "\n",
       "                 date target  \\\n",
       "0        Apr 23, 2016      0   \n",
       "1         Oct 9, 2016      0   \n",
       "2        Aug 10, 2016      0   \n",
       "3        Apr 14, 2015      0   \n",
       "4  December 11, 2016       1   \n",
       "\n",
       "                                        title_tokcan  \\\n",
       "0  [new<allcaps>, law<allcaps>, will<allcaps>, pu...   \n",
       "1  [stunning<allcaps>, :, hillary’s, own, numbers...   \n",
       "2  [how, is, panama’s, “, migrant, crisis, ”, giv...   \n",
       "3  [the<allcaps>, list<allcaps>, of<allcaps>, oba...   \n",
       "4  [top, tech, executives, to, attend, trump, sum...   \n",
       "\n",
       "                                           title_POS  \\\n",
       "0               [A, N, V, V, A, N, ,, V, &, V, T, ,]   \n",
       "1  [A, ,, Z, A, N, V, D, N, N, N, V, V, A, N, A, ...   \n",
       "2  [R, V, Z, ,, A, N, ,, V, D, A, N, P, D, ^, P, ...   \n",
       "3         [D, N, P, Z, A, N, G, R, ^, N, V, ^, R, A]   \n",
       "4                  [A, N, N, P, V, ^, N, P, ^, ,, ^]   \n",
       "\n",
       "                                         text_tokcan  \\\n",
       "0  [is, this, common, sense, law, even, practical...   \n",
       "1  [wow, !, if, i, were, hillary, ,, i, d, stop, ...   \n",
       "2  [let, this, sink, in, the, word, has, been, ou...   \n",
       "3  [wow, !, what, a, list, of, accomplishments, !...   \n",
       "4  [new<allcaps>, york<allcaps>, (, reuters, ), -...   \n",
       "\n",
       "                                            text_POS  \\\n",
       "0  [V, D, N, N, N, R, A, V, R, A, N, N, P, N, P, ...   \n",
       "1  [!, ,, P, O, V, ^, ,, O, V, V, V, N, P, D, N, ...   \n",
       "2  [V, O, V, P, D, N, V, V, T, P, D, N, R, P, D, ...   \n",
       "3  [!, ,, O, D, N, P, N, ,, D, N, V, P, D, N, V, ...   \n",
       "4  [A, ^, ,, ^, ,, ,, A, N, P, ^, ^, ,, ^, ^, &, ...   \n",
       "\n",
       "                                      embedded_title  \\\n",
       "0  [[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...   \n",
       "1  [[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...   \n",
       "2  [[0.68938, -0.10644, 0.17083, -0.37583, 0.7517...   \n",
       "3  [[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...   \n",
       "4  [[-0.66508, 0.78362, 0.74452, 0.68549, 0.648, ...   \n",
       "\n",
       "                                       embedded_text  \n",
       "0  [[0.6185, 0.64254, -0.46552, 0.3757, 0.74838, ...  \n",
       "1  [[-0.36362, 0.26759, 0.5763, -0.37784, 0.19068...  \n",
       "2  [[0.067025, -0.010427, 0.61778, -0.29952, 0.68...  \n",
       "3  [[-0.36362, 0.26759, 0.5763, -0.37784, 0.19068...  \n",
       "4  [[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select ISOT features and labels for training model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape: (31428,)\n",
      "[list(['brainiac<allcaps>', 'gets', 'rejected', 'after', 'trying', 'to', 'buy', 'bmw<allcaps>', 'with', 'ebt<allcaps>', 'card', '…', 'what', 'happens', 'next', 'is', 'hysterical<allcaps>', '!'])]\n",
      "train_labels shape: (31428,)\n",
      "[0 0 0 ... 0 1 0]\n",
      "\n",
      "dev_data shape: (6735,)\n",
      "[list(['turkey', 'condemns', 'u.s.', 'move', 'on', 'jerusalem', 'as', \"'\", 'irresponsible', \"'\"])]\n",
      "dev_labels shape: (6735,)\n",
      "[1 1 1 ... 0 0 0]\n",
      "\n",
      "test_data shape: (6735,)\n",
      "[list(['new<allcaps>', 'law<allcaps>', 'will<allcaps>', 'punish<allcaps>', 'muslim<allcaps>', 'migrants', '…', 'assimilate', 'or', 'get', 'out', '!'])]\n",
      "test_labels shape: (6735,)\n",
      "[0 0 0 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels = train_set.title_tokcan.values, train_set.target.values\n",
    "dev_data, dev_labels = dev_set.title_tokcan.values, dev_set.target.values\n",
    "test_data, test_labels = test_set.title_tokcan.values, test_set.target.values\n",
    "\n",
    "train_labels = train_labels.astype(int)\n",
    "dev_labels = dev_labels.astype(int)\n",
    "test_labels = test_labels.astype(int)\n",
    "\n",
    "#train_data.head()\n",
    "print('train_data shape:', train_data.shape)\n",
    "#print(train_data[0].shape)\n",
    "print(train_data[:1])\n",
    "print('train_labels shape:', train_labels.shape)\n",
    "print(train_labels)\n",
    "print()\n",
    "print('dev_data shape:', dev_data.shape)\n",
    "print(dev_data[:1])\n",
    "print('dev_labels shape:', dev_labels.shape)\n",
    "print(dev_labels)\n",
    "print()\n",
    "print('test_data shape:', test_data.shape)\n",
    "print(test_data[:1])\n",
    "print('test_labels shape:', test_labels.shape)\n",
    "print(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95th percentile: 25.0\n"
     ]
    }
   ],
   "source": [
    "# characterize length of documents in train_data\n",
    "\n",
    "lengths = [len(train_data[i]) for i in range(train_data.shape[0])]\n",
    "\n",
    "a = np.array(lengths)\n",
    "p = np.percentile(a, 95) # return 95th percentile\n",
    "print('95th percentile:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.1.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.1.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Bokeh for plotting.\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool\n",
    "bp.output_notebook()\n",
    "\n",
    "# Helper code for plotting histograms\n",
    "def plot_length_histogram(lengths, x_range=[0,100], bins=40, normed=True):\n",
    "    hist, bin_edges = np.histogram(a=lengths, bins=bins, normed=normed, range=x_range)\n",
    "    bin_centers = (bin_edges[1:] + bin_edges[:-1])/2\n",
    "    bin_widths =  (bin_edges[1:] - bin_edges[:-1])\n",
    "\n",
    "    hover = HoverTool(tooltips=[(\"bucket\", \"@x\"), (\"count\", \"@top\")], mode=\"vline\")\n",
    "    fig = bp.figure(plot_width=800, plot_height=400, tools=[hover])\n",
    "    fig.vbar(x=bin_centers, width=bin_widths, top=hist, hover_fill_color=\"firebrick\")\n",
    "    fig.y_range.start = 0\n",
    "    fig.x_range.start = 0\n",
    "    fig.xaxis.axis_label = \"Example length (number of tokens)\"\n",
    "    fig.yaxis.axis_label = \"Frequency\"\n",
    "    bp.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"0d3be73d-f11b-4175-944d-0a33c31f6db0\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"c42df3d6-e6b6-48eb-867b-a0486f385ab0\":{\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1012\",\"type\":\"LinearAxis\"}],\"left\":[{\"id\":\"1017\",\"type\":\"LinearAxis\"}],\"plot_height\":400,\"plot_width\":800,\"renderers\":[{\"id\":\"1012\",\"type\":\"LinearAxis\"},{\"id\":\"1016\",\"type\":\"Grid\"},{\"id\":\"1017\",\"type\":\"LinearAxis\"},{\"id\":\"1021\",\"type\":\"Grid\"},{\"id\":\"1028\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"1030\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"1022\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"1004\",\"type\":\"DataRange1d\"},\"x_scale\":{\"id\":\"1008\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"1006\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"1010\",\"type\":\"LinearScale\"}},\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"1032\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"1018\",\"type\":\"BasicTicker\"},{\"attributes\":{\"plot\":{\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1013\",\"type\":\"BasicTicker\"}},\"id\":\"1016\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1013\",\"type\":\"BasicTicker\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"field\":\"width\"},\"x\":{\"field\":\"x\"}},\"id\":\"1026\",\"type\":\"VBar\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1018\",\"type\":\"BasicTicker\"}},\"id\":\"1021\",\"type\":\"Grid\"},{\"attributes\":{\"source\":{\"id\":\"1024\",\"type\":\"ColumnDataSource\"}},\"id\":\"1029\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1037\",\"type\":\"Selection\"},{\"attributes\":{\"axis_label\":\"Example length (number of tokens)\",\"formatter\":{\"id\":\"1032\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1013\",\"type\":\"BasicTicker\"}},\"id\":\"1012\",\"type\":\"LinearAxis\"},{\"attributes\":{\"plot\":null,\"text\":\"\"},\"id\":\"1030\",\"type\":\"Title\"},{\"attributes\":{\"callback\":null,\"start\":0},\"id\":\"1004\",\"type\":\"DataRange1d\"},{\"attributes\":{\"axis_label\":\"Frequency\",\"formatter\":{\"id\":\"1034\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1018\",\"type\":\"BasicTicker\"}},\"id\":\"1017\",\"type\":\"LinearAxis\"},{\"attributes\":{\"callback\":null,\"data\":{\"top\":{\"__ndarray__\":\"4nFM2QaxGj/icUzZBrEaP79OHOjJGIQ//hN0X6iKqj+0nK127B3AP3T4rfta3q4/5tma2XOWsD8eAHa0JweeP1OENsqF+Zk/3RWxVdrDhT/3HLfMBa6DP9iR53DKTnE/yBrqnxS3bT9rtUCeN+VVP/oYC9W/ulY/4nFM2QaxSj+m4yL+5Vo3P8Sqt2v2BSk/hxyOkNWvJT8tx89HpK4QP2lV+SLFBAQ/4nFM2Qax6j4AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=\",\"dtype\":\"float64\",\"shape\":[40]},\"width\":{\"__ndarray__\":\"AAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEA=\",\"dtype\":\"float64\",\"shape\":[40]},\"x\":{\"__ndarray__\":\"AAAAAAAA9D8AAAAAAAAOQAAAAAAAABlAAAAAAACAIUAAAAAAAIAmQAAAAAAAgCtAAAAAAABAMEAAAAAAAMAyQAAAAAAAQDVAAAAAAADAN0AAAAAAAEA6QAAAAAAAwDxAAAAAAABAP0AAAAAAAOBAQAAAAAAAIEJAAAAAAABgQ0AAAAAAAKBEQAAAAAAA4EVAAAAAAAAgR0AAAAAAAGBIQAAAAAAAoElAAAAAAADgSkAAAAAAACBMQAAAAAAAYE1AAAAAAACgTkAAAAAAAOBPQAAAAAAAkFBAAAAAAAAwUUAAAAAAANBRQAAAAAAAcFJAAAAAAAAQU0AAAAAAALBTQAAAAAAAUFRAAAAAAADwVEAAAAAAAJBVQAAAAAAAMFZAAAAAAADQVkAAAAAAAHBXQAAAAAAAEFhAAAAAAACwWEA=\",\"dtype\":\"float64\",\"shape\":[40]}},\"selected\":{\"id\":\"1037\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"1036\",\"type\":\"UnionRenderers\"}},\"id\":\"1024\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"fill_color\":{\"value\":\"firebrick\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"field\":\"width\"},\"x\":{\"field\":\"x\"}},\"id\":\"1027\",\"type\":\"VBar\"},{\"attributes\":{\"callback\":null,\"mode\":\"vline\",\"renderers\":\"auto\",\"tooltips\":[[\"bucket\",\"@x\"],[\"count\",\"@top\"]]},\"id\":\"1002\",\"type\":\"HoverTool\"},{\"attributes\":{\"callback\":null,\"start\":0},\"id\":\"1006\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"1036\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"fill_color\":{\"value\":\"#1f77b4\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"field\":\"width\"},\"x\":{\"field\":\"x\"}},\"id\":\"1025\",\"type\":\"VBar\"},{\"attributes\":{},\"id\":\"1010\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"1008\",\"type\":\"LinearScale\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1002\",\"type\":\"HoverTool\"}]},\"id\":\"1022\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"1034\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"data_source\":{\"id\":\"1024\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"1025\",\"type\":\"VBar\"},\"hover_glyph\":{\"id\":\"1027\",\"type\":\"VBar\"},\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1026\",\"type\":\"VBar\"},\"selection_glyph\":null,\"view\":{\"id\":\"1029\",\"type\":\"CDSView\"}},\"id\":\"1028\",\"type\":\"GlyphRenderer\"}],\"root_ids\":[\"1003\"]},\"title\":\"Bokeh Application\",\"version\":\"1.0.1\"}};\n",
       "  var render_items = [{\"docid\":\"c42df3d6-e6b6-48eb-867b-a0486f385ab0\",\"roots\":{\"1003\":\"0d3be73d-f11b-4175-944d-0a33c31f6db0\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1003"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_length_histogram(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LIAR data to evaluate various models below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23258 entries, 0 to 23257\n",
      "Data columns (total 6 columns):\n",
      "target            23258 non-null int64\n",
      "title             23258 non-null object\n",
      "title_tokcan      23258 non-null object\n",
      "title_POS         23258 non-null object\n",
      "binary_target     23258 non-null int64\n",
      "embedded_title    23258 non-null object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 19.1 MB\n"
     ]
    }
   ],
   "source": [
    "# DON\"T Read LIAR data from pickle file. ****************\n",
    "#liar_data = pd.read_pickle('parsed_data/df_liardata2.pkl')  # data (CMU) tokenized and POS tags added\n",
    "# Heads up on the df_liardata2.pkl: it looks like during the process, \n",
    "# \"mostly-false\" was used in place of \"barely_true\".  \n",
    "# I believe it means that the \"barely_true\" items were omitted from the pickled file.\n",
    "# ^^^^^^^^^^^\n",
    "\n",
    "\n",
    "#### USE THIS ONE WHEN AVAILABLE!!!!!!!!!!!\n",
    "#liar_data = pd.read_pickle('parsed_data/df_liardata2binary.pkl')  # data (CMU) tokenized and POS tags added\n",
    "liar_data = pd.read_pickle('parsed_data/df_liarpolitifact_data_embed.pkl')\n",
    "\n",
    "\n",
    "liar_data.info(memory_usage='deep', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>title</th>\n",
       "      <th>title_tokcan</th>\n",
       "      <th>title_POS</th>\n",
       "      <th>binary_target</th>\n",
       "      <th>embedded_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Says 31 percent of Texas physicians accept all...</td>\n",
       "      <td>[says, &lt;number&gt;, percent, of, texas, physician...</td>\n",
       "      <td>[V, $, N, P, ^, N, V, D, A, ^, N, ,, R, P, $, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.11797, 0.21126, 0.29075, -0.021211, 0.7819...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>''Both Democrats and Republicans are advocatin...</td>\n",
       "      <td>['', both, democrats, and, republicans, are, a...</td>\n",
       "      <td>[,, D, N, &amp;, N, V, V, P, D, N, P, N, N, V, P, ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[[0.0028594, 0.19457, -0.19449, -0.037583, 0.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>A Republican-led softening of firearms trainin...</td>\n",
       "      <td>[a, republican-led, softening, of, firearms, t...</td>\n",
       "      <td>[D, A, N, P, N, N, N, V, D, A, N, V, V, V, P, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.21705, 0.46515, -0.46757, 0.10082, 1.0135,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>The first tweet was sent from Austin.</td>\n",
       "      <td>[the, first, tweet, was, sent, from, austin, .]</td>\n",
       "      <td>[D, A, N, V, V, P, ^, ,]</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Georgia has the countrys second highest number...</td>\n",
       "      <td>[georgia, has, the, countrys, second, highest,...</td>\n",
       "      <td>[^, V, D, N, A, A, N, P, A, N, N, N, ,]</td>\n",
       "      <td>-1</td>\n",
       "      <td>[[-1.3427, 0.4592, 0.19281, 0.71305, -0.5934, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>Because of Gov. Scott Walkers budgeting, a gre...</td>\n",
       "      <td>[because, of, gov, ., scott, walkers, budgetin...</td>\n",
       "      <td>[P, P, ^, ,, ^, ^, N, ,, D, A, N, P, A, N, N, ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[[0.52905, -0.30145, 0.056191, -0.17905, 0.156...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Florida has reduced its carbon emissions by 20...</td>\n",
       "      <td>[florida, has, reduced, its, carbon, emissions...</td>\n",
       "      <td>[^, V, V, L, N, N, P, $, N, P, $, ,]</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.52717, 0.16878, 0.16146, 0.93858, -0.6549...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>Louisianas film incentives program is so big t...</td>\n",
       "      <td>[louisianas, film, incentives, program, is, so...</td>\n",
       "      <td>[^, N, N, N, V, R, A, O, R, N, D, A, N, R, V, ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>Under the Obama economy ... utility bills are ...</td>\n",
       "      <td>[under, the, obama, economy, .&lt;repeat&gt;, utilit...</td>\n",
       "      <td>[P, D, ^, N, ,, N, N, V, A, ,]</td>\n",
       "      <td>-1</td>\n",
       "      <td>[[0.13721, -0.295, -0.05916, -0.59235, 0.02301...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>Mt. Hood Community College is No. 1 on average...</td>\n",
       "      <td>[mt, ., hood, community, college, is, no, ., &lt;...</td>\n",
       "      <td>[^, ,, N, N, N, V, !, ,, $, P, A, &amp;, A, N, N, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.055441, 2.3025, 0.98466, -0.020482, -0.26...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                              title  \\\n",
       "0       1  Says 31 percent of Texas physicians accept all...   \n",
       "1       2  ''Both Democrats and Republicans are advocatin...   \n",
       "2       0  A Republican-led softening of firearms trainin...   \n",
       "3       5              The first tweet was sent from Austin.   \n",
       "4       2  Georgia has the countrys second highest number...   \n",
       "5       2  Because of Gov. Scott Walkers budgeting, a gre...   \n",
       "6       1  Florida has reduced its carbon emissions by 20...   \n",
       "7       2  Louisianas film incentives program is so big t...   \n",
       "8       2  Under the Obama economy ... utility bills are ...   \n",
       "9       1  Mt. Hood Community College is No. 1 on average...   \n",
       "\n",
       "                                        title_tokcan  \\\n",
       "0  [says, <number>, percent, of, texas, physician...   \n",
       "1  ['', both, democrats, and, republicans, are, a...   \n",
       "2  [a, republican-led, softening, of, firearms, t...   \n",
       "3    [the, first, tweet, was, sent, from, austin, .]   \n",
       "4  [georgia, has, the, countrys, second, highest,...   \n",
       "5  [because, of, gov, ., scott, walkers, budgetin...   \n",
       "6  [florida, has, reduced, its, carbon, emissions...   \n",
       "7  [louisianas, film, incentives, program, is, so...   \n",
       "8  [under, the, obama, economy, .<repeat>, utilit...   \n",
       "9  [mt, ., hood, community, college, is, no, ., <...   \n",
       "\n",
       "                                           title_POS  binary_target  \\\n",
       "0  [V, $, N, P, ^, N, V, D, A, ^, N, ,, R, P, $, ...              1   \n",
       "1  [,, D, N, &, N, V, V, P, D, N, P, N, N, V, P, ...             -1   \n",
       "2  [D, A, N, P, N, N, N, V, D, A, N, V, V, V, P, ...              1   \n",
       "3                           [D, A, N, V, V, P, ^, ,]              0   \n",
       "4            [^, V, D, N, A, A, N, P, A, N, N, N, ,]             -1   \n",
       "5  [P, P, ^, ,, ^, ^, N, ,, D, A, N, P, A, N, N, ...             -1   \n",
       "6               [^, V, V, L, N, N, P, $, N, P, $, ,]              1   \n",
       "7  [^, N, N, N, V, R, A, O, R, N, D, A, N, R, V, ...             -1   \n",
       "8                     [P, D, ^, N, ,, N, N, V, A, ,]             -1   \n",
       "9  [^, ,, N, N, N, V, !, ,, $, P, A, &, A, N, N, ...              1   \n",
       "\n",
       "                                      embedded_title  \n",
       "0  [[0.11797, 0.21126, 0.29075, -0.021211, 0.7819...  \n",
       "1  [[0.0028594, 0.19457, -0.19449, -0.037583, 0.9...  \n",
       "2  [[0.21705, 0.46515, -0.46757, 0.10082, 1.0135,...  \n",
       "3  [[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -...  \n",
       "4  [[-1.3427, 0.4592, 0.19281, 0.71305, -0.5934, ...  \n",
       "5  [[0.52905, -0.30145, 0.056191, -0.17905, 0.156...  \n",
       "6  [[-0.52717, 0.16878, 0.16146, 0.93858, -0.6549...  \n",
       "7  [[-0.79149, 0.86617, 0.11998, 0.00092287, 0.27...  \n",
       "8  [[0.13721, -0.295, -0.05916, -0.59235, 0.02301...  \n",
       "9  [[-0.055441, 2.3025, 0.98466, -0.020482, -0.26...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liar_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 -1  0]\n",
      "\n",
      "binary_target,  number of examples\n",
      "1 8283\n",
      "-1 4776\n",
      "0 10199\n"
     ]
    }
   ],
   "source": [
    "binary_targets = liar_data.binary_target.unique()\n",
    "print(binary_targets)\n",
    "\n",
    "print('\\nbinary_target,  number of examples')\n",
    "for binary_target in binary_targets:\n",
    "    print(binary_target, len(liar_data[liar_data.binary_target==binary_target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>title</th>\n",
       "      <th>title_tokcan</th>\n",
       "      <th>title_POS</th>\n",
       "      <th>binary_target</th>\n",
       "      <th>embedded_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Says 31 percent of Texas physicians accept all...</td>\n",
       "      <td>[says, &lt;number&gt;, percent, of, texas, physician...</td>\n",
       "      <td>[V, $, N, P, ^, N, V, D, A, ^, N, ,, R, P, $, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.11797, 0.21126, 0.29075, -0.021211, 0.7819...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>A Republican-led softening of firearms trainin...</td>\n",
       "      <td>[a, republican-led, softening, of, firearms, t...</td>\n",
       "      <td>[D, A, N, P, N, N, N, V, D, A, N, V, V, V, P, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.21705, 0.46515, -0.46757, 0.10082, 1.0135,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>The first tweet was sent from Austin.</td>\n",
       "      <td>[the, first, tweet, was, sent, from, austin, .]</td>\n",
       "      <td>[D, A, N, V, V, P, ^, ,]</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Florida has reduced its carbon emissions by 20...</td>\n",
       "      <td>[florida, has, reduced, its, carbon, emissions...</td>\n",
       "      <td>[^, V, V, L, N, N, P, $, N, P, $, ,]</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.52717, 0.16878, 0.16146, 0.93858, -0.6549...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>Mt. Hood Community College is No. 1 on average...</td>\n",
       "      <td>[mt, ., hood, community, college, is, no, ., &lt;...</td>\n",
       "      <td>[^, ,, N, N, N, V, !, ,, $, P, A, &amp;, A, N, N, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.055441, 2.3025, 0.98466, -0.020482, -0.26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>Latinos now make up the majority population in...</td>\n",
       "      <td>[latinos, now, make, up, the, majority, popula...</td>\n",
       "      <td>[N, R, V, T, D, N, N, P, ^, ,]</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.89161, -0.52478, 0.36263, -0.56291, 0.110...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>They were going to build the wall a while ago,...</td>\n",
       "      <td>[they, were, going, to, build, the, wall, a, w...</td>\n",
       "      <td>[O, V, V, P, V, D, N, N, N, R, ,, R, R, A, R, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.70835, -0.57361, 0.15375, -0.63335, 0.4687...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>Says Texas has been waiting for two years for ...</td>\n",
       "      <td>[\", says, texas, has, been, waiting, for, two,...</td>\n",
       "      <td>[,, V, ^, V, V, V, P, $, N, P, D, A, N, P, V, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.25769, 0.45629, -0.76974, -0.37679, 0.5927...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "      <td>Barack Hussein Obama will ... force courts to ...</td>\n",
       "      <td>[barack, hussein, obama, will, .&lt;repeat&gt;, forc...</td>\n",
       "      <td>[^, ^, ^, V, ,, V, N, P, V, ^, ^, N, P, A, N, ,]</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.21021, 0.8672, 1.1193, -0.35676, 0.042284,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>Austin has more lobbyists working for it than ...</td>\n",
       "      <td>[austin, has, more, lobbyists, working, for, i...</td>\n",
       "      <td>[^, V, A, N, V, P, O, P, D, A, N, P, ^, ,]</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.88499, 0.73486, 0.42771, 0.53152, -0.3327...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    target                                              title  \\\n",
       "0        1  Says 31 percent of Texas physicians accept all...   \n",
       "2        0  A Republican-led softening of firearms trainin...   \n",
       "3        5              The first tweet was sent from Austin.   \n",
       "6        1  Florida has reduced its carbon emissions by 20...   \n",
       "9        1  Mt. Hood Community College is No. 1 on average...   \n",
       "10       4  Latinos now make up the majority population in...   \n",
       "11       3  They were going to build the wall a while ago,...   \n",
       "12       4  Says Texas has been waiting for two years for ...   \n",
       "13       5  Barack Hussein Obama will ... force courts to ...   \n",
       "14       1  Austin has more lobbyists working for it than ...   \n",
       "\n",
       "                                         title_tokcan  \\\n",
       "0   [says, <number>, percent, of, texas, physician...   \n",
       "2   [a, republican-led, softening, of, firearms, t...   \n",
       "3     [the, first, tweet, was, sent, from, austin, .]   \n",
       "6   [florida, has, reduced, its, carbon, emissions...   \n",
       "9   [mt, ., hood, community, college, is, no, ., <...   \n",
       "10  [latinos, now, make, up, the, majority, popula...   \n",
       "11  [they, were, going, to, build, the, wall, a, w...   \n",
       "12  [\", says, texas, has, been, waiting, for, two,...   \n",
       "13  [barack, hussein, obama, will, .<repeat>, forc...   \n",
       "14  [austin, has, more, lobbyists, working, for, i...   \n",
       "\n",
       "                                            title_POS  binary_target  \\\n",
       "0   [V, $, N, P, ^, N, V, D, A, ^, N, ,, R, P, $, ...              1   \n",
       "2   [D, A, N, P, N, N, N, V, D, A, N, V, V, V, P, ...              1   \n",
       "3                            [D, A, N, V, V, P, ^, ,]              0   \n",
       "6                [^, V, V, L, N, N, P, $, N, P, $, ,]              1   \n",
       "9   [^, ,, N, N, N, V, !, ,, $, P, A, &, A, N, N, ...              1   \n",
       "10                     [N, R, V, T, D, N, N, P, ^, ,]              0   \n",
       "11  [O, V, V, P, V, D, N, N, N, R, ,, R, R, A, R, ...              0   \n",
       "12  [,, V, ^, V, V, V, P, $, N, P, D, A, N, P, V, ...              0   \n",
       "13   [^, ^, ^, V, ,, V, N, P, V, ^, ^, N, P, A, N, ,]              0   \n",
       "14         [^, V, A, N, V, P, O, P, D, A, N, P, ^, ,]              1   \n",
       "\n",
       "                                       embedded_title  \n",
       "0   [[0.11797, 0.21126, 0.29075, -0.021211, 0.7819...  \n",
       "2   [[0.21705, 0.46515, -0.46757, 0.10082, 1.0135,...  \n",
       "3   [[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -...  \n",
       "6   [[-0.52717, 0.16878, 0.16146, 0.93858, -0.6549...  \n",
       "9   [[-0.055441, 2.3025, 0.98466, -0.020482, -0.26...  \n",
       "10  [[-0.89161, -0.52478, 0.36263, -0.56291, 0.110...  \n",
       "11  [[0.70835, -0.57361, 0.15375, -0.63335, 0.4687...  \n",
       "12  [[0.25769, 0.45629, -0.76974, -0.37679, 0.5927...  \n",
       "13  [[0.21021, 0.8672, 1.1193, -0.35676, 0.042284,...  \n",
       "14  [[-0.88499, 0.73486, 0.42771, 0.53152, -0.3327...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liar_data_binary = liar_data[liar_data.binary_target >= 0]  ## discard \"half-true\"!!!!\n",
    "liar_data_binary.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>title</th>\n",
       "      <th>title_tokcan</th>\n",
       "      <th>title_POS</th>\n",
       "      <th>binary_target</th>\n",
       "      <th>embedded_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Says 31 percent of Texas physicians accept all...</td>\n",
       "      <td>[says, &lt;number&gt;, percent, of, texas, physician...</td>\n",
       "      <td>[V, $, N, P, ^, N, V, D, A, ^, N, ,, R, P, $, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.11797, 0.21126, 0.29075, -0.021211, 0.7819...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>A Republican-led softening of firearms trainin...</td>\n",
       "      <td>[a, republican-led, softening, of, firearms, t...</td>\n",
       "      <td>[D, A, N, P, N, N, N, V, D, A, N, V, V, V, P, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.21705, 0.46515, -0.46757, 0.10082, 1.0135,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>The first tweet was sent from Austin.</td>\n",
       "      <td>[the, first, tweet, was, sent, from, austin, .]</td>\n",
       "      <td>[D, A, N, V, V, P, ^, ,]</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Florida has reduced its carbon emissions by 20...</td>\n",
       "      <td>[florida, has, reduced, its, carbon, emissions...</td>\n",
       "      <td>[^, V, V, L, N, N, P, $, N, P, $, ,]</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.52717, 0.16878, 0.16146, 0.93858, -0.6549...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Mt. Hood Community College is No. 1 on average...</td>\n",
       "      <td>[mt, ., hood, community, college, is, no, ., &lt;...</td>\n",
       "      <td>[^, ,, N, N, N, V, !, ,, $, P, A, &amp;, A, N, N, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.055441, 2.3025, 0.98466, -0.020482, -0.26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>Latinos now make up the majority population in...</td>\n",
       "      <td>[latinos, now, make, up, the, majority, popula...</td>\n",
       "      <td>[N, R, V, T, D, N, N, P, ^, ,]</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.89161, -0.52478, 0.36263, -0.56291, 0.110...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>They were going to build the wall a while ago,...</td>\n",
       "      <td>[they, were, going, to, build, the, wall, a, w...</td>\n",
       "      <td>[O, V, V, P, V, D, N, N, N, R, ,, R, R, A, R, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.70835, -0.57361, 0.15375, -0.63335, 0.4687...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>Says Texas has been waiting for two years for ...</td>\n",
       "      <td>[\", says, texas, has, been, waiting, for, two,...</td>\n",
       "      <td>[,, V, ^, V, V, V, P, $, N, P, D, A, N, P, V, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.25769, 0.45629, -0.76974, -0.37679, 0.5927...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>Barack Hussein Obama will ... force courts to ...</td>\n",
       "      <td>[barack, hussein, obama, will, .&lt;repeat&gt;, forc...</td>\n",
       "      <td>[^, ^, ^, V, ,, V, N, P, V, ^, ^, N, P, A, N, ,]</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.21021, 0.8672, 1.1193, -0.35676, 0.042284,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>Austin has more lobbyists working for it than ...</td>\n",
       "      <td>[austin, has, more, lobbyists, working, for, i...</td>\n",
       "      <td>[^, V, A, N, V, P, O, P, D, A, N, P, ^, ,]</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.88499, 0.73486, 0.42771, 0.53152, -0.3327...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                              title  \\\n",
       "0       1  Says 31 percent of Texas physicians accept all...   \n",
       "1       0  A Republican-led softening of firearms trainin...   \n",
       "2       5              The first tweet was sent from Austin.   \n",
       "3       1  Florida has reduced its carbon emissions by 20...   \n",
       "4       1  Mt. Hood Community College is No. 1 on average...   \n",
       "5       4  Latinos now make up the majority population in...   \n",
       "6       3  They were going to build the wall a while ago,...   \n",
       "7       4  Says Texas has been waiting for two years for ...   \n",
       "8       5  Barack Hussein Obama will ... force courts to ...   \n",
       "9       1  Austin has more lobbyists working for it than ...   \n",
       "\n",
       "                                        title_tokcan  \\\n",
       "0  [says, <number>, percent, of, texas, physician...   \n",
       "1  [a, republican-led, softening, of, firearms, t...   \n",
       "2    [the, first, tweet, was, sent, from, austin, .]   \n",
       "3  [florida, has, reduced, its, carbon, emissions...   \n",
       "4  [mt, ., hood, community, college, is, no, ., <...   \n",
       "5  [latinos, now, make, up, the, majority, popula...   \n",
       "6  [they, were, going, to, build, the, wall, a, w...   \n",
       "7  [\", says, texas, has, been, waiting, for, two,...   \n",
       "8  [barack, hussein, obama, will, .<repeat>, forc...   \n",
       "9  [austin, has, more, lobbyists, working, for, i...   \n",
       "\n",
       "                                           title_POS  binary_target  \\\n",
       "0  [V, $, N, P, ^, N, V, D, A, ^, N, ,, R, P, $, ...              1   \n",
       "1  [D, A, N, P, N, N, N, V, D, A, N, V, V, V, P, ...              1   \n",
       "2                           [D, A, N, V, V, P, ^, ,]              0   \n",
       "3               [^, V, V, L, N, N, P, $, N, P, $, ,]              1   \n",
       "4  [^, ,, N, N, N, V, !, ,, $, P, A, &, A, N, N, ...              1   \n",
       "5                     [N, R, V, T, D, N, N, P, ^, ,]              0   \n",
       "6  [O, V, V, P, V, D, N, N, N, R, ,, R, R, A, R, ...              0   \n",
       "7  [,, V, ^, V, V, V, P, $, N, P, D, A, N, P, V, ...              0   \n",
       "8   [^, ^, ^, V, ,, V, N, P, V, ^, ^, N, P, A, N, ,]              0   \n",
       "9         [^, V, A, N, V, P, O, P, D, A, N, P, ^, ,]              1   \n",
       "\n",
       "                                      embedded_title  \n",
       "0  [[0.11797, 0.21126, 0.29075, -0.021211, 0.7819...  \n",
       "1  [[0.21705, 0.46515, -0.46757, 0.10082, 1.0135,...  \n",
       "2  [[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -...  \n",
       "3  [[-0.52717, 0.16878, 0.16146, 0.93858, -0.6549...  \n",
       "4  [[-0.055441, 2.3025, 0.98466, -0.020482, -0.26...  \n",
       "5  [[-0.89161, -0.52478, 0.36263, -0.56291, 0.110...  \n",
       "6  [[0.70835, -0.57361, 0.15375, -0.63335, 0.4687...  \n",
       "7  [[0.25769, 0.45629, -0.76974, -0.37679, 0.5927...  \n",
       "8  [[0.21021, 0.8672, 1.1193, -0.35676, 0.042284,...  \n",
       "9  [[-0.88499, 0.73486, 0.42771, 0.53152, -0.3327...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liar_data_binary = liar_data_binary.reset_index(drop=True)\n",
    "liar_data_binary.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIAR true: 8283\n",
      "LIAR false: 10199\n"
     ]
    }
   ],
   "source": [
    "print('LIAR true:', len(liar_data[liar_data.binary_target == 1]))\n",
    "print('LIAR false:', len(liar_data[liar_data.binary_target == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liar titles: [list(['says', '<number>', 'percent', 'of', 'texas', 'physicians', 'accept', 'all', 'new', 'medicaid', 'patients', ',', 'down', 'from', '<number>', 'percent', 'in', '<number>', '.'])\n",
      " list(['a', 'republican-led', 'softening', 'of', 'firearms', 'training', 'rules', 'means', 'that', 'untrained', 'individuals', 'would', 'be', 'allowed', 'to', 'carry', 'guns', 'with', 'a', 'state', 'permit', '.'])\n",
      " list(['the', 'first', 'tweet', 'was', 'sent', 'from', 'austin', '.']) ...\n",
      " list(['rep.', 'adam', 'putnam', 'was', 'silent', 'when', 'rush', 'limbaugh', 'called', 'sonia', 'sotomayor', 'a', 'racist', '.'])\n",
      " list(['elorza', 'wants', 'to', 'teach', 'our', 'public', 'school', 'children', 'about', 'the', 'non-existence', 'of', 'god', '.'])\n",
      " list(['we', 'had', 'people', 'that', 'were', 'getting', 'killed', '(', 'in', 'benghazi', ')', ',', 'we', 'had', 'people', 'who', 'are', 'willing', 'to', 'risk', 'their', 'lives', 'to', 'go', 'save', 'them', ',', 'and', 'somebody', 'told', 'them', 'to', 'stand', 'down', '.'])]\n",
      "liar labels: [1 1 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "liar_title_tokans = liar_data_binary.title_tokcan.values\n",
    "liar_labels = liar_data_binary.binary_target.values\n",
    "\n",
    "print('liar titles:', liar_title_tokans)\n",
    "print('liar labels:', liar_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN: ISOT \"title\" data WITH GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add in reference functions for viewing convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef build_vocab(corpus, V=10000, **kw):\\n    from . import vocabulary\\n    if isinstance(corpus, list):\\n        token_feed = (canonicalize_word(w) for w in corpus)\\n        vocab = vocabulary.Vocabulary(token_feed, size=V, **kw)\\n    else:\\n        token_feed = (canonicalize_word(w) for w in corpus.words())\\n        vocab = vocabulary.Vocabulary(token_feed, size=V, **kw)\\n\\n    print(\"Vocabulary: {:,} types\".format(vocab.size))\\n    return vocab\\n\\n# Window and batch functions\\ndef pad_np_array(example_ids, max_len=250, pad_id=0):\\n    \"\"\"Pad a list of lists of ids into a rectangular NumPy array.\\n\\n    Longer sequences will be truncated to max_len ids, while shorter ones will\\n    be padded with pad_id.\\n\\n    Args:\\n        example_ids: list(list(int)), sequence of ids for each example\\n        max_len: maximum sequence length\\n        pad_id: id to pad shorter sequences with\\n\\n    Returns: (x, ns)\\n        x: [num_examples, max_len] NumPy array of integer ids\\n        ns: [num_examples] NumPy array of sequence lengths (<= max_len)\\n    \"\"\"\\n    arr = np.full([len(example_ids), max_len], pad_id, dtype=np.int32)\\n    ns = np.zeros([len(example_ids)], dtype=np.int32)\\n    for i, ids in enumerate(example_ids):\\n        cpy_len = min(len(ids), max_len)\\n        arr[i,:cpy_len] = ids[:cpy_len]\\n        ns[i] = cpy_len\\n    return arr, ns\\n\\ndef id_lists_to_sparse_bow(id_lists, vocab_size):\\n    \"\"\"Convert a list-of-lists-of-ids to a sparse bag-of-words matrix.\\n\\n    Args:\\n        id_lists: (list(list(int))) list of lists of word ids\\n        vocab_size: (int) vocab size; must be greater than the largest word id\\n            in id_lists.\\n\\n    Returns:\\n        (scipy.sparse.csr_matrix) where each row is a sparse vector of word\\n        counts for the corresponding example.\\n    \"\"\"\\n    from scipy import sparse\\n    ii = []  # row indices (example ids)\\n    jj = []  # column indices (token ids)\\n    for row_id, ids in enumerate(id_lists):\\n        ii.extend([row_id]*len(ids))\\n        jj.extend(ids)\\n    x = sparse.csr_matrix((np.ones_like(ii), (ii, jj)),\\n                          shape=[len(id_lists), vocab_size])\\n    return x\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# May need this info (from utils.py)\n",
    "'''\n",
    "def build_vocab(corpus, V=10000, **kw):\n",
    "    from . import vocabulary\n",
    "    if isinstance(corpus, list):\n",
    "        token_feed = (canonicalize_word(w) for w in corpus)\n",
    "        vocab = vocabulary.Vocabulary(token_feed, size=V, **kw)\n",
    "    else:\n",
    "        token_feed = (canonicalize_word(w) for w in corpus.words())\n",
    "        vocab = vocabulary.Vocabulary(token_feed, size=V, **kw)\n",
    "\n",
    "    print(\"Vocabulary: {:,} types\".format(vocab.size))\n",
    "    return vocab\n",
    "\n",
    "# Window and batch functions\n",
    "def pad_np_array(example_ids, max_len=250, pad_id=0):\n",
    "    \"\"\"Pad a list of lists of ids into a rectangular NumPy array.\n",
    "\n",
    "    Longer sequences will be truncated to max_len ids, while shorter ones will\n",
    "    be padded with pad_id.\n",
    "\n",
    "    Args:\n",
    "        example_ids: list(list(int)), sequence of ids for each example\n",
    "        max_len: maximum sequence length\n",
    "        pad_id: id to pad shorter sequences with\n",
    "\n",
    "    Returns: (x, ns)\n",
    "        x: [num_examples, max_len] NumPy array of integer ids\n",
    "        ns: [num_examples] NumPy array of sequence lengths (<= max_len)\n",
    "    \"\"\"\n",
    "    arr = np.full([len(example_ids), max_len], pad_id, dtype=np.int32)\n",
    "    ns = np.zeros([len(example_ids)], dtype=np.int32)\n",
    "    for i, ids in enumerate(example_ids):\n",
    "        cpy_len = min(len(ids), max_len)\n",
    "        arr[i,:cpy_len] = ids[:cpy_len]\n",
    "        ns[i] = cpy_len\n",
    "    return arr, ns\n",
    "\n",
    "def id_lists_to_sparse_bow(id_lists, vocab_size):\n",
    "    \"\"\"Convert a list-of-lists-of-ids to a sparse bag-of-words matrix.\n",
    "\n",
    "    Args:\n",
    "        id_lists: (list(list(int))) list of lists of word ids\n",
    "        vocab_size: (int) vocab size; must be greater than the largest word id\n",
    "            in id_lists.\n",
    "\n",
    "    Returns:\n",
    "        (scipy.sparse.csr_matrix) where each row is a sparse vector of word\n",
    "        counts for the corresponding example.\n",
    "    \"\"\"\n",
    "    from scipy import sparse\n",
    "    ii = []  # row indices (example ids)\n",
    "    jj = []  # column indices (token ids)\n",
    "    for row_id, ids in enumerate(id_lists):\n",
    "        ii.extend([row_id]*len(ids))\n",
    "        jj.extend(ids)\n",
    "    x = sparse.csr_matrix((np.ones_like(ii), (ii, jj)),\n",
    "                          shape=[len(id_lists), vocab_size])\n",
    "    return x\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef get_filtered_split(split=\\'train\\', df_idxs=None, root_only=False):\\n    if not hasattr(split):\\n        raise ValueError(\"Invalid split name \\'%s\\'\" % name)\\n    df = getattr(split)\\n    if df_idxs is not None:\\n        df = df.loc[df_idxs]\\n    #if root_only:          # Should not need in Final Project.\\n        #df = df[df.is_root]\\n    return df\\n\\ndef as_padded_array(split=\\'train\\', max_len=40, pad_id=0,\\n                    root_only=False, df_idxs=None):\\n    \"\"\"Return the dataset as a (padded) NumPy array.\\n    Longer sequences will be truncated to max_len ids, while shorter ones\\n    will be padded with pad_id.\\n    Args:\\n      split: \\'train\\' or \\'test\\'\\n      max_len: maximum sequence length\\n      pad_id: id to pad shorter sequences with\\n      root_only: if true, will only export root phrases\\n      df_idxs: (optional) custom list of indices to export\\n    Returns: (x, ns, y)\\n      x: [num_examples, max_len] NumPy array of integer ids\\n      ns: [num_examples] NumPy array of sequence lengths (<= max_len)\\n      y: [num_examples] NumPy array of target ids\\n    \"\"\"\\n    df = get_filtered_split(split, df_idxs, root_only)\\n    x, ns = utils.pad_np_array(df.ids, max_len=max_len, pad_id=pad_id)\\n    return x, ns, np.array(df.label, dtype=np.int32)\\n\\ndef as_sparse_bow(split=\\'train\\', root_only=False, df_idxs=None):\\n    from scipy import sparse\\n    df = get_filtered_split(split, df_idxs, root_only)\\n    x = utils.id_lists_to_sparse_bow(df[\\'ids\\'], self.vocab.size)\\n    y = np.array(df.label, dtype=np.int32)\\n    return x, y\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are functions that were in the \"SSTDataset\" class in sst.py from A2\n",
    "'''\n",
    "def get_filtered_split(split='train', df_idxs=None, root_only=False):\n",
    "    if not hasattr(split):\n",
    "        raise ValueError(\"Invalid split name '%s'\" % name)\n",
    "    df = getattr(split)\n",
    "    if df_idxs is not None:\n",
    "        df = df.loc[df_idxs]\n",
    "    #if root_only:          # Should not need in Final Project.\n",
    "        #df = df[df.is_root]\n",
    "    return df\n",
    "\n",
    "def as_padded_array(split='train', max_len=40, pad_id=0,\n",
    "                    root_only=False, df_idxs=None):\n",
    "    \"\"\"Return the dataset as a (padded) NumPy array.\n",
    "    Longer sequences will be truncated to max_len ids, while shorter ones\n",
    "    will be padded with pad_id.\n",
    "    Args:\n",
    "      split: 'train' or 'test'\n",
    "      max_len: maximum sequence length\n",
    "      pad_id: id to pad shorter sequences with\n",
    "      root_only: if true, will only export root phrases\n",
    "      df_idxs: (optional) custom list of indices to export\n",
    "    Returns: (x, ns, y)\n",
    "      x: [num_examples, max_len] NumPy array of integer ids\n",
    "      ns: [num_examples] NumPy array of sequence lengths (<= max_len)\n",
    "      y: [num_examples] NumPy array of target ids\n",
    "    \"\"\"\n",
    "    df = get_filtered_split(split, df_idxs, root_only)\n",
    "    x, ns = utils.pad_np_array(df.ids, max_len=max_len, pad_id=pad_id)\n",
    "    return x, ns, np.array(df.label, dtype=np.int32)\n",
    "\n",
    "def as_sparse_bow(split='train', root_only=False, df_idxs=None):\n",
    "    from scipy import sparse\n",
    "    df = get_filtered_split(split, df_idxs, root_only)\n",
    "    x = utils.id_lists_to_sparse_bow(df['ids'], self.vocab.size)\n",
    "    y = np.array(df.label, dtype=np.int32)\n",
    "    return x, y\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct train, dev, test data arrays  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[61446, 990, 1530, 58, 375, 6, 1413, 15748, 21, 15579, 2669, 546, 67, 1814, 244, 16, 13711, 114], [5462, 13, 16, 5198, 217, 27116, 32, 51, 358, 199, 6, 8238, 20, 33], [11865, 11780, 3, 132, 9, 214, 11625, 30, 184, 35, 115, 5688, 4405, 1667, 3300, 284, 21360, 900, 6, 5508, 628, 8277, 101, 727, 8878, 4, 183, 173, 137, 331], [164, 10696, 1423, 3056, 3626, 53446, 298, 3394, 4, 2526, 6052, 6849], [97, 79, 3249, 3501, 71854, 1249, 35, 242]]\n",
      "\n",
      "[[61446   990  1530    58   375     6  1413 15748    21 15579  2669   546\n",
      "     67  1814   244    16 13711   114     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]\n",
      " [ 5462    13    16  5198   217 27116    32    51   358   199     6  8238\n",
      "     20    33     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]]\n",
      "\n",
      "[18 14]\n",
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "## Training data\n",
    "\n",
    "all_train_ids=[]\n",
    "for i, tokens in enumerate(train_data):  # here, tokens are the words in a single sentence\n",
    "    sent_ids = vocab.words_to_ids(tokens)\n",
    "    all_train_ids.append(sent_ids)\n",
    "print(all_train_ids[:5])\n",
    "\n",
    "max_len = 40   # Retain this setting, since it fits the ISOT \"title\" length distribution quite well.\n",
    "train_x, train_ns = utils.pad_np_array(all_train_ids, max_len=max_len)\n",
    "print()\n",
    "print(train_x[:2])\n",
    "print()\n",
    "print(train_ns[:2])\n",
    "\n",
    "train_y = train_labels\n",
    "print(train_y[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[667, 7856, 48, 349, 12, 1217, 23, 324, 5344, 324], [1506, 1320, 35206, 550, 2038, 25, 1714, 15232, 718, 23, 500, 12153], [1471, 84, 1479, 2948, 30, 162, 76, 6403, 140], [862, 159, 45, 167, 4700, 161, 434, 14, 109], [25681, 114, 8122, 105, 16, 10, 48747, 6434, 546, 50, 671, 42, 10579, 53956, 44, 310, 3888, 20, 209, 414, 221]]\n",
      "\n",
      "[[  667  7856    48   349    12  1217    23   324  5344   324     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]\n",
      " [ 1506  1320 35206   550  2038    25  1714 15232   718    23   500 12153\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]]\n",
      "\n",
      "[10 12]\n",
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "## Dev data\n",
    "\n",
    "all_dev_ids=[]\n",
    "for i, tokens in enumerate(dev_data):  # here, tokens are the words in a single sentence\n",
    "    sent_ids = vocab.words_to_ids(tokens)\n",
    "    all_dev_ids.append(sent_ids)\n",
    "print(all_dev_ids[:5])\n",
    "\n",
    "max_len = 40   # Retain this setting, since it fits the ISOT \"title\" length distribution quite well.\n",
    "dev_x, dev_ns = utils.pad_np_array(all_dev_ids, max_len=max_len)\n",
    "print()\n",
    "print(dev_x[:2])\n",
    "print()\n",
    "print(dev_ns[:2])\n",
    "\n",
    "dev_y = dev_labels\n",
    "print(dev_y[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1736, 11781, 6755, 54015, 5717, 1292, 546, 12633, 57, 141, 65, 114], [11865, 35, 5577, 231, 1110, 255, 62, 203, 6309, 2067, 49, 974, 148, 638, 1288, 149, 22317], [115, 16, 73985, 42, 3272, 530, 44, 966, 8, 8622, 37059, 1381, 734, 48, 6, 519, 30, 1621, 4, 1334, 4, 1531, 4, 4781, 546, 68, 128, 43, 449, 3, 4440, 9160, 1116, 78], [734, 11868, 1495, 5688, 32164, 62246, 23932, 5539, 7055, 22765, 62247, 322, 15433, 1464], [287, 3117, 2389, 6, 1836, 19, 1386, 12, 219, 35, 26587]]\n",
      "\n",
      "[[ 1736 11781  6755 54015  5717  1292   546 12633    57   141    65   114\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]\n",
      " [11865    35  5577   231  1110   255    62   203  6309  2067    49   974\n",
      "    148   638  1288   149 22317     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]]\n",
      "\n",
      "[12 17]\n",
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "## Test data\n",
    "\n",
    "all_test_ids=[]\n",
    "for i, tokens in enumerate(test_data):  # here, tokens are the words in a single sentence\n",
    "    sent_ids = vocab.words_to_ids(tokens)\n",
    "    all_test_ids.append(sent_ids)\n",
    "print(all_test_ids[:5])\n",
    "\n",
    "max_len = 40   # Retain this setting, since it fits the ISOT \"title\" length distribution quite well.\n",
    "test_x, test_ns = utils.pad_np_array(all_test_ids, max_len=max_len)\n",
    "print()\n",
    "print(test_x[:2])\n",
    "print()\n",
    "print(test_ns[:2])\n",
    "\n",
    "test_y = test_labels\n",
    "print(test_y[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples:\n",
      " [[61446   990  1530    58   375     6  1413 15748    21 15579  2669   546\n",
      "     67  1814   244    16 13711   114     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]\n",
      " [ 5462    13    16  5198   217 27116    32    51   358   199     6  8238\n",
      "     20    33     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]\n",
      " [11865 11780     3   132     9   214 11625    30   184    35   115  5688\n",
      "   4405  1667  3300   284 21360   900     6  5508   628  8277   101   727\n",
      "   8878     4   183   173   137   331     0     0     0     0     0     0\n",
      "      0     0     0     0]]\n",
      "Original sequence lengths:  [18 14 30]\n",
      "Target labels:  [0 0 0]\n",
      "\n",
      "Padded:\n",
      " brainiac<allcaps> gets rejected after trying to buy bmw<allcaps> with ebt<allcaps> card … what happens next is hysterical<allcaps> ! <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>\n",
      "Un-padded:\n",
      " brainiac<allcaps> gets rejected after trying to buy bmw<allcaps> with ebt<allcaps> card … what happens next is hysterical<allcaps> !\n"
     ]
    }
   ],
   "source": [
    "print(\"Examples:\\n\", train_x[:3])\n",
    "print(\"Original sequence lengths: \", train_ns[:3])\n",
    "print(\"Target labels: \", train_y[:3])\n",
    "print(\"\")\n",
    "print(\"Padded:\\n\", \" \".join(vocab.ids_to_words(train_x[0])))\n",
    "print(\"Un-padded:\\n\", \" \".join(vocab.ids_to_words(train_x[0,:train_ns[0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use tf.Estimator API along with nbow_models.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things to consider:  \n",
    "- Start w/ 2 epochs (20 was original)       \n",
    "- Consider use of dropouts in fully-connected layers     \n",
    "-  Use embed_dim = 300 rather than 50??  \n",
    "- Try to fix Tensorboard display issue (http://localhost:6006 not working.  \"This site can't be reached; ERR_CONNECTION_REFUSED)  \n",
    "- xx  \n",
    "...  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 152182\n"
     ]
    }
   ],
   "source": [
    "print('vocab size:', vocab.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (152,182 words) written to '/tmp/tf_cnn_20181208-0854/metadata.tsv'\n",
      "Projector config written to /tmp/tf_cnn_20181208-0854/projector_config.pbtxt\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tf_cnn_20181208-0854', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd9a80c8898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "\n",
      "To view training (once it starts), run:\n",
      "\n",
      "    tensorboard --logdir='/tmp/tf_cnn_20181208-0854' --port 6006\n",
      "\n",
      "Then in your browser, open: http://localhost:6006\n"
     ]
    }
   ],
   "source": [
    "## Setup model framework\n",
    "## (Must specify correct nbow_model_x name in this cell to use the correct nbow_model_x.py file.)\n",
    "\n",
    "import cnn_models_2; reload(cnn_models_2)\n",
    "\n",
    "# Specify model hyperparameters as used by model_fn\n",
    "model_params = dict(V=vocab.size, embed_dim=50,filters=50, kernel_sizes=[2,5,8], hidden_dims=[25,25], num_classes=2,\n",
    "                    encoder_type='cnn', dropout_rate=.99,\n",
    "                    lr=0.1, optimizer='adagrad', beta=0.01)  # can set optimizer to 'adagrad' or 'adam', which is slower here\n",
    "\n",
    "checkpoint_dir = \"/tmp/tf_cnn_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "if os.path.isdir(checkpoint_dir):\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "# Write vocabulary to file, so TensorBoard can label embeddings.\n",
    "# creates checkpoint_dir/projector_config.pbtxt and checkpoint_dir/metadata.tsv\n",
    "#ds.vocab.write_projector_config(checkpoint_dir, \"Encoder/Embedding_Layer/W_embed\")\n",
    "vocab.write_projector_config(checkpoint_dir, \"Encoder/Embedding_Layer/W_embed\")\n",
    "\n",
    "model = tf.estimator.Estimator(model_fn=cnn_models_2.classifier_model_fn, \n",
    "                               params=model_params,\n",
    "                               model_dir=checkpoint_dir)\n",
    "print(\"\")\n",
    "print(\"To view training (once it starts), run:\\n\")\n",
    "print(\"    tensorboard --logdir='{:s}' --port 6006\".format(checkpoint_dir))\n",
    "print(\"\\nThen in your browser, open: http://localhost:6006\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tf_cnn_20181208-0854/model.ckpt.\n",
      "INFO:tensorflow:loss = 68.522255, step = 1\n",
      "INFO:tensorflow:global_step/sec: 50.5407\n",
      "INFO:tensorflow:loss = 37.524548, step = 101 (1.981 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.0004\n",
      "INFO:tensorflow:loss = 31.890059, step = 201 (1.639 sec)\n",
      "INFO:tensorflow:global_step/sec: 59.9699\n",
      "INFO:tensorflow:loss = 33.30606, step = 301 (1.667 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.9703\n",
      "INFO:tensorflow:loss = 27.78269, step = 401 (1.641 sec)\n",
      "INFO:tensorflow:global_step/sec: 62.2611\n",
      "INFO:tensorflow:loss = 27.78172, step = 501 (1.608 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.4758\n",
      "INFO:tensorflow:loss = 28.934265, step = 601 (1.625 sec)\n",
      "INFO:tensorflow:global_step/sec: 59.9343\n",
      "INFO:tensorflow:loss = 24.20354, step = 701 (1.669 sec)\n",
      "INFO:tensorflow:global_step/sec: 59.6454\n",
      "INFO:tensorflow:loss = 22.859669, step = 801 (1.676 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.4528\n",
      "INFO:tensorflow:loss = 21.234913, step = 901 (1.627 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 983 into /tmp/tf_cnn_20181208-0854/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4.573742.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-08-08:54:48\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181208-0854/model.ckpt-983\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-08-08:54:49\n",
      "INFO:tensorflow:Saving dict for global step 983: accuracy = 0.5613957, cross_entropy_loss = 0.689952, global_step = 983, loss = 83.18453\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 983: /tmp/tf_cnn_20181208-0854/model.ckpt-983\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181208-0854/model.ckpt-983\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 983 into /tmp/tf_cnn_20181208-0854/model.ckpt.\n",
      "INFO:tensorflow:loss = 22.564991, step = 984\n",
      "INFO:tensorflow:global_step/sec: 50.6327\n",
      "INFO:tensorflow:loss = 19.701517, step = 1084 (1.977 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.3879\n",
      "INFO:tensorflow:loss = 18.0975, step = 1184 (1.657 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.5297\n",
      "INFO:tensorflow:loss = 19.785309, step = 1284 (1.625 sec)\n",
      "INFO:tensorflow:global_step/sec: 62.2545\n",
      "INFO:tensorflow:loss = 18.946505, step = 1384 (1.606 sec)\n",
      "INFO:tensorflow:global_step/sec: 64.4715\n",
      "INFO:tensorflow:loss = 19.529541, step = 1484 (1.551 sec)\n",
      "INFO:tensorflow:global_step/sec: 62.3041\n",
      "INFO:tensorflow:loss = 20.370497, step = 1584 (1.605 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.1899\n",
      "INFO:tensorflow:loss = 17.596909, step = 1684 (1.662 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.0994\n",
      "INFO:tensorflow:loss = 17.006205, step = 1784 (1.664 sec)\n",
      "INFO:tensorflow:global_step/sec: 59.57\n",
      "INFO:tensorflow:loss = 15.941528, step = 1884 (1.680 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1966 into /tmp/tf_cnn_20181208-0854/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.8631618.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-08-08:55:20\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181208-0854/model.ckpt-1966\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-08-08:55:21\n",
      "INFO:tensorflow:Saving dict for global step 1966: accuracy = 0.52308834, cross_entropy_loss = 0.6922983, global_step = 1966, loss = 63.282764\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1966: /tmp/tf_cnn_20181208-0854/model.ckpt-1966\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181208-0854/model.ckpt-1966\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1966 into /tmp/tf_cnn_20181208-0854/model.ckpt.\n",
      "INFO:tensorflow:loss = 17.303568, step = 1967\n",
      "INFO:tensorflow:global_step/sec: 51.6057\n",
      "INFO:tensorflow:loss = 15.162731, step = 2067 (1.939 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.2537\n",
      "INFO:tensorflow:loss = 13.904421, step = 2167 (1.661 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.2554\n",
      "INFO:tensorflow:loss = 15.2362795, step = 2267 (1.658 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.6271\n",
      "INFO:tensorflow:loss = 15.200836, step = 2367 (1.650 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.0585\n",
      "INFO:tensorflow:loss = 15.856499, step = 2467 (1.666 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.5609\n",
      "INFO:tensorflow:loss = 16.558592, step = 2567 (1.650 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.2451\n",
      "INFO:tensorflow:loss = 14.436636, step = 2667 (1.632 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.3811\n",
      "INFO:tensorflow:loss = 14.053516, step = 2767 (1.630 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.3782\n",
      "INFO:tensorflow:loss = 13.216907, step = 2867 (1.655 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2949 into /tmp/tf_cnn_20181208-0854/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.416497.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-08-08:55:50\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181208-0854/model.ckpt-2949\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-08-08:55:52\n",
      "INFO:tensorflow:Saving dict for global step 2949: accuracy = 0.52308834, cross_entropy_loss = 0.69237924, global_step = 2949, loss = 52.729916\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2949: /tmp/tf_cnn_20181208-0854/model.ckpt-2949\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181208-0854/model.ckpt-2949\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 2949 into /tmp/tf_cnn_20181208-0854/model.ckpt.\n",
      "INFO:tensorflow:loss = 14.405589, step = 2950\n",
      "INFO:tensorflow:global_step/sec: 48.898\n",
      "INFO:tensorflow:loss = 12.6571665, step = 3050 (2.047 sec)\n",
      "INFO:tensorflow:global_step/sec: 59.2387\n",
      "INFO:tensorflow:loss = 11.573833, step = 3150 (1.688 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.2725\n",
      "INFO:tensorflow:loss = 12.753872, step = 3250 (1.632 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.3381\n",
      "INFO:tensorflow:loss = 12.92311, step = 3350 (1.630 sec)\n",
      "INFO:tensorflow:global_step/sec: 59.6249\n",
      "INFO:tensorflow:loss = 13.582718, step = 3450 (1.677 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.6024\n",
      "INFO:tensorflow:loss = 14.2565975, step = 3550 (1.650 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.1307\n",
      "INFO:tensorflow:loss = 12.526407, step = 3650 (1.663 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.0055\n",
      "INFO:tensorflow:loss = 12.199948, step = 3750 (1.639 sec)\n",
      "INFO:tensorflow:global_step/sec: 62.1606\n",
      "INFO:tensorflow:loss = 11.49113, step = 3850 (1.609 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3932 into /tmp/tf_cnn_20181208-0854/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.1191387.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-08-08:56:22\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181208-0854/model.ckpt-3932\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-08-08:56:24\n",
      "INFO:tensorflow:Saving dict for global step 3932: accuracy = 0.52308834, cross_entropy_loss = 0.69240916, global_step = 3932, loss = 45.904964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3932: /tmp/tf_cnn_20181208-0854/model.ckpt-3932\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181208-0854/model.ckpt-3932\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3932 into /tmp/tf_cnn_20181208-0854/model.ckpt.\n",
      "INFO:tensorflow:loss = 12.561265, step = 3933\n",
      "INFO:tensorflow:global_step/sec: 49.0254\n",
      "INFO:tensorflow:loss = 10.953579, step = 4033 (2.041 sec)\n",
      "INFO:tensorflow:global_step/sec: 59.4279\n",
      "INFO:tensorflow:loss = 10.02161, step = 4133 (1.685 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.2333\n",
      "INFO:tensorflow:loss = 11.147617, step = 4233 (1.631 sec)\n",
      "INFO:tensorflow:global_step/sec: 59.6457\n",
      "INFO:tensorflow:loss = 11.3674345, step = 4333 (1.677 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.6602\n",
      "INFO:tensorflow:loss = 11.9944105, step = 4433 (1.622 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.9352\n",
      "INFO:tensorflow:loss = 12.66519, step = 4533 (1.642 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.2464\n",
      "INFO:tensorflow:loss = 11.071773, step = 4633 (1.632 sec)\n",
      "INFO:tensorflow:global_step/sec: 59.8236\n",
      "INFO:tensorflow:loss = 10.895512, step = 4733 (1.672 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.013\n",
      "INFO:tensorflow:loss = 10.268424, step = 4833 (1.639 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4915 into /tmp/tf_cnn_20181208-0854/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.8905225.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-08-08:56:53\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181208-0854/model.ckpt-4915\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-08-08:56:54\n",
      "INFO:tensorflow:Saving dict for global step 4915: accuracy = 0.52308834, cross_entropy_loss = 0.69223404, global_step = 4915, loss = 41.02459\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4915: /tmp/tf_cnn_20181208-0854/model.ckpt-4915\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181208-0854/model.ckpt-4915\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 4915 into /tmp/tf_cnn_20181208-0854/model.ckpt.\n",
      "INFO:tensorflow:loss = 11.251426, step = 4916\n",
      "INFO:tensorflow:global_step/sec: 49.7812\n",
      "INFO:tensorflow:loss = 9.757458, step = 5016 (2.011 sec)\n",
      "INFO:tensorflow:global_step/sec: 59.7876\n",
      "INFO:tensorflow:loss = 8.893796, step = 5116 (1.675 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.7088\n",
      "INFO:tensorflow:loss = 10.00108, step = 5216 (1.645 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.2716\n",
      "INFO:tensorflow:loss = 10.226899, step = 5316 (1.635 sec)\n",
      "INFO:tensorflow:global_step/sec: 62.2668\n",
      "INFO:tensorflow:loss = 10.83587, step = 5416 (1.604 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.4478\n",
      "INFO:tensorflow:loss = 11.500216, step = 5516 (1.654 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.9268\n",
      "INFO:tensorflow:loss = 9.967521, step = 5616 (1.615 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.5079\n",
      "INFO:tensorflow:loss = 9.91731, step = 5716 (1.626 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.2287\n",
      "INFO:tensorflow:loss = 9.343108, step = 5816 (1.633 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5898 into /tmp/tf_cnn_20181208-0854/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.6975539.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-08-08:57:25\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181208-0854/model.ckpt-5898\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-08-08:57:26\n",
      "INFO:tensorflow:Saving dict for global step 5898: accuracy = 0.52308834, cross_entropy_loss = 0.6921753, global_step = 5898, loss = 37.30769\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5898: /tmp/tf_cnn_20181208-0854/model.ckpt-5898\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181208-0854/model.ckpt-5898\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 5898 into /tmp/tf_cnn_20181208-0854/model.ckpt.\n",
      "INFO:tensorflow:loss = 10.25525, step = 5899\n",
      "INFO:tensorflow:global_step/sec: 51.0372\n",
      "INFO:tensorflow:loss = 8.82838, step = 5999 (1.961 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.4811\n",
      "INFO:tensorflow:loss = 8.038638, step = 6099 (1.654 sec)\n",
      "INFO:tensorflow:global_step/sec: 57.7353\n",
      "INFO:tensorflow:loss = 9.112459, step = 6199 (1.731 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.2781\n",
      "INFO:tensorflow:loss = 9.337336, step = 6299 (1.632 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.3998\n",
      "INFO:tensorflow:loss = 9.892641, step = 6399 (1.628 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.1704\n",
      "INFO:tensorflow:loss = 10.565016, step = 6499 (1.662 sec)\n",
      "INFO:tensorflow:global_step/sec: 62.0596\n",
      "INFO:tensorflow:loss = 9.145325, step = 6599 (1.611 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.736\n",
      "INFO:tensorflow:loss = 9.180009, step = 6699 (1.621 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.6317\n",
      "INFO:tensorflow:loss = 8.617797, step = 6799 (1.622 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6881 into /tmp/tf_cnn_20181208-0854/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.5410159.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-08-08:57:55\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181208-0854/model.ckpt-6881\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-08-08:57:56\n",
      "INFO:tensorflow:Saving dict for global step 6881: accuracy = 0.52308834, cross_entropy_loss = 0.6922855, global_step = 6881, loss = 34.36816\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 6881: /tmp/tf_cnn_20181208-0854/model.ckpt-6881\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181208-0854/model.ckpt-6881\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 6881 into /tmp/tf_cnn_20181208-0854/model.ckpt.\n",
      "INFO:tensorflow:loss = 9.457118, step = 6882\n",
      "INFO:tensorflow:global_step/sec: 50.3405\n",
      "INFO:tensorflow:loss = 8.091675, step = 6982 (1.989 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.9873\n",
      "INFO:tensorflow:loss = 7.3678217, step = 7082 (1.639 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.4201\n",
      "INFO:tensorflow:loss = 8.413703, step = 7182 (1.628 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.2879\n",
      "INFO:tensorflow:loss = 8.633746, step = 7282 (1.659 sec)\n",
      "INFO:tensorflow:global_step/sec: 59.6355\n",
      "INFO:tensorflow:loss = 9.177519, step = 7382 (1.678 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.1865\n",
      "INFO:tensorflow:loss = 9.814372, step = 7482 (1.633 sec)\n",
      "INFO:tensorflow:global_step/sec: 63.7138\n",
      "INFO:tensorflow:loss = 8.459757, step = 7582 (1.570 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.7952\n",
      "INFO:tensorflow:loss = 8.534333, step = 7682 (1.645 sec)\n",
      "INFO:tensorflow:global_step/sec: 62.0359\n",
      "INFO:tensorflow:loss = 8.0285, step = 7782 (1.611 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7864 into /tmp/tf_cnn_20181208-0854/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.4070215.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-08-08:58:27\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181208-0854/model.ckpt-7864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-08-08:58:28\n",
      "INFO:tensorflow:Saving dict for global step 7864: accuracy = 0.52308834, cross_entropy_loss = 0.6923199, global_step = 7864, loss = 31.969212\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 7864: /tmp/tf_cnn_20181208-0854/model.ckpt-7864\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181208-0854/model.ckpt-7864\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 7864 into /tmp/tf_cnn_20181208-0854/model.ckpt.\n",
      "INFO:tensorflow:loss = 8.772514, step = 7865\n",
      "INFO:tensorflow:global_step/sec: 51.9941\n",
      "INFO:tensorflow:loss = 7.4892673, step = 7965 (1.925 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.152\n",
      "INFO:tensorflow:loss = 6.811702, step = 8065 (1.663 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.0031\n",
      "INFO:tensorflow:loss = 7.8359175, step = 8165 (1.640 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.4883\n",
      "INFO:tensorflow:loss = 8.0467205, step = 8265 (1.625 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.1465\n",
      "INFO:tensorflow:loss = 8.555532, step = 8365 (1.663 sec)\n",
      "INFO:tensorflow:global_step/sec: 63.9157\n",
      "INFO:tensorflow:loss = 9.190955, step = 8465 (1.566 sec)\n",
      "INFO:tensorflow:global_step/sec: 63.8415\n",
      "INFO:tensorflow:loss = 7.884144, step = 8565 (1.565 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.0818\n",
      "INFO:tensorflow:loss = 8.008814, step = 8665 (1.638 sec)\n",
      "INFO:tensorflow:global_step/sec: 62.5183\n",
      "INFO:tensorflow:loss = 7.5427914, step = 8765 (1.598 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 8847 into /tmp/tf_cnn_20181208-0854/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.293823.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-08-08:58:57\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181208-0854/model.ckpt-8847\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-08-08:58:58\n",
      "INFO:tensorflow:Saving dict for global step 8847: accuracy = 0.52308834, cross_entropy_loss = 0.6921249, global_step = 8847, loss = 29.968714\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 8847: /tmp/tf_cnn_20181208-0854/model.ckpt-8847\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181208-0854/model.ckpt-8847\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 8847 into /tmp/tf_cnn_20181208-0854/model.ckpt.\n",
      "INFO:tensorflow:loss = 8.225966, step = 8848\n",
      "INFO:tensorflow:global_step/sec: 53.24\n",
      "INFO:tensorflow:loss = 6.9839587, step = 8948 (1.880 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.2478\n",
      "INFO:tensorflow:loss = 6.356428, step = 9048 (1.662 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.1159\n",
      "INFO:tensorflow:loss = 7.3585706, step = 9148 (1.635 sec)\n",
      "INFO:tensorflow:global_step/sec: 60.4367\n",
      "INFO:tensorflow:loss = 7.562217, step = 9248 (1.655 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.5052\n",
      "INFO:tensorflow:loss = 8.001267, step = 9348 (1.628 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.8192\n",
      "INFO:tensorflow:loss = 8.653652, step = 9448 (1.615 sec)\n",
      "INFO:tensorflow:global_step/sec: 62.6322\n",
      "INFO:tensorflow:loss = 7.3902774, step = 9548 (1.599 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.559\n",
      "INFO:tensorflow:loss = 7.5794525, step = 9648 (1.625 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.831\n",
      "INFO:tensorflow:loss = 7.111754, step = 9748 (1.615 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 9830 into /tmp/tf_cnn_20181208-0854/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.182796.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-08-08:59:28\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181208-0854/model.ckpt-9830\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-08-08:59:29\n",
      "INFO:tensorflow:Saving dict for global step 9830: accuracy = 0.52308834, cross_entropy_loss = 0.69213855, global_step = 9830, loss = 28.254084\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 9830: /tmp/tf_cnn_20181208-0854/model.ckpt-9830\n"
     ]
    }
   ],
   "source": [
    "## Train model and Evaluate on Dev data\n",
    "\n",
    "# Training params, just used in this cell for the input_fn-s\n",
    "train_params = dict(batch_size=32, total_epochs=10, eval_every=1) # start with 2 epochs rather than 20; eval_every=1 (was 2)\n",
    "assert(train_params['total_epochs'] % train_params['eval_every'] == 0)\n",
    "\n",
    "# Construct and train the model, saving checkpoints to the directory above.\n",
    "# Input function for training set batches\n",
    "# Do 'eval_every' epochs at once, followed by evaluating on the dev set.\n",
    "# NOTE: use patch_numpy_io.numpy_input_fn instead of tf.estimator.inputs.numpy_input_fn\n",
    "train_input_fn = patched_numpy_io.numpy_input_fn(\n",
    "                    x={\"ids\": train_x, \"ns\": train_ns}, y=train_y,\n",
    "                    batch_size=train_params['batch_size'], \n",
    "                    num_epochs=train_params['eval_every'], shuffle=True, seed=42\n",
    "                 )\n",
    "\n",
    "# Input function for dev set batches. As above, but:\n",
    "# - Don't randomize order\n",
    "# - Iterate exactly once (one epoch)\n",
    "dev_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": dev_x, \"ns\": dev_ns}, y=dev_y,\n",
    "                    batch_size=128, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "for _ in range(train_params['total_epochs'] // train_params['eval_every']):\n",
    "    # Train for a few epochs, then evaluate on dev\n",
    "    model.train(input_fn=train_input_fn)\n",
    "    eval_metrics = model.evaluate(input_fn=dev_input_fn, name=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-08-08:59:30\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181208-0854/model.ckpt-9830\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-08-08:59:31\n",
      "INFO:tensorflow:Saving dict for global step 9830: accuracy = 0.5270972, cross_entropy_loss = 0.6918415, global_step = 9830, loss = 28.403616\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 9830: /tmp/tf_cnn_20181208-0854/model.ckpt-9830\n",
      "Accuracy on test set: 52.71%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5270972,\n",
       " 'cross_entropy_loss': 0.6918415,\n",
       " 'loss': 28.403616,\n",
       " 'global_step': 9830}"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Evaluate model on (ISOT) Test data\n",
    "\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": test_x, \"ns\": test_ns}, y=test_y,\n",
    "                    batch_size=128, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "eval_metrics = model.evaluate(input_fn=test_input_fn, name=\"test\")\n",
    "\n",
    "print(\"Accuracy on test set: {:.02%}\".format(eval_metrics['accuracy']))\n",
    "eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181208-0854/model.ckpt-9830\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Accuracy on test set: 52.71%\n"
     ]
    }
   ],
   "source": [
    "## We can also evaluate the old-fashioned way, by calling model.predict(...) and working with the predicted labels directly:\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "predictions = list(model.predict(test_input_fn))  # list of dicts\n",
    "y_pred = [p['max'] for p in predictions]\n",
    "acc = accuracy_score(y_pred, test_y)\n",
    "print(\"Accuracy on test set: {:.02%}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy 98% with trainable=True (for GloVe embeddings); 92% with trainable=False.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create padded LIAR data and apply prediction function to LIAR data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[159, 13, 150, 7, 575, 10817, 1395, 66, 71, 1957, 3758, 4, 173, 30, 13, 150, 10, 13, 5], [8, 4931, 11790, 7, 3566, 1713, 599, 678, 11, 67186, 1117, 45, 27, 751, 6, 1574, 1294, 21, 8, 69, 4147, 5], [3, 117, 788, 22, 624, 30, 6938, 5], [645, 26, 3845, 73, 3218, 2700, 25, 13, 150, 147, 13, 5], [31885, 5, 8522, 464, 882, 16, 77, 5, 13, 12, 1679, 9, 287, 6247, 472, 9, 1543, 5]]\n",
      "\n",
      "[[  159    13   150     7   575 10817  1395    66    71  1957  3758     4\n",
      "    173    30    13   150    10    13     5     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]\n",
      " [    8  4931 11790     7  3566  1713   599   678    11 67186  1117    45\n",
      "     27   751     6  1574  1294    21     8    69  4147     5     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]]\n",
      "\n",
      "[19 22]\n",
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "## LIAR data padding\n",
    "\n",
    "all_liar_ids=[]\n",
    "for i, tokens in enumerate(liar_title_tokans):  # here, tokens are the words in a single sentence\n",
    "    sent_ids = vocab.words_to_ids(tokens)\n",
    "    all_liar_ids.append(sent_ids)\n",
    "print(all_liar_ids[:5])\n",
    "\n",
    "max_len = 40   # Retain this setting, since it fits the ISOT \"title\" length distribution quite well.\n",
    "liar_x, liar_ns = utils.pad_np_array(all_liar_ids, max_len=max_len)\n",
    "print()\n",
    "print(liar_x[:2])\n",
    "print()\n",
    "print(liar_ns[:2])\n",
    "\n",
    "liar_y = liar_labels\n",
    "print(liar_y[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-08-08:59:35\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181208-0854/model.ckpt-9830\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-08-08:59:37\n",
      "INFO:tensorflow:Saving dict for global step 9830: accuracy = 0.5518342, cross_entropy_loss = 0.68975437, global_step = 9830, loss = 48.412205\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 9830: /tmp/tf_cnn_20181208-0854/model.ckpt-9830\n",
      "Accuracy on LIAR set: 55.18%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5518342,\n",
       " 'cross_entropy_loss': 0.68975437,\n",
       " 'loss': 48.412205,\n",
       " 'global_step': 9830}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Evaluate model on LIAR data\n",
    "\n",
    "test_input_fn_liar = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": liar_x, \"ns\": liar_ns}, y=liar_y,\n",
    "                    batch_size=128, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "eval_metrics = model.evaluate(input_fn=test_input_fn_liar, name=\"test\")\n",
    "\n",
    "print(\"Accuracy on LIAR set: {:.02%}\".format(eval_metrics['accuracy']))\n",
    "eval_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prediction accuracy 52% with trainable=True for GloVe embeddings.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(liar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! ls ./parsed_data -lah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
