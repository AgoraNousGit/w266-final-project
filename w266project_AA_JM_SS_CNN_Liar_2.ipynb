{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIAR DETECTION GROUP PROJECT - Neural BOW Models  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONTENTS  \n",
    "\n",
    "Imports  \n",
    "Load ISOT data from appropriate pickle file  \n",
    "Load ISOT vocabulary from pickle file  (note: vocab contains both \"title\" and \"text\" words)  \n",
    "Train/Dev/Test split ISOT data  \n",
    "Load LIWC data for custom features  \n",
    "Load LIAR data (for evaluating models)  \n",
    "\n",
    "#### Neural BOW Models:\n",
    "- Model_1: Initial run replicating settings from Assignment 2, but with ISOT \"title\" data.  \n",
    "- Model_2: Use GloVe word embeddings rather than initializing embeddings with uniform random numbers.  \n",
    "- Model_3: Random word embeddings, but custom LIWC features concatenated into the model. \n",
    "- Model_4: Incorporate GloVe embeddings as well as LIWC features. Still training with ISOT \"title\" data. \n",
    "- Model_5: Train using LIAR/Politifact data, GloVe embeddings, but NO LIWC features; then predict on same plus ISOT \"title\" data also.  \n",
    "- Model_6: Train using LIAR/Politifact data, GloVe embeddings, and LIWC features; then predict on same plus ISOT \"title\" data also.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "from importlib import reload\n",
    "import os, sys, re, json, time, datetime, shutil\n",
    "import itertools, collections\n",
    "from functools import reduce\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.utils import shuffle\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "#assert(tf.__version__.startswith(\"1.8\"))\n",
    "\n",
    "import pickle\n",
    "import dill\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary, tf_embed_viz\n",
    "from w266_common import patched_numpy_io\n",
    "import timeit  #For timing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.10.1\n"
     ]
    }
   ],
   "source": [
    "print('TensorFlow version:', tf.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LIAR/Politifact data and vocabulary from pickle files  \n",
    "Load the COMBINED LIAR and Politifact dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23258 entries, 0 to 23257\n",
      "Data columns (total 6 columns):\n",
      "target            23258 non-null int64\n",
      "title             23258 non-null object\n",
      "title_tokcan      23258 non-null object\n",
      "title_POS         23258 non-null object\n",
      "binary_target     23258 non-null int64\n",
      "embedded_title    23258 non-null object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 19.1 MB\n"
     ]
    }
   ],
   "source": [
    "# Read LIAR/Politifact data from pickle file.\n",
    "all_data = pd.read_pickle('parsed_data/df_liarpolitifact_data_embed.pkl')  # \n",
    "\n",
    "all_data.info(memory_usage='deep', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>title</th>\n",
       "      <th>title_tokcan</th>\n",
       "      <th>title_POS</th>\n",
       "      <th>binary_target</th>\n",
       "      <th>embedded_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Says 31 percent of Texas physicians accept all...</td>\n",
       "      <td>[says, &lt;number&gt;, percent, of, texas, physician...</td>\n",
       "      <td>[V, $, N, P, ^, N, V, D, A, ^, N, ,, R, P, $, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.11797, 0.21126, 0.29075, -0.021211, 0.7819...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>''Both Democrats and Republicans are advocatin...</td>\n",
       "      <td>['', both, democrats, and, republicans, are, a...</td>\n",
       "      <td>[,, D, N, &amp;, N, V, V, P, D, N, P, N, N, V, P, ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[[0.0028594, 0.19457, -0.19449, -0.037583, 0.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>A Republican-led softening of firearms trainin...</td>\n",
       "      <td>[a, republican-led, softening, of, firearms, t...</td>\n",
       "      <td>[D, A, N, P, N, N, N, V, D, A, N, V, V, V, P, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.21705, 0.46515, -0.46757, 0.10082, 1.0135,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>The first tweet was sent from Austin.</td>\n",
       "      <td>[the, first, tweet, was, sent, from, austin, .]</td>\n",
       "      <td>[D, A, N, V, V, P, ^, ,]</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Georgia has the countrys second highest number...</td>\n",
       "      <td>[georgia, has, the, countrys, second, highest,...</td>\n",
       "      <td>[^, V, D, N, A, A, N, P, A, N, N, N, ,]</td>\n",
       "      <td>-1</td>\n",
       "      <td>[[-1.3427, 0.4592, 0.19281, 0.71305, -0.5934, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                              title  \\\n",
       "0       1  Says 31 percent of Texas physicians accept all...   \n",
       "1       2  ''Both Democrats and Republicans are advocatin...   \n",
       "2       0  A Republican-led softening of firearms trainin...   \n",
       "3       5              The first tweet was sent from Austin.   \n",
       "4       2  Georgia has the countrys second highest number...   \n",
       "\n",
       "                                        title_tokcan  \\\n",
       "0  [says, <number>, percent, of, texas, physician...   \n",
       "1  ['', both, democrats, and, republicans, are, a...   \n",
       "2  [a, republican-led, softening, of, firearms, t...   \n",
       "3    [the, first, tweet, was, sent, from, austin, .]   \n",
       "4  [georgia, has, the, countrys, second, highest,...   \n",
       "\n",
       "                                           title_POS  binary_target  \\\n",
       "0  [V, $, N, P, ^, N, V, D, A, ^, N, ,, R, P, $, ...              1   \n",
       "1  [,, D, N, &, N, V, V, P, D, N, P, N, N, V, P, ...             -1   \n",
       "2  [D, A, N, P, N, N, N, V, D, A, N, V, V, V, P, ...              1   \n",
       "3                           [D, A, N, V, V, P, ^, ,]              0   \n",
       "4            [^, V, D, N, A, A, N, P, A, N, N, N, ,]             -1   \n",
       "\n",
       "                                      embedded_title  \n",
       "0  [[0.11797, 0.21126, 0.29075, -0.021211, 0.7819...  \n",
       "1  [[0.0028594, 0.19457, -0.19449, -0.037583, 0.9...  \n",
       "2  [[0.21705, 0.46515, -0.46757, 0.10082, 1.0135,...  \n",
       "3  [[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -...  \n",
       "4  [[-1.3427, 0.4592, 0.19281, 0.71305, -0.5934, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Says 31 percent of Texas physicians accept all new Medicaid patients, down from 67 percent in 2000.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.title[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['says',\n",
       " '<number>',\n",
       " 'percent',\n",
       " 'of',\n",
       " 'texas',\n",
       " 'physicians',\n",
       " 'accept',\n",
       " 'all',\n",
       " 'new',\n",
       " 'medicaid',\n",
       " 'patients',\n",
       " ',',\n",
       " 'down',\n",
       " 'from',\n",
       " '<number>',\n",
       " 'percent',\n",
       " 'in',\n",
       " '<number>',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.title_tokcan[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>title</th>\n",
       "      <th>title_tokcan</th>\n",
       "      <th>title_POS</th>\n",
       "      <th>binary_target</th>\n",
       "      <th>embedded_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17668</th>\n",
       "      <td>1</td>\n",
       "      <td>Says Republican  lieutenants to House Speaker ...</td>\n",
       "      <td>[says, republican, lieutenants, to, house, spe...</td>\n",
       "      <td>[V, A, N, P, ^, ^, ^, ^, V, N, N, V, A, N, ^, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.11797, 0.21126, 0.29075, -0.021211, 0.7819...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17669</th>\n",
       "      <td>2</td>\n",
       "      <td>Texas state Reps. Dan Branch and Jim Pitts, li...</td>\n",
       "      <td>[texas, state, reps, ., dan, branch, and, jim,...</td>\n",
       "      <td>[^, N, N, ,, ^, ^, &amp;, ^, ^, ,, N, P, ^, ^, ^, ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[[-1.1389, 0.091966, 0.21446, 0.62652, -0.1462...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17670</th>\n",
       "      <td>3</td>\n",
       "      <td>Powerful Houston  Democrats Sylvia Garcia (Dem...</td>\n",
       "      <td>[powerful, houston, democrats, sylvia, garcia,...</td>\n",
       "      <td>[A, ^, N, ^, ^, ,, ^, N, N, P, ^, ^, ,, &amp;, ^, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.64857, -0.77899, 0.64121, 0.22266, 0.82489...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17671</th>\n",
       "      <td>3</td>\n",
       "      <td>Says Jeanne Shaheen \"got behind the idea of us...</td>\n",
       "      <td>[\", says, jeanne, shaheen, \"\", got, behind, th...</td>\n",
       "      <td>[,, V, ^, ^, ,, V, P, D, N, P, V, D, ^, P, V, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.25769, 0.45629, -0.76974, -0.37679, 0.5927...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17672</th>\n",
       "      <td>3</td>\n",
       "      <td>In Texas public schools, \"we spend an average ...</td>\n",
       "      <td>[\", in, texas, public, schools, ,, \"\", we, spe...</td>\n",
       "      <td>[,, P, ^, A, N, ,, ,, O, V, D, N, P, G, $, P, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.25769, 0.45629, -0.76974, -0.37679, 0.5927...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       target                                              title  \\\n",
       "17668       1  Says Republican  lieutenants to House Speaker ...   \n",
       "17669       2  Texas state Reps. Dan Branch and Jim Pitts, li...   \n",
       "17670       3  Powerful Houston  Democrats Sylvia Garcia (Dem...   \n",
       "17671       3  Says Jeanne Shaheen \"got behind the idea of us...   \n",
       "17672       3  In Texas public schools, \"we spend an average ...   \n",
       "\n",
       "                                            title_tokcan  \\\n",
       "17668  [says, republican, lieutenants, to, house, spe...   \n",
       "17669  [texas, state, reps, ., dan, branch, and, jim,...   \n",
       "17670  [powerful, houston, democrats, sylvia, garcia,...   \n",
       "17671  [\", says, jeanne, shaheen, \"\", got, behind, th...   \n",
       "17672  [\", in, texas, public, schools, ,, \"\", we, spe...   \n",
       "\n",
       "                                               title_POS  binary_target  \\\n",
       "17668  [V, A, N, P, ^, ^, ^, ^, V, N, N, V, A, N, ^, ...              1   \n",
       "17669  [^, N, N, ,, ^, ^, &, ^, ^, ,, N, P, ^, ^, ^, ...             -1   \n",
       "17670  [A, ^, N, ^, ^, ,, ^, N, N, P, ^, ^, ,, &, ^, ...              0   \n",
       "17671  [,, V, ^, ^, ,, V, P, D, N, P, V, D, ^, P, V, ...              0   \n",
       "17672  [,, P, ^, A, N, ,, ,, O, V, D, N, P, G, $, P, ...              0   \n",
       "\n",
       "                                          embedded_title  \n",
       "17668  [[0.11797, 0.21126, 0.29075, -0.021211, 0.7819...  \n",
       "17669  [[-1.1389, 0.091966, 0.21446, 0.62652, -0.1462...  \n",
       "17670  [[0.64857, -0.77899, 0.64121, 0.22266, 0.82489...  \n",
       "17671  [[0.25769, 0.45629, -0.76974, -0.37679, 0.5927...  \n",
       "17672  [[0.25769, 0.45629, -0.76974, -0.37679, 0.5927...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## remove duplicates\n",
    "all_data = all_data[all_data.duplicated(['title' , 'target'])==False]\n",
    "all_data = all_data.reset_index(drop =True)\n",
    "all_data.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target=1 (real): 6020\n",
      "target=0 (fake): 7977\n",
      "target=-1 (half true; DROP): 3676\n"
     ]
    }
   ],
   "source": [
    "print('target=1 (real):', len(all_data[all_data.binary_target == 1]))\n",
    "print('target=0 (fake):', len(all_data[all_data.binary_target == 0]))\n",
    "print('target=-1 (half true; DROP):', len(all_data[all_data.binary_target == -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read LIAR/Politifact (lp) vocab from pickle file.\n",
    "\n",
    "vocab = pd.read_pickle('parsed_data/vocab_lp.pkl')  # COMBINED LIAR and Politifact data (CMU) tokenized and POS tags added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14,460 words\n",
      "wordset:  ['<s>', '</s>', '<unk>', '.', 'the', ',', 'in', 'of', 'to', '<number>', 'a', '\"\"', 'and', '\"', 'says', 'for', '\"\"\"', 'that', 'is', 'on', 'has', 'have', 'percent', 'than', 'are', 'more', '$<number>', 'was', 'we', 'by']\n",
      "<w266_common.vocabulary2.Vocabulary2 object at 0x7fb91cd946d8>\n"
     ]
    }
   ],
   "source": [
    "print(\"{:,} words\".format(vocab.size))  # Note: this combines words from ISOT \"title\" AND \"text\" fields!\n",
    "print(\"wordset: \",vocab.ordered_words()[:30])\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LIAR and ISOT LIWC features from pickle file \n",
    "### (NOTE: will need to get new LIWC files to match vocab size of 14,460 words)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_liar = pd.read_pickle('parsed_data/liwc_liarpolitifact.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>function</th>\n",
       "      <th>pronoun</th>\n",
       "      <th>ppron</th>\n",
       "      <th>i</th>\n",
       "      <th>we</th>\n",
       "      <th>you</th>\n",
       "      <th>shehe</th>\n",
       "      <th>they</th>\n",
       "      <th>ipron</th>\n",
       "      <th>article</th>\n",
       "      <th>...</th>\n",
       "      <th>money</th>\n",
       "      <th>relig</th>\n",
       "      <th>death</th>\n",
       "      <th>informal</th>\n",
       "      <th>swear</th>\n",
       "      <th>netspeak</th>\n",
       "      <th>assent</th>\n",
       "      <th>nonflu</th>\n",
       "      <th>filler</th>\n",
       "      <th>Unnamed: 74</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   function  pronoun  ppron  i  we  you  shehe  they  ipron  article  \\\n",
       "0         0        0      0  0   0    0      0     0      0        0   \n",
       "1         0        0      0  0   0    0      0     0      0        0   \n",
       "2         0        0      0  0   0    0      0     0      0        0   \n",
       "3         0        0      0  0   0    0      0     0      0        0   \n",
       "4         1        0      0  0   0    0      0     0      0        1   \n",
       "\n",
       "      ...       money  relig  death  informal  swear  netspeak  assent  \\\n",
       "0     ...           0      0      0         0      0         0       0   \n",
       "1     ...           0      0      0         0      0         0       0   \n",
       "2     ...           0      0      0         0      0         0       0   \n",
       "3     ...           0      0      0         0      0         0       0   \n",
       "4     ...           0      0      0         0      0         0       0   \n",
       "\n",
       "   nonflu  filler  Unnamed: 74  \n",
       "0       0       0            0  \n",
       "1       0       0            0  \n",
       "2       0       0            0  \n",
       "3       0       0            0  \n",
       "4       0       0            0  \n",
       "\n",
       "[5 rows x 74 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liwc_liar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14460, 74)\n"
     ]
    }
   ],
   "source": [
    "print(liwc_liar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_isot = pd.read_pickle('parsed_data/liwc_isot2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>function</th>\n",
       "      <th>pronoun</th>\n",
       "      <th>ppron</th>\n",
       "      <th>i</th>\n",
       "      <th>we</th>\n",
       "      <th>you</th>\n",
       "      <th>shehe</th>\n",
       "      <th>they</th>\n",
       "      <th>ipron</th>\n",
       "      <th>article</th>\n",
       "      <th>...</th>\n",
       "      <th>money</th>\n",
       "      <th>relig</th>\n",
       "      <th>death</th>\n",
       "      <th>informal</th>\n",
       "      <th>swear</th>\n",
       "      <th>netspeak</th>\n",
       "      <th>assent</th>\n",
       "      <th>nonflu</th>\n",
       "      <th>filler</th>\n",
       "      <th>Unnamed: 74</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   function  pronoun  ppron  i  we  you  shehe  they  ipron  article  \\\n",
       "0         0        0      0  0   0    0      0     0      0        0   \n",
       "1         0        0      0  0   0    0      0     0      0        0   \n",
       "2         0        0      0  0   0    0      0     0      0        0   \n",
       "3         1        0      0  0   0    0      0     0      0        1   \n",
       "4         0        0      0  0   0    0      0     0      0        0   \n",
       "\n",
       "      ...       money  relig  death  informal  swear  netspeak  assent  \\\n",
       "0     ...           0      0      0         0      0         0       0   \n",
       "1     ...           0      0      0         0      0         0       0   \n",
       "2     ...           0      0      0         0      0         0       0   \n",
       "3     ...           0      0      0         0      0         0       0   \n",
       "4     ...           0      0      0         0      0         0       0   \n",
       "\n",
       "   nonflu  filler  Unnamed: 74  \n",
       "0       0       0            0  \n",
       "1       0       0            0  \n",
       "2       0       0            0  \n",
       "3       0       0            0  \n",
       "4       0       0            0  \n",
       "\n",
       "[5 rows x 74 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liwc_isot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "(152182, 74)\n"
     ]
    }
   ],
   "source": [
    "print(liwc_isot.values)\n",
    "print(liwc_isot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(liwc_isot.values[:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#liwc = tf.to_float(liwc_isot.values)\n",
    "liwc = liwc_isot.astype('float32')\n",
    "print(liwc.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(liwc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Dev / Test Split LIAR/Politifact data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, drop the data having binary_target = -1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13997, 6)\n",
      "target=1 (real): 6020\n",
      "target=0 (fake): 7977\n",
      "target=-1 (half true; DROP): 0\n"
     ]
    }
   ],
   "source": [
    "all_data = all_data[all_data.binary_target >= 0]\n",
    "print(all_data.shape)\n",
    "print('target=1 (real):', len(all_data[all_data.binary_target == 1]))\n",
    "print('target=0 (fake):', len(all_data[all_data.binary_target == 0]))\n",
    "print('target=-1 (half true; DROP):', len(all_data[all_data.binary_target == -1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split fractions add up to 1.0\n",
      "training set:  (9797, 6)\n",
      "dev set:  (2100, 6)\n",
      "test set:  (2100, 6)\n"
     ]
    }
   ],
   "source": [
    "#train/dev/train split\n",
    "#train_dev_split = 0.8\n",
    "\n",
    "train_fract = 0.70\n",
    "dev_fract = 0.15\n",
    "test_fract = 0.15\n",
    "\n",
    "if (train_fract+dev_fract+test_fract) == 1.0:\n",
    "    print('Split fractions add up to 1.0')\n",
    "else:\n",
    "    print('SPLIT FRACTIONS DO NOT ADD UP TO 1.0; PLEASE TRY AGAIN.............')\n",
    "\n",
    "#train_data = all_data[:int(len(all_data)*train_dev_split)].reset_index(drop=True)\n",
    "#dev_data = all_data[int(len(all_data)*train_dev_split):].reset_index(drop=True)\n",
    "\n",
    "train_set = all_data[ :int(len(all_data)*train_fract)].reset_index(drop=True)\n",
    "dev_set = all_data[int(len(all_data)*(train_fract)) : int(len(all_data)*(train_fract+dev_fract))].reset_index(drop=True)\n",
    "test_set = all_data[int(len(all_data)*(train_fract+dev_fract)) : ].reset_index(drop=True)\n",
    "\n",
    "print('training set: ',train_set.shape)\n",
    "print('dev set: ',dev_set.shape)\n",
    "print('test set: ',test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>title</th>\n",
       "      <th>title_tokcan</th>\n",
       "      <th>title_POS</th>\n",
       "      <th>binary_target</th>\n",
       "      <th>embedded_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Says 31 percent of Texas physicians accept all...</td>\n",
       "      <td>[says, &lt;number&gt;, percent, of, texas, physician...</td>\n",
       "      <td>[V, $, N, P, ^, N, V, D, A, ^, N, ,, R, P, $, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.11797, 0.21126, 0.29075, -0.021211, 0.7819...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>A Republican-led softening of firearms trainin...</td>\n",
       "      <td>[a, republican-led, softening, of, firearms, t...</td>\n",
       "      <td>[D, A, N, P, N, N, N, V, D, A, N, V, V, V, P, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.21705, 0.46515, -0.46757, 0.10082, 1.0135,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>The first tweet was sent from Austin.</td>\n",
       "      <td>[the, first, tweet, was, sent, from, austin, .]</td>\n",
       "      <td>[D, A, N, V, V, P, ^, ,]</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Florida has reduced its carbon emissions by 20...</td>\n",
       "      <td>[florida, has, reduced, its, carbon, emissions...</td>\n",
       "      <td>[^, V, V, L, N, N, P, $, N, P, $, ,]</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.52717, 0.16878, 0.16146, 0.93858, -0.6549...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Mt. Hood Community College is No. 1 on average...</td>\n",
       "      <td>[mt, ., hood, community, college, is, no, ., &lt;...</td>\n",
       "      <td>[^, ,, N, N, N, V, !, ,, $, P, A, &amp;, A, N, N, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.055441, 2.3025, 0.98466, -0.020482, -0.26...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                              title  \\\n",
       "0       1  Says 31 percent of Texas physicians accept all...   \n",
       "1       0  A Republican-led softening of firearms trainin...   \n",
       "2       5              The first tweet was sent from Austin.   \n",
       "3       1  Florida has reduced its carbon emissions by 20...   \n",
       "4       1  Mt. Hood Community College is No. 1 on average...   \n",
       "\n",
       "                                        title_tokcan  \\\n",
       "0  [says, <number>, percent, of, texas, physician...   \n",
       "1  [a, republican-led, softening, of, firearms, t...   \n",
       "2    [the, first, tweet, was, sent, from, austin, .]   \n",
       "3  [florida, has, reduced, its, carbon, emissions...   \n",
       "4  [mt, ., hood, community, college, is, no, ., <...   \n",
       "\n",
       "                                           title_POS  binary_target  \\\n",
       "0  [V, $, N, P, ^, N, V, D, A, ^, N, ,, R, P, $, ...              1   \n",
       "1  [D, A, N, P, N, N, N, V, D, A, N, V, V, V, P, ...              1   \n",
       "2                           [D, A, N, V, V, P, ^, ,]              0   \n",
       "3               [^, V, V, L, N, N, P, $, N, P, $, ,]              1   \n",
       "4  [^, ,, N, N, N, V, !, ,, $, P, A, &, A, N, N, ...              1   \n",
       "\n",
       "                                      embedded_title  \n",
       "0  [[0.11797, 0.21126, 0.29075, -0.021211, 0.7819...  \n",
       "1  [[0.21705, 0.46515, -0.46757, 0.10082, 1.0135,...  \n",
       "2  [[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -...  \n",
       "3  [[-0.52717, 0.16878, 0.16146, 0.93858, -0.6549...  \n",
       "4  [[-0.055441, 2.3025, 0.98466, -0.020482, -0.26...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>title</th>\n",
       "      <th>title_tokcan</th>\n",
       "      <th>title_POS</th>\n",
       "      <th>binary_target</th>\n",
       "      <th>embedded_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Says Jeb Bush -- not Charlie Crist -- signed l...</td>\n",
       "      <td>[says, jeb, bush, --, not, charlie, crist, --,...</td>\n",
       "      <td>[V, ^, ^, ,, R, ^, ^, ,, V, N, P, V, ^, N, V, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.11797, 0.21126, 0.29075, -0.021211, 0.7819...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Estimates for adopting Obamacares Medicaid exp...</td>\n",
       "      <td>[estimates, for, adopting, obamacares, medicai...</td>\n",
       "      <td>[N, P, V, ^, ^, N, N, O, V, V, ^, N, $, $, P, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.37704, -0.35551, 1.4605, -0.28685, 0.34969...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Just like Hillary Clinton, Russ Feingold had a...</td>\n",
       "      <td>[just, like, hillary, clinton, ,, russ, feingo...</td>\n",
       "      <td>[R, P, ^, ^, ,, ^, ^, V, D, N, P, N, &amp;, N, P, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.17698, 0.065221, 0.28548, -0.4243, 0.7499,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Its been 17 years that weve had unemployment h...</td>\n",
       "      <td>[its, been, &lt;number&gt;, years, that, weve, had, ...</td>\n",
       "      <td>[L, V, $, N, P, L, V, N, A, P, D, N, N, P, ^, ,]</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.76719, 0.1239, -0.11119, 0.13355, 0.18356,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Of the roughly 15 percent of Americans who don...</td>\n",
       "      <td>[of, the, roughly, &lt;number&gt;, percent, of, amer...</td>\n",
       "      <td>[P, D, R, $, N, P, ^, O, V, V, N, N, ,, N, P, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.70853, 0.57088, -0.4716, 0.18048, 0.54449,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                              title  \\\n",
       "0       0  Says Jeb Bush -- not Charlie Crist -- signed l...   \n",
       "1       3  Estimates for adopting Obamacares Medicaid exp...   \n",
       "2       3  Just like Hillary Clinton, Russ Feingold had a...   \n",
       "3       0  Its been 17 years that weve had unemployment h...   \n",
       "4       4  Of the roughly 15 percent of Americans who don...   \n",
       "\n",
       "                                        title_tokcan  \\\n",
       "0  [says, jeb, bush, --, not, charlie, crist, --,...   \n",
       "1  [estimates, for, adopting, obamacares, medicai...   \n",
       "2  [just, like, hillary, clinton, ,, russ, feingo...   \n",
       "3  [its, been, <number>, years, that, weve, had, ...   \n",
       "4  [of, the, roughly, <number>, percent, of, amer...   \n",
       "\n",
       "                                           title_POS  binary_target  \\\n",
       "0  [V, ^, ^, ,, R, ^, ^, ,, V, N, P, V, ^, N, V, ...              1   \n",
       "1  [N, P, V, ^, ^, N, N, O, V, V, ^, N, $, $, P, ...              0   \n",
       "2  [R, P, ^, ^, ,, ^, ^, V, D, N, P, N, &, N, P, ...              0   \n",
       "3   [L, V, $, N, P, L, V, N, A, P, D, N, N, P, ^, ,]              1   \n",
       "4  [P, D, R, $, N, P, ^, O, V, V, N, N, ,, N, P, ...              0   \n",
       "\n",
       "                                      embedded_title  \n",
       "0  [[0.11797, 0.21126, 0.29075, -0.021211, 0.7819...  \n",
       "1  [[0.37704, -0.35551, 1.4605, -0.28685, 0.34969...  \n",
       "2  [[0.17698, 0.065221, 0.28548, -0.4243, 0.7499,...  \n",
       "3  [[0.76719, 0.1239, -0.11119, 0.13355, 0.18356,...  \n",
       "4  [[0.70853, 0.57088, -0.4716, 0.18048, 0.54449,...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out dev set\n",
    "#dev_set.to_csv('isot_dev_set.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>title</th>\n",
       "      <th>title_tokcan</th>\n",
       "      <th>title_POS</th>\n",
       "      <th>binary_target</th>\n",
       "      <th>embedded_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>The economic impact of Atlanta's 2000 Super Bo...</td>\n",
       "      <td>[the, economic, impact, of, atlanta's, &lt;number...</td>\n",
       "      <td>[D, A, N, P, Z, $, ^, ^, V, $, N, ,]</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Health insurance companies deny \"1 out of 5 tr...</td>\n",
       "      <td>[\", health, insurance, companies, deny, \"\", &lt;n...</td>\n",
       "      <td>[,, N, N, N, V, ,, $, P, P, $, N, V, P, N, ,, ,]</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.25769, 0.45629, -0.76974, -0.37679, 0.5927...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Mitt Romney \"left Massachusetts $1 billion in ...</td>\n",
       "      <td>[\", mitt, romney, \"\", left, massachusetts, $&lt;n...</td>\n",
       "      <td>[,, ^, ^, ,, V, ^, $, $, P, N, ,, ,]</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.25769, 0.45629, -0.76974, -0.37679, 0.5927...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Horse racing could boost Georgia's economy by ...</td>\n",
       "      <td>[horse, racing, could, boost, georgia's, econo...</td>\n",
       "      <td>[N, N, V, V, Z, N, P, $, $, D, N, &amp;, V, $, P, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.20454, 0.23321, -0.59158, -0.29205, 0.293...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>With only 67 bills or so passed into law, \"201...</td>\n",
       "      <td>[\", with, only, &lt;number&gt;, bills, or, so, passe...</td>\n",
       "      <td>[,, P, A, $, N, &amp;, R, V, P, N, ,, ,, $, V, D, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.25769, 0.45629, -0.76974, -0.37679, 0.5927...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                              title  \\\n",
       "0       4  The economic impact of Atlanta's 2000 Super Bo...   \n",
       "1       4  Health insurance companies deny \"1 out of 5 tr...   \n",
       "2       4  Mitt Romney \"left Massachusetts $1 billion in ...   \n",
       "3       3  Horse racing could boost Georgia's economy by ...   \n",
       "4       0  With only 67 bills or so passed into law, \"201...   \n",
       "\n",
       "                                        title_tokcan  \\\n",
       "0  [the, economic, impact, of, atlanta's, <number...   \n",
       "1  [\", health, insurance, companies, deny, \"\", <n...   \n",
       "2  [\", mitt, romney, \"\", left, massachusetts, $<n...   \n",
       "3  [horse, racing, could, boost, georgia's, econo...   \n",
       "4  [\", with, only, <number>, bills, or, so, passe...   \n",
       "\n",
       "                                           title_POS  binary_target  \\\n",
       "0               [D, A, N, P, Z, $, ^, ^, V, $, N, ,]              0   \n",
       "1   [,, N, N, N, V, ,, $, P, P, $, N, V, P, N, ,, ,]              0   \n",
       "2               [,, ^, ^, ,, V, ^, $, $, P, N, ,, ,]              0   \n",
       "3  [N, N, V, V, Z, N, P, $, $, D, N, &, V, $, P, ...              0   \n",
       "4  [,, P, A, $, N, &, R, V, P, N, ,, ,, $, V, D, ...              1   \n",
       "\n",
       "                                      embedded_title  \n",
       "0  [[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -...  \n",
       "1  [[0.25769, 0.45629, -0.76974, -0.37679, 0.5927...  \n",
       "2  [[0.25769, 0.45629, -0.76974, -0.37679, 0.5927...  \n",
       "3  [[-0.20454, 0.23321, -0.59158, -0.29205, 0.293...  \n",
       "4  [[0.25769, 0.45629, -0.76974, -0.37679, 0.5927...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select LIAR/Politifact features and (binary) labels for training model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape: (9797,)\n",
      "[list(['says', '<number>', 'percent', 'of', 'texas', 'physicians', 'accept', 'all', 'new', 'medicaid', 'patients', ',', 'down', 'from', '<number>', 'percent', 'in', '<number>', '.'])]\n",
      "train_labels shape: (9797,)\n",
      "[1 1 0 ... 0 0 0]\n",
      "\n",
      "dev_data shape: (2100,)\n",
      "[list(['says', 'jeb', 'bush', '--', 'not', 'charlie', 'crist', '--', 'signed', 'legislation', 'that', 'let', 'duke', 'energy', 'collect', 'money', 'for', 'nuclear', 'projects', '.'])]\n",
      "dev_labels shape: (2100,)\n",
      "[1 0 0 ... 0 0 0]\n",
      "\n",
      "test_data shape: (2100,)\n",
      "[list(['the', 'economic', 'impact', 'of', \"atlanta's\", '<number>', 'super', 'bowl', 'was', '$<number>', 'million', '.'])]\n",
      "test_labels shape: (2100,)\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels = train_set.title_tokcan.values, train_set.binary_target.values\n",
    "dev_data, dev_labels = dev_set.title_tokcan.values, dev_set.binary_target.values\n",
    "test_data, test_labels = test_set.title_tokcan.values, test_set.binary_target.values\n",
    "\n",
    "train_labels = train_labels.astype(int)\n",
    "dev_labels = dev_labels.astype(int)\n",
    "test_labels = test_labels.astype(int)\n",
    "\n",
    "#train_data.head()\n",
    "print('train_data shape:', train_data.shape)\n",
    "#print(train_data[0].shape)\n",
    "print(train_data[:1])\n",
    "print('train_labels shape:', train_labels.shape)\n",
    "print(train_labels)\n",
    "print()\n",
    "print('dev_data shape:', dev_data.shape)\n",
    "print(dev_data[:1])\n",
    "print('dev_labels shape:', dev_labels.shape)\n",
    "print(dev_labels)\n",
    "print()\n",
    "print('test_data shape:', test_data.shape)\n",
    "print(test_data[:1])\n",
    "print('test_labels shape:', test_labels.shape)\n",
    "print(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95th percentile: 36.0\n"
     ]
    }
   ],
   "source": [
    "# characterize length of documents in train_data\n",
    "\n",
    "lengths = [len(train_data[i]) for i in range(train_data.shape[0])]\n",
    "\n",
    "a = np.array(lengths)\n",
    "p = np.percentile(a, 95) # return 95th percentile\n",
    "print('95th percentile:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.1.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.1.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.1.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.1.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.1.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Bokeh for plotting.\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool\n",
    "bp.output_notebook()\n",
    "\n",
    "# Helper code for plotting histograms\n",
    "def plot_length_histogram(lengths, x_range=[0,100], bins=40, normed=True):\n",
    "    hist, bin_edges = np.histogram(a=lengths, bins=bins, normed=normed, range=x_range)\n",
    "    bin_centers = (bin_edges[1:] + bin_edges[:-1])/2\n",
    "    bin_widths =  (bin_edges[1:] - bin_edges[:-1])\n",
    "\n",
    "    hover = HoverTool(tooltips=[(\"bucket\", \"@x\"), (\"count\", \"@top\")], mode=\"vline\")\n",
    "    fig = bp.figure(plot_width=800, plot_height=400, tools=[hover])\n",
    "    fig.vbar(x=bin_centers, width=bin_widths, top=hist, hover_fill_color=\"firebrick\")\n",
    "    fig.y_range.start = 0\n",
    "    fig.x_range.start = 0\n",
    "    fig.xaxis.axis_label = \"Example length (number of tokens)\"\n",
    "    fig.yaxis.axis_label = \"Frequency\"\n",
    "    bp.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"6832b403-8fb3-4efe-9cc2-33f163fdd3be\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"69f01cc0-432e-4877-8b11-32a0ac5b27dc\":{\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1012\",\"type\":\"LinearAxis\"}],\"left\":[{\"id\":\"1017\",\"type\":\"LinearAxis\"}],\"plot_height\":400,\"plot_width\":800,\"renderers\":[{\"id\":\"1012\",\"type\":\"LinearAxis\"},{\"id\":\"1016\",\"type\":\"Grid\"},{\"id\":\"1017\",\"type\":\"LinearAxis\"},{\"id\":\"1021\",\"type\":\"Grid\"},{\"id\":\"1028\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"1031\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"1022\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"1004\",\"type\":\"DataRange1d\"},\"x_scale\":{\"id\":\"1008\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"1006\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"1010\",\"type\":\"LinearScale\"}},\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"callback\":null,\"start\":0},\"id\":\"1006\",\"type\":\"DataRange1d\"},{\"attributes\":{\"plot\":{\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1013\",\"type\":\"BasicTicker\"}},\"id\":\"1016\",\"type\":\"Grid\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1002\",\"type\":\"HoverTool\"}]},\"id\":\"1022\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"1018\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1033\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"fill_color\":{\"value\":\"#1f77b4\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"field\":\"width\"},\"x\":{\"field\":\"x\"}},\"id\":\"1025\",\"type\":\"VBar\"},{\"attributes\":{\"callback\":null,\"data\":{\"top\":{\"__ndarray__\":\"DNOdkaQPID8M052RpA9QPwzTnZGkD4A/PbC32p4QkT+XYFEXhIOpP4DL8XjHC6c/eF+8WjGgsD8alhL5yPijP8JwTdRtvag/1ooBhSAkmj8g7VFCV3ahPzQHBvMJ3ZI/84rj5maGkT+qZQ/zLvyBP8qjIHmAp4I/pcEqtiGQcD8ccqZUTWVwPy0Rrxf2umA/dm/lhLHGXD8UtbHyvMRKP89HBbaNE0Q/EMQnwjBqJT8QxCfCMGolPxDEJ8IwagU/FLWx8rzEKj8QxCfCMGolPxDEJ8IwagU/AAAAAAAAAAAQxCfCMGoFPwAAAAAAAAAAEMQnwjBqBT8AAAAAAAAAABDEJ8IwahU/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=\",\"dtype\":\"float64\",\"shape\":[40]},\"width\":{\"__ndarray__\":\"AAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEAAAAAAAAAEQAAAAAAAAARAAAAAAAAABEA=\",\"dtype\":\"float64\",\"shape\":[40]},\"x\":{\"__ndarray__\":\"AAAAAAAA9D8AAAAAAAAOQAAAAAAAABlAAAAAAACAIUAAAAAAAIAmQAAAAAAAgCtAAAAAAABAMEAAAAAAAMAyQAAAAAAAQDVAAAAAAADAN0AAAAAAAEA6QAAAAAAAwDxAAAAAAABAP0AAAAAAAOBAQAAAAAAAIEJAAAAAAABgQ0AAAAAAAKBEQAAAAAAA4EVAAAAAAAAgR0AAAAAAAGBIQAAAAAAAoElAAAAAAADgSkAAAAAAACBMQAAAAAAAYE1AAAAAAACgTkAAAAAAAOBPQAAAAAAAkFBAAAAAAAAwUUAAAAAAANBRQAAAAAAAcFJAAAAAAAAQU0AAAAAAALBTQAAAAAAAUFRAAAAAAADwVEAAAAAAAJBVQAAAAAAAMFZAAAAAAADQVkAAAAAAAHBXQAAAAAAAEFhAAAAAAACwWEA=\",\"dtype\":\"float64\",\"shape\":[40]}},\"selected\":{\"id\":\"1036\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"1037\",\"type\":\"UnionRenderers\"}},\"id\":\"1024\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1018\",\"type\":\"BasicTicker\"}},\"id\":\"1021\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1013\",\"type\":\"BasicTicker\"},{\"attributes\":{\"fill_color\":{\"value\":\"firebrick\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"field\":\"width\"},\"x\":{\"field\":\"x\"}},\"id\":\"1027\",\"type\":\"VBar\"},{\"attributes\":{},\"id\":\"1008\",\"type\":\"LinearScale\"},{\"attributes\":{\"callback\":null,\"mode\":\"vline\",\"renderers\":\"auto\",\"tooltips\":[[\"bucket\",\"@x\"],[\"count\",\"@top\"]]},\"id\":\"1002\",\"type\":\"HoverTool\"},{\"attributes\":{\"callback\":null,\"start\":0},\"id\":\"1004\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"1037\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1010\",\"type\":\"LinearScale\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"field\":\"width\"},\"x\":{\"field\":\"x\"}},\"id\":\"1026\",\"type\":\"VBar\"},{\"attributes\":{\"axis_label\":\"Frequency\",\"formatter\":{\"id\":\"1035\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1018\",\"type\":\"BasicTicker\"}},\"id\":\"1017\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1035\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"plot\":null,\"text\":\"\"},\"id\":\"1031\",\"type\":\"Title\"},{\"attributes\":{\"axis_label\":\"Example length (number of tokens)\",\"formatter\":{\"id\":\"1033\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1013\",\"type\":\"BasicTicker\"}},\"id\":\"1012\",\"type\":\"LinearAxis\"},{\"attributes\":{\"data_source\":{\"id\":\"1024\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"1025\",\"type\":\"VBar\"},\"hover_glyph\":{\"id\":\"1027\",\"type\":\"VBar\"},\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1026\",\"type\":\"VBar\"},\"selection_glyph\":null,\"view\":{\"id\":\"1029\",\"type\":\"CDSView\"}},\"id\":\"1028\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"source\":{\"id\":\"1024\",\"type\":\"ColumnDataSource\"}},\"id\":\"1029\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1036\",\"type\":\"Selection\"}],\"root_ids\":[\"1003\"]},\"title\":\"Bokeh Application\",\"version\":\"1.0.1\"}};\n",
       "  var render_items = [{\"docid\":\"69f01cc0-432e-4877-8b11-32a0ac5b27dc\",\"roots\":{\"1003\":\"6832b403-8fb3-4efe-9cc2-33f163fdd3be\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1003"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_length_histogram(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ISOT data to evaluate various models below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44898 entries, 0 to 44897\n",
      "Data columns (total 9 columns):\n",
      "title           44898 non-null object\n",
      "text            44898 non-null object\n",
      "subject         44898 non-null object\n",
      "date            44898 non-null object\n",
      "target          44898 non-null object\n",
      "title_tokcan    44898 non-null object\n",
      "title_POS       44898 non-null object\n",
      "text_tokcan     44898 non-null object\n",
      "text_POS        44898 non-null object\n",
      "dtypes: object(9)\n",
      "memory usage: 527.8 MB\n"
     ]
    }
   ],
   "source": [
    "# Read ISOT data from pickle file.\n",
    "isot_data = pd.read_pickle('parsed_data/df_alldata2.pkl')  # ISOT data (CMU) tokenized and POS tags added\n",
    "\n",
    "isot_data.info(memory_usage='deep', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>target</th>\n",
       "      <th>title_tokcan</th>\n",
       "      <th>title_POS</th>\n",
       "      <th>text_tokcan</th>\n",
       "      <th>text_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BRAINIAC Gets Rejected After Trying To Buy BMW...</td>\n",
       "      <td>Does anyone else out there see a future BMW ca...</td>\n",
       "      <td>Government News</td>\n",
       "      <td>Mar 20, 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>[brainiac&lt;allcaps&gt;, gets, rejected, after, try...</td>\n",
       "      <td>[N, V, V, P, V, P, V, ^, P, ^, ^, ,, O, V, A, ...</td>\n",
       "      <td>[does, anyone, else, out, there, see, a, futur...</td>\n",
       "      <td>[V, N, R, P, R, V, D, A, ^, N, N, P, D, N, ,, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Windows 10 is Stealing Your Bandwidth (You Mig...</td>\n",
       "      <td>21st Century Wire says We ve heard a lot of no...</td>\n",
       "      <td>US_News</td>\n",
       "      <td>April 7, 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>[windows, &lt;number&gt;, is, stealing, your, bandwi...</td>\n",
       "      <td>[^, $, V, V, D, N, ,, O, V, V, P, V, O, ,]</td>\n",
       "      <td>[&lt;number&gt;st, century, wire, says, we, ve, hear...</td>\n",
       "      <td>[A, N, ^, V, O, V, V, D, N, P, R, R, A, N, P, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>STUNNING STORY The Media And Democrats Hid Fro...</td>\n",
       "      <td>In an email sent on April 15, 2011, our upstan...</td>\n",
       "      <td>left-news</td>\n",
       "      <td>Mar 2, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>[stunning&lt;allcaps&gt;, story&lt;allcaps&gt;, the, media...</td>\n",
       "      <td>[A, N, D, N, &amp;, N, V, P, ^, ,, R, Z, ^, ^, N, ...</td>\n",
       "      <td>[in, an, email, sent, on, april, &lt;number&gt;, ,, ...</td>\n",
       "      <td>[P, D, N, V, P, ^, $, ,, $, ,, D, A, N, A, ^, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>North Korea's Kim Jong Un fetes nuclear scient...</td>\n",
       "      <td>SEOUL (Reuters) - North Korean leader Kim Jong...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>September 10, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>[north, korea's, kim, jong, un, fetes, nuclear...</td>\n",
       "      <td>[^, Z, ^, ^, ^, V, A, N, ,, V, N, N]</td>\n",
       "      <td>[seoul&lt;allcaps&gt;, (, reuters, ), -, north, kore...</td>\n",
       "      <td>[^, ,, ^, ,, ,, ^, ^, N, ^, ^, ^, V, D, A, N, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>White House developing comprehensive biosecuri...</td>\n",
       "      <td>ASPEN, Colorado (Reuters) - The Trump administ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>July 20, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>[white, house, developing, comprehensive, bios...</td>\n",
       "      <td>[A, N, V, A, N, N, ,, A]</td>\n",
       "      <td>[aspen&lt;allcaps&gt;, ,, colorado, (, reuters, ), -...</td>\n",
       "      <td>[^, ,, ^, ,, ^, ,, ,, D, ^, N, V, V, D, A, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LOL! GEORGE LOPEZ Booed Off Stage At Children’...</td>\n",
       "      <td>George Lopez was hired to be the emcee for the...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Oct 14, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>[lol&lt;allcaps&gt;, !, george&lt;allcaps&gt;, lopez&lt;allca...</td>\n",
       "      <td>[!, ,, ^, ^, V, P, N, P, ^, ^, N, P, N, V, D, ...</td>\n",
       "      <td>[george, lopez, was, hired, to, be, the, emcee...</td>\n",
       "      <td>[^, ^, V, V, P, V, D, N, P, D, ^, G, N, N, G, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HILLARY CLINTON CRONYISM VIOLATES FEDERAL RULE...</td>\n",
       "      <td>Former Secretary of State Hillary Clinton soug...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Oct 6, 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>[hillary&lt;allcaps&gt;, clinton&lt;allcaps&gt;, cronyism&lt;...</td>\n",
       "      <td>[^, ^, N, V, A, N, ,, Z, ,, A, N, ,, V, N, P, ...</td>\n",
       "      <td>[former, secretary, of, state, hillary, clinto...</td>\n",
       "      <td>[A, N, P, ^, ^, ^, V, P, V, ^, &amp;, ^, ^, N, N, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Republican Senator Alexander to consult on bip...</td>\n",
       "      <td>WASHINGTON (Reuters) - U.S. Republican Senator...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>September 26, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>[republican, senator, alexander, to, consult, ...</td>\n",
       "      <td>[A, N, ^, P, V, P, A, N, N]</td>\n",
       "      <td>[washington&lt;allcaps&gt;, (, reuters, ), -, u.s., ...</td>\n",
       "      <td>[^, ,, ^, ,, ,, ^, ^, ^, ^, ^, V, ^, P, O, V, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kellyanne Conway Announces Trump’s HUGE ‘Than...</td>\n",
       "      <td>Kellyanne Conway accidentally announced exactl...</td>\n",
       "      <td>News</td>\n",
       "      <td>January 9, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>[kellyanne, conway, announces, trump’s, huge&lt;a...</td>\n",
       "      <td>[^, ^, V, Z, A, ,, V, O, ,, N, P, ^, ,, &amp;, L, ...</td>\n",
       "      <td>[kellyanne, conway, accidentally, announced, e...</td>\n",
       "      <td>[^, ^, R, V, R, R, ^, ^, V, P, V, ^, ^, P, D, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Zimbabwe's army seizes power, Mugabe confined ...</td>\n",
       "      <td>HARARE (Reuters) - Zimbabwe s military seized ...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>November 15, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>[\", zimbabwe's, army, seizes, power, ,, mugabe...</td>\n",
       "      <td>[,, Z, N, N, N, ,, ^, V, &amp;, ,, A, ,]</td>\n",
       "      <td>[harare&lt;allcaps&gt;, (, reuters, ), -, zimbabwe, ...</td>\n",
       "      <td>[^, ,, ^, ,, ,, ^, G, A, A, N, P, ^, V, O, V, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  BRAINIAC Gets Rejected After Trying To Buy BMW...   \n",
       "1  Windows 10 is Stealing Your Bandwidth (You Mig...   \n",
       "2  STUNNING STORY The Media And Democrats Hid Fro...   \n",
       "3  North Korea's Kim Jong Un fetes nuclear scient...   \n",
       "4  White House developing comprehensive biosecuri...   \n",
       "5  LOL! GEORGE LOPEZ Booed Off Stage At Children’...   \n",
       "6  HILLARY CLINTON CRONYISM VIOLATES FEDERAL RULE...   \n",
       "7  Republican Senator Alexander to consult on bip...   \n",
       "8   Kellyanne Conway Announces Trump’s HUGE ‘Than...   \n",
       "9  Zimbabwe's army seizes power, Mugabe confined ...   \n",
       "\n",
       "                                                text          subject  \\\n",
       "0  Does anyone else out there see a future BMW ca...  Government News   \n",
       "1  21st Century Wire says We ve heard a lot of no...          US_News   \n",
       "2  In an email sent on April 15, 2011, our upstan...        left-news   \n",
       "3  SEOUL (Reuters) - North Korean leader Kim Jong...        worldnews   \n",
       "4  ASPEN, Colorado (Reuters) - The Trump administ...     politicsNews   \n",
       "5  George Lopez was hired to be the emcee for the...         politics   \n",
       "6  Former Secretary of State Hillary Clinton soug...         politics   \n",
       "7  WASHINGTON (Reuters) - U.S. Republican Senator...     politicsNews   \n",
       "8  Kellyanne Conway accidentally announced exactl...             News   \n",
       "9  HARARE (Reuters) - Zimbabwe s military seized ...        worldnews   \n",
       "\n",
       "                  date target  \\\n",
       "0         Mar 20, 2016      0   \n",
       "1        April 7, 2016      0   \n",
       "2          Mar 2, 2017      0   \n",
       "3  September 10, 2017       1   \n",
       "4       July 20, 2017       1   \n",
       "5         Oct 14, 2017      0   \n",
       "6          Oct 6, 2016      0   \n",
       "7  September 26, 2017       1   \n",
       "8      January 9, 2017      0   \n",
       "9   November 15, 2017       1   \n",
       "\n",
       "                                        title_tokcan  \\\n",
       "0  [brainiac<allcaps>, gets, rejected, after, try...   \n",
       "1  [windows, <number>, is, stealing, your, bandwi...   \n",
       "2  [stunning<allcaps>, story<allcaps>, the, media...   \n",
       "3  [north, korea's, kim, jong, un, fetes, nuclear...   \n",
       "4  [white, house, developing, comprehensive, bios...   \n",
       "5  [lol<allcaps>, !, george<allcaps>, lopez<allca...   \n",
       "6  [hillary<allcaps>, clinton<allcaps>, cronyism<...   \n",
       "7  [republican, senator, alexander, to, consult, ...   \n",
       "8  [kellyanne, conway, announces, trump’s, huge<a...   \n",
       "9  [\", zimbabwe's, army, seizes, power, ,, mugabe...   \n",
       "\n",
       "                                           title_POS  \\\n",
       "0  [N, V, V, P, V, P, V, ^, P, ^, ^, ,, O, V, A, ...   \n",
       "1         [^, $, V, V, D, N, ,, O, V, V, P, V, O, ,]   \n",
       "2  [A, N, D, N, &, N, V, P, ^, ,, R, Z, ^, ^, N, ...   \n",
       "3               [^, Z, ^, ^, ^, V, A, N, ,, V, N, N]   \n",
       "4                           [A, N, V, A, N, N, ,, A]   \n",
       "5  [!, ,, ^, ^, V, P, N, P, ^, ^, N, P, N, V, D, ...   \n",
       "6  [^, ^, N, V, A, N, ,, Z, ,, A, N, ,, V, N, P, ...   \n",
       "7                        [A, N, ^, P, V, P, A, N, N]   \n",
       "8  [^, ^, V, Z, A, ,, V, O, ,, N, P, ^, ,, &, L, ...   \n",
       "9               [,, Z, N, N, N, ,, ^, V, &, ,, A, ,]   \n",
       "\n",
       "                                         text_tokcan  \\\n",
       "0  [does, anyone, else, out, there, see, a, futur...   \n",
       "1  [<number>st, century, wire, says, we, ve, hear...   \n",
       "2  [in, an, email, sent, on, april, <number>, ,, ...   \n",
       "3  [seoul<allcaps>, (, reuters, ), -, north, kore...   \n",
       "4  [aspen<allcaps>, ,, colorado, (, reuters, ), -...   \n",
       "5  [george, lopez, was, hired, to, be, the, emcee...   \n",
       "6  [former, secretary, of, state, hillary, clinto...   \n",
       "7  [washington<allcaps>, (, reuters, ), -, u.s., ...   \n",
       "8  [kellyanne, conway, accidentally, announced, e...   \n",
       "9  [harare<allcaps>, (, reuters, ), -, zimbabwe, ...   \n",
       "\n",
       "                                            text_POS  \n",
       "0  [V, N, R, P, R, V, D, A, ^, N, N, P, D, N, ,, ...  \n",
       "1  [A, N, ^, V, O, V, V, D, N, P, R, R, A, N, P, ...  \n",
       "2  [P, D, N, V, P, ^, $, ,, $, ,, D, A, N, A, ^, ...  \n",
       "3  [^, ,, ^, ,, ,, ^, ^, N, ^, ^, ^, V, D, A, N, ...  \n",
       "4  [^, ,, ^, ,, ^, ,, ,, D, ^, N, V, V, D, A, A, ...  \n",
       "5  [^, ^, V, V, P, V, D, N, P, D, ^, G, N, N, G, ...  \n",
       "6  [A, N, P, ^, ^, ^, V, P, V, ^, &, ^, ^, N, N, ...  \n",
       "7  [^, ,, ^, ,, ,, ^, ^, ^, ^, ^, V, ^, P, O, V, ...  \n",
       "8  [^, ^, R, V, R, R, ^, ^, V, P, V, ^, ^, P, D, ...  \n",
       "9  [^, ,, ^, ,, ,, ^, G, A, A, N, P, ^, V, O, V, ...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isot_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isot titles: [list(['brainiac<allcaps>', 'gets', 'rejected', 'after', 'trying', 'to', 'buy', 'bmw<allcaps>', 'with', 'ebt<allcaps>', 'card', '…', 'what', 'happens', 'next', 'is', 'hysterical<allcaps>', '!'])\n",
      " list(['windows', '<number>', 'is', 'stealing', 'your', 'bandwidth', '(', 'you', 'might', 'want', 'to', 'delete', 'it', ')'])\n",
      " list(['stunning<allcaps>', 'story<allcaps>', 'the', 'media', 'and', 'democrats', 'hid', 'from', 'public', ':', 'how', 'obama<allcaps>’s', 'ag<allcaps>', 'eric', 'holder', 'used', 'taxpayer<allcaps>', 'dollars', 'to', 'organize', 'street', 'mobs', 'against', 'george', 'zimmerman', ',', 'take', 'down', 'police', 'chief'])\n",
      " ...\n",
      " list(['(', 'video<allcaps>', ')', 'the<allcaps>', 'great<allcaps>', 'divider<allcaps>', ':', 'obama<allcaps>', 'pulls<allcaps>', 'out<allcaps>', 'the<allcaps>', 'straw<allcaps>', 'man<allcaps>', 'argument<allcaps>', 'at<allcaps>', 'the<allcaps>', 'poverty<allcaps>', 'summit<allcaps>'])\n",
      " list(['seasons<allcaps>', 'beatings<allcaps>', '!', '<number>-yr', 'old', 'shot<allcaps>', '…', 'mall', 'brawls', 'spills', 'outside', '…', 'topless', 'feminist', 'destroys', 'candy', 'store', '…', 'worst', 'of', '#blackfriday', 'videos<allcaps>'])\n",
      " list(['after', 'u.s.', 'visit', ',', 'south', \"sudan's\", 'kiir', 'orders', 'unhindered', 'aid', 'access'])]\n",
      "isot labels: [0 0 0 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "isot_title_tokans = isot_data.title_tokcan.values\n",
    "isot_labels = isot_data.target.values.astype(int)\n",
    "\n",
    "print('isot titles:', isot_title_tokans)\n",
    "print('isot labels:', isot_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISOT size: (44898,)\n"
     ]
    }
   ],
   "source": [
    "print('ISOT size:', isot_title_tokans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN: LIAR/Politifact data WITH GloVe embeddings with LIWC features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Include reference functions for viewing convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef build_vocab(corpus, V=10000, **kw):\\n    from . import vocabulary\\n    if isinstance(corpus, list):\\n        token_feed = (canonicalize_word(w) for w in corpus)\\n        vocab = vocabulary.Vocabulary(token_feed, size=V, **kw)\\n    else:\\n        token_feed = (canonicalize_word(w) for w in corpus.words())\\n        vocab = vocabulary.Vocabulary(token_feed, size=V, **kw)\\n\\n    print(\"Vocabulary: {:,} types\".format(vocab.size))\\n    return vocab\\n\\n# Window and batch functions\\ndef pad_np_array(example_ids, max_len=250, pad_id=0):\\n    \"\"\"Pad a list of lists of ids into a rectangular NumPy array.\\n\\n    Longer sequences will be truncated to max_len ids, while shorter ones will\\n    be padded with pad_id.\\n\\n    Args:\\n        example_ids: list(list(int)), sequence of ids for each example\\n        max_len: maximum sequence length\\n        pad_id: id to pad shorter sequences with\\n\\n    Returns: (x, ns)\\n        x: [num_examples, max_len] NumPy array of integer ids\\n        ns: [num_examples] NumPy array of sequence lengths (<= max_len)\\n    \"\"\"\\n    arr = np.full([len(example_ids), max_len], pad_id, dtype=np.int32)\\n    ns = np.zeros([len(example_ids)], dtype=np.int32)\\n    for i, ids in enumerate(example_ids):\\n        cpy_len = min(len(ids), max_len)\\n        arr[i,:cpy_len] = ids[:cpy_len]\\n        ns[i] = cpy_len\\n    return arr, ns\\n\\ndef id_lists_to_sparse_bow(id_lists, vocab_size):\\n    \"\"\"Convert a list-of-lists-of-ids to a sparse bag-of-words matrix.\\n\\n    Args:\\n        id_lists: (list(list(int))) list of lists of word ids\\n        vocab_size: (int) vocab size; must be greater than the largest word id\\n            in id_lists.\\n\\n    Returns:\\n        (scipy.sparse.csr_matrix) where each row is a sparse vector of word\\n        counts for the corresponding example.\\n    \"\"\"\\n    from scipy import sparse\\n    ii = []  # row indices (example ids)\\n    jj = []  # column indices (token ids)\\n    for row_id, ids in enumerate(id_lists):\\n        ii.extend([row_id]*len(ids))\\n        jj.extend(ids)\\n    x = sparse.csr_matrix((np.ones_like(ii), (ii, jj)),\\n                          shape=[len(id_lists), vocab_size])\\n    return x\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# May need this info (from utils.py)\n",
    "'''\n",
    "def build_vocab(corpus, V=10000, **kw):\n",
    "    from . import vocabulary\n",
    "    if isinstance(corpus, list):\n",
    "        token_feed = (canonicalize_word(w) for w in corpus)\n",
    "        vocab = vocabulary.Vocabulary(token_feed, size=V, **kw)\n",
    "    else:\n",
    "        token_feed = (canonicalize_word(w) for w in corpus.words())\n",
    "        vocab = vocabulary.Vocabulary(token_feed, size=V, **kw)\n",
    "\n",
    "    print(\"Vocabulary: {:,} types\".format(vocab.size))\n",
    "    return vocab\n",
    "\n",
    "# Window and batch functions\n",
    "def pad_np_array(example_ids, max_len=250, pad_id=0):\n",
    "    \"\"\"Pad a list of lists of ids into a rectangular NumPy array.\n",
    "\n",
    "    Longer sequences will be truncated to max_len ids, while shorter ones will\n",
    "    be padded with pad_id.\n",
    "\n",
    "    Args:\n",
    "        example_ids: list(list(int)), sequence of ids for each example\n",
    "        max_len: maximum sequence length\n",
    "        pad_id: id to pad shorter sequences with\n",
    "\n",
    "    Returns: (x, ns)\n",
    "        x: [num_examples, max_len] NumPy array of integer ids\n",
    "        ns: [num_examples] NumPy array of sequence lengths (<= max_len)\n",
    "    \"\"\"\n",
    "    arr = np.full([len(example_ids), max_len], pad_id, dtype=np.int32)\n",
    "    ns = np.zeros([len(example_ids)], dtype=np.int32)\n",
    "    for i, ids in enumerate(example_ids):\n",
    "        cpy_len = min(len(ids), max_len)\n",
    "        arr[i,:cpy_len] = ids[:cpy_len]\n",
    "        ns[i] = cpy_len\n",
    "    return arr, ns\n",
    "\n",
    "def id_lists_to_sparse_bow(id_lists, vocab_size):\n",
    "    \"\"\"Convert a list-of-lists-of-ids to a sparse bag-of-words matrix.\n",
    "\n",
    "    Args:\n",
    "        id_lists: (list(list(int))) list of lists of word ids\n",
    "        vocab_size: (int) vocab size; must be greater than the largest word id\n",
    "            in id_lists.\n",
    "\n",
    "    Returns:\n",
    "        (scipy.sparse.csr_matrix) where each row is a sparse vector of word\n",
    "        counts for the corresponding example.\n",
    "    \"\"\"\n",
    "    from scipy import sparse\n",
    "    ii = []  # row indices (example ids)\n",
    "    jj = []  # column indices (token ids)\n",
    "    for row_id, ids in enumerate(id_lists):\n",
    "        ii.extend([row_id]*len(ids))\n",
    "        jj.extend(ids)\n",
    "    x = sparse.csr_matrix((np.ones_like(ii), (ii, jj)),\n",
    "                          shape=[len(id_lists), vocab_size])\n",
    "    return x\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef get_filtered_split(split=\\'train\\', df_idxs=None, root_only=False):\\n    if not hasattr(split):\\n        raise ValueError(\"Invalid split name \\'%s\\'\" % name)\\n    df = getattr(split)\\n    if df_idxs is not None:\\n        df = df.loc[df_idxs]\\n    #if root_only:          # Should not need in Final Project.\\n        #df = df[df.is_root]\\n    return df\\n\\ndef as_padded_array(split=\\'train\\', max_len=40, pad_id=0,\\n                    root_only=False, df_idxs=None):\\n    \"\"\"Return the dataset as a (padded) NumPy array.\\n    Longer sequences will be truncated to max_len ids, while shorter ones\\n    will be padded with pad_id.\\n    Args:\\n      split: \\'train\\' or \\'test\\'\\n      max_len: maximum sequence length\\n      pad_id: id to pad shorter sequences with\\n      root_only: if true, will only export root phrases\\n      df_idxs: (optional) custom list of indices to export\\n    Returns: (x, ns, y)\\n      x: [num_examples, max_len] NumPy array of integer ids\\n      ns: [num_examples] NumPy array of sequence lengths (<= max_len)\\n      y: [num_examples] NumPy array of target ids\\n    \"\"\"\\n    df = get_filtered_split(split, df_idxs, root_only)\\n    x, ns = utils.pad_np_array(df.ids, max_len=max_len, pad_id=pad_id)\\n    return x, ns, np.array(df.label, dtype=np.int32)\\n\\ndef as_sparse_bow(split=\\'train\\', root_only=False, df_idxs=None):\\n    from scipy import sparse\\n    df = get_filtered_split(split, df_idxs, root_only)\\n    x = utils.id_lists_to_sparse_bow(df[\\'ids\\'], self.vocab.size)\\n    y = np.array(df.label, dtype=np.int32)\\n    return x, y\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are functions that were in the \"SSTDataset\" class in sst.py from A2\n",
    "'''\n",
    "def get_filtered_split(split='train', df_idxs=None, root_only=False):\n",
    "    if not hasattr(split):\n",
    "        raise ValueError(\"Invalid split name '%s'\" % name)\n",
    "    df = getattr(split)\n",
    "    if df_idxs is not None:\n",
    "        df = df.loc[df_idxs]\n",
    "    #if root_only:          # Should not need in Final Project.\n",
    "        #df = df[df.is_root]\n",
    "    return df\n",
    "\n",
    "def as_padded_array(split='train', max_len=40, pad_id=0,\n",
    "                    root_only=False, df_idxs=None):\n",
    "    \"\"\"Return the dataset as a (padded) NumPy array.\n",
    "    Longer sequences will be truncated to max_len ids, while shorter ones\n",
    "    will be padded with pad_id.\n",
    "    Args:\n",
    "      split: 'train' or 'test'\n",
    "      max_len: maximum sequence length\n",
    "      pad_id: id to pad shorter sequences with\n",
    "      root_only: if true, will only export root phrases\n",
    "      df_idxs: (optional) custom list of indices to export\n",
    "    Returns: (x, ns, y)\n",
    "      x: [num_examples, max_len] NumPy array of integer ids\n",
    "      ns: [num_examples] NumPy array of sequence lengths (<= max_len)\n",
    "      y: [num_examples] NumPy array of target ids\n",
    "    \"\"\"\n",
    "    df = get_filtered_split(split, df_idxs, root_only)\n",
    "    x, ns = utils.pad_np_array(df.ids, max_len=max_len, pad_id=pad_id)\n",
    "    return x, ns, np.array(df.label, dtype=np.int32)\n",
    "\n",
    "def as_sparse_bow(split='train', root_only=False, df_idxs=None):\n",
    "    from scipy import sparse\n",
    "    df = get_filtered_split(split, df_idxs, root_only)\n",
    "    x = utils.id_lists_to_sparse_bow(df['ids'], self.vocab.size)\n",
    "    y = np.array(df.label, dtype=np.int32)\n",
    "    return x, y\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct train, dev, test data arrays  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14, 9, 22, 7, 73, 4446, 1590, 79, 60, 306, 1257, 5, 145, 34, 9, 22, 6, 9, 3], [10, 4447, 7509, 7, 1070, 1292, 668, 766, 17, 5483, 926, 50, 51, 489, 8, 896, 446, 35, 10, 32, 1552, 3], [4, 108, 7510, 27, 545, 34, 390, 3], [124, 20, 639, 110, 1214, 1648, 29, 9, 22, 90, 9, 3], [7512, 3, 2642, 701, 253, 18, 82, 3, 9, 19, 132, 12, 291, 2643, 102, 12, 328, 3]]\n",
      "\n",
      "[[  14    9   22    7   73 4446 1590   79   60  306 1257    5  145   34\n",
      "     9   22    6    9    3    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  10 4447 7509    7 1070 1292  668  766   17 5483  926   50   51  489\n",
      "     8  896  446   35   10   32 1552    3    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "\n",
      "[19 22]\n",
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "## Training data\n",
    "\n",
    "all_train_ids=[]\n",
    "for i, tokens in enumerate(train_data):  # here, tokens are the words in a single sentence\n",
    "    sent_ids = vocab.words_to_ids(tokens)\n",
    "    all_train_ids.append(sent_ids)\n",
    "print(all_train_ids[:5])\n",
    "\n",
    "max_len = 40   # Retain this setting, since it fits the ISOT \"title\" length distribution quite well.\n",
    "train_x, train_ns = utils.pad_np_array(all_train_ids, max_len=max_len)\n",
    "print()\n",
    "print(train_x[:2])\n",
    "print()\n",
    "print(train_ns[:2])\n",
    "\n",
    "train_y = train_labels\n",
    "print(train_y[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14, 1013, 173, 118, 39, 534, 562, 118, 484, 288, 17, 582, 4474, 337, 1936, 103, 15, 431, 755, 3], [1141, 15, 5110, 7497, 306, 782, 557, 33, 58, 157, 339, 336, 26, 52, 258, 9, 3], [123, 207, 206, 143, 5, 1365, 1316, 93, 10, 14252, 35, 2819, 12, 2496, 29, 690, 10, 812, 3417, 281, 118, 14253, 4, 1316, 1215, 118, 156, 7283, 581, 493, 15, 70, 121, 3], [110, 65, 9, 42, 17, 315, 93, 152, 243, 23, 4, 138, 132, 6, 355, 3], [7, 4, 762, 9, 22, 7, 116, 68, 226, 21, 40, 119, 5, 188, 7, 158, 238, 25, 23, 85, 9, 10, 46, 3]]\n",
      "\n",
      "[[  14 1013  173  118   39  534  562  118  484  288   17  582 4474  337\n",
      "  1936  103   15  431  755    3    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [1141   15 5110 7497  306  782  557   33   58  157  339  336   26   52\n",
      "   258    9    3    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "\n",
      "[20 17]\n",
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "## Dev data\n",
    "\n",
    "all_dev_ids=[]\n",
    "for i, tokens in enumerate(dev_data):  # here, tokens are the words in a single sentence\n",
    "    sent_ids = vocab.words_to_ids(tokens)\n",
    "    all_dev_ids.append(sent_ids)\n",
    "print(all_dev_ids[:5])\n",
    "\n",
    "max_len = 40   # Retain this setting, since it fits the ISOT \"title\" length distribution quite well.\n",
    "dev_x, dev_ns = utils.pad_np_array(all_dev_ids, max_len=max_len)\n",
    "print()\n",
    "print(dev_x[:2])\n",
    "print()\n",
    "print(dev_ns[:2])\n",
    "\n",
    "dev_y = dev_labels\n",
    "print(dev_y[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 293, 944, 7, 12775, 9, 1254, 1857, 27, 26, 52, 3], [13, 40, 119, 263, 1169, 11, 9, 81, 7, 9, 2441, 4252, 29, 823, 3, 16], [13, 213, 184, 11, 403, 412, 26, 75, 6, 128, 3, 16], [2978, 4564, 162, 7015, 4191, 222, 29, 26, 75, 10, 46, 12, 386, 9, 8, 9, 53, 3], [13, 35, 83, 9, 437, 67, 198, 231, 115, 97, 5, 11, 9, 27, 4, 387, 5419, 46, 6, 383, 189, 90, 315, 65, 2113, 300, 3, 16]]\n",
      "\n",
      "[[    4   293   944     7 12775     9  1254  1857    27    26    52     3\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]\n",
      " [   13    40   119   263  1169    11     9    81     7     9  2441  4252\n",
      "     29   823     3    16     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]]\n",
      "\n",
      "[12 16]\n",
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "## Test data\n",
    "\n",
    "all_test_ids=[]\n",
    "for i, tokens in enumerate(test_data):  # here, tokens are the words in a single sentence\n",
    "    sent_ids = vocab.words_to_ids(tokens)\n",
    "    all_test_ids.append(sent_ids)\n",
    "print(all_test_ids[:5])\n",
    "\n",
    "max_len = 40   # Retain this setting, since it fits the ISOT \"title\" length distribution quite well.\n",
    "test_x, test_ns = utils.pad_np_array(all_test_ids, max_len=max_len)\n",
    "print()\n",
    "print(test_x[:2])\n",
    "print()\n",
    "print(test_ns[:2])\n",
    "\n",
    "test_y = test_labels\n",
    "print(test_y[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples:\n",
      " [[  14    9   22    7   73 4446 1590   79   60  306 1257    5  145   34\n",
      "     9   22    6    9    3    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  10 4447 7509    7 1070 1292  668  766   17 5483  926   50   51  489\n",
      "     8  896  446   35   10   32 1552    3    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   4  108 7510   27  545   34  390    3    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "Original sequence lengths:  [19 22  8]\n",
      "Target labels:  [1 1 0]\n",
      "\n",
      "Padded:\n",
      " says <number> percent of texas physicians accept all new medicaid patients , down from <number> percent in <number> . <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>\n",
      "Un-padded:\n",
      " says <number> percent of texas physicians accept all new medicaid patients , down from <number> percent in <number> .\n"
     ]
    }
   ],
   "source": [
    "print(\"Examples:\\n\", train_x[:3])\n",
    "print(\"Original sequence lengths: \", train_ns[:3])\n",
    "print(\"Target labels: \", train_y[:3])\n",
    "print(\"\")\n",
    "print(\"Padded:\\n\", \" \".join(vocab.ids_to_words(train_x[0])))\n",
    "print(\"Un-padded:\\n\", \" \".join(vocab.ids_to_words(train_x[0,:train_ns[0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use tf.Estimator API along with nbow_models_x.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things to consider:  \n",
    "- Start w/ 2 epochs (20 was original)       \n",
    "- Consider use of dropouts in fully-connected layers     \n",
    "-  Use embed_dim = 300 rather than 50??   \n",
    "- xx  \n",
    "...  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 14460\n"
     ]
    }
   ],
   "source": [
    "print('vocab size:', vocab.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (14,460 words) written to '/tmp/tf_cnn_20181209-0550/metadata.tsv'\n",
      "Projector config written to /tmp/tf_cnn_20181209-0550/projector_config.pbtxt\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tf_cnn_20181209-0550', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb8828c2a20>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "\n",
      "To view training (once it starts), run:\n",
      "\n",
      "    tensorboard --logdir='/tmp/tf_cnn_20181209-0550' --port 6006\n",
      "\n",
      "Then in your browser, open: http://localhost:6006\n"
     ]
    }
   ],
   "source": [
    "## Setup model framework\n",
    "## (Must specify correct nbow_model_x name in this cell to use the correct nbow_model_x.py file.)\n",
    "\n",
    "import cnn_models_2; reload(cnn_models_2)\n",
    "\n",
    "# Specify model hyperparameters as used by model_fn.  Use embed_dim2=74 for all LIWC\n",
    "### ADD NEW PARAMETER: liwc_dim???\n",
    "\n",
    "model_params = dict(V=vocab.size, embed_dim=300, filters=100, kernel_sizes=[2,4,7], \n",
    "                    hidden_dims=[25,75], num_classes=2, encoder_type='cnn', input_data = 'liar', \n",
    "                    dropout_rate=0.5, lr=0.1, optimizer='adagrad', beta=0.01)  # can set optimizer to 'adagrad' or 'adam', which is slower here\n",
    "\n",
    "checkpoint_dir = \"/tmp/tf_cnn_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "if os.path.isdir(checkpoint_dir):\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "# Write vocabulary to file, so TensorBoard can label embeddings.\n",
    "# creates checkpoint_dir/projector_config.pbtxt and checkpoint_dir/metadata.tsv\n",
    "#ds.vocab.write_projector_config(checkpoint_dir, \"Encoder/Embedding_Layer/W_embed\")\n",
    "vocab.write_projector_config(checkpoint_dir, \"Encoder/Embedding_Layer/W_embed\")\n",
    "\n",
    "model = tf.estimator.Estimator(model_fn=cnn_models_2.classifier_model_fn, \n",
    "                               params=model_params,\n",
    "                               model_dir=checkpoint_dir)\n",
    "print(\"\")\n",
    "print(\"To view training (once it starts), run:\\n\")\n",
    "print(\"    tensorboard --logdir='{:s}' --port 6006\".format(checkpoint_dir))\n",
    "print(\"\\nThen in your browser, open: http://localhost:6006\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tf_cnn_20181209-0550/model.ckpt.\n",
      "INFO:tensorflow:loss = 116.93371, step = 1\n",
      "INFO:tensorflow:global_step/sec: 8.23974\n",
      "INFO:tensorflow:loss = 63.3027, step = 101 (12.138 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.75318\n",
      "INFO:tensorflow:loss = 56.32376, step = 201 (14.810 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.82484\n",
      "INFO:tensorflow:loss = 51.992393, step = 301 (14.650 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 307 into /tmp/tf_cnn_20181209-0550/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 8.551152.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-09-05:51:40\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181209-0550/model.ckpt-307\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-09-05:51:43\n",
      "INFO:tensorflow:Saving dict for global step 307: accuracy = 0.6109524, cross_entropy_loss = 0.6708942, global_step = 307, loss = 201.11145\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 307: /tmp/tf_cnn_20181209-0550/model.ckpt-307\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181209-0550/model.ckpt-307\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 307 into /tmp/tf_cnn_20181209-0550/model.ckpt.\n",
      "INFO:tensorflow:loss = 47.2973, step = 308\n",
      "INFO:tensorflow:global_step/sec: 12.6223\n",
      "INFO:tensorflow:loss = 40.286076, step = 408 (7.925 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.3644\n",
      "INFO:tensorflow:loss = 42.074986, step = 508 (7.482 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.3525\n",
      "INFO:tensorflow:loss = 40.53483, step = 608 (7.489 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 614 into /tmp/tf_cnn_20181209-0550/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 6.3712687.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-09-05:52:18\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181209-0550/model.ckpt-614\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-09-05:52:19\n",
      "INFO:tensorflow:Saving dict for global step 614: accuracy = 0.6109524, cross_entropy_loss = 0.6593918, global_step = 614, loss = 157.63205\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 614: /tmp/tf_cnn_20181209-0550/model.ckpt-614\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181209-0550/model.ckpt-614\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 614 into /tmp/tf_cnn_20181209-0550/model.ckpt.\n",
      "INFO:tensorflow:loss = 37.617043, step = 615\n",
      "INFO:tensorflow:global_step/sec: 12.7437\n",
      "INFO:tensorflow:loss = 33.50321, step = 715 (7.850 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.373\n",
      "INFO:tensorflow:loss = 35.92975, step = 815 (7.476 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.3724\n",
      "INFO:tensorflow:loss = 34.53531, step = 915 (7.479 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 921 into /tmp/tf_cnn_20181209-0550/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 5.6158276.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-09-05:52:52\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181209-0550/model.ckpt-921\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-09-05:52:54\n",
      "INFO:tensorflow:Saving dict for global step 921: accuracy = 0.6304762, cross_entropy_loss = 0.6416362, global_step = 921, loss = 135.99232\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 921: /tmp/tf_cnn_20181209-0550/model.ckpt-921\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181209-0550/model.ckpt-921\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 921 into /tmp/tf_cnn_20181209-0550/model.ckpt.\n",
      "INFO:tensorflow:loss = 32.633163, step = 922\n",
      "INFO:tensorflow:global_step/sec: 12.9769\n",
      "INFO:tensorflow:loss = 29.746506, step = 1022 (7.709 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.6748\n",
      "INFO:tensorflow:loss = 32.07744, step = 1122 (7.312 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.4962\n",
      "INFO:tensorflow:loss = 30.623138, step = 1222 (7.409 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1228 into /tmp/tf_cnn_20181209-0550/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 5.10569.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-09-05:53:25\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181209-0550/model.ckpt-1228\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-09-05:53:27\n",
      "INFO:tensorflow:Saving dict for global step 1228: accuracy = 0.64238095, cross_entropy_loss = 0.63009274, global_step = 1228, loss = 121.83203\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1228: /tmp/tf_cnn_20181209-0550/model.ckpt-1228\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181209-0550/model.ckpt-1228\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1228 into /tmp/tf_cnn_20181209-0550/model.ckpt.\n",
      "INFO:tensorflow:loss = 29.3574, step = 1229\n",
      "INFO:tensorflow:global_step/sec: 12.9774\n",
      "INFO:tensorflow:loss = 27.188284, step = 1329 (7.710 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.4626\n",
      "INFO:tensorflow:loss = 29.305298, step = 1429 (7.428 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.4793\n",
      "INFO:tensorflow:loss = 27.758913, step = 1529 (7.416 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1535 into /tmp/tf_cnn_20181209-0550/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4.7018185.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-09-05:53:59\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181209-0550/model.ckpt-1535\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-09-05:54:01\n",
      "INFO:tensorflow:Saving dict for global step 1535: accuracy = 0.65238094, cross_entropy_loss = 0.6177525, global_step = 1535, loss = 111.44267\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1535: /tmp/tf_cnn_20181209-0550/model.ckpt-1535\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181209-0550/model.ckpt-1535\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1535 into /tmp/tf_cnn_20181209-0550/model.ckpt.\n",
      "INFO:tensorflow:loss = 26.955404, step = 1536\n",
      "INFO:tensorflow:global_step/sec: 12.7377\n",
      "INFO:tensorflow:loss = 25.260342, step = 1636 (7.852 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.4096\n",
      "INFO:tensorflow:loss = 27.181984, step = 1736 (7.458 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.2248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 25.54451, step = 1836 (7.562 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1842 into /tmp/tf_cnn_20181209-0550/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4.3430767.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-09-05:54:34\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181209-0550/model.ckpt-1842\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-09-05:54:35\n",
      "INFO:tensorflow:Saving dict for global step 1842: accuracy = 0.6566667, cross_entropy_loss = 0.6073793, global_step = 1842, loss = 103.32674\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1842: /tmp/tf_cnn_20181209-0550/model.ckpt-1842\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181209-0550/model.ckpt-1842\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1842 into /tmp/tf_cnn_20181209-0550/model.ckpt.\n",
      "INFO:tensorflow:loss = 25.10214, step = 1843\n",
      "INFO:tensorflow:global_step/sec: 13.2964\n",
      "INFO:tensorflow:loss = 23.728954, step = 1943 (7.523 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.4293\n",
      "INFO:tensorflow:loss = 25.442915, step = 2043 (7.446 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.311\n",
      "INFO:tensorflow:loss = 23.776312, step = 2143 (7.513 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2149 into /tmp/tf_cnn_20181209-0550/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4.1061707.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-09-05:55:07\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181209-0550/model.ckpt-2149\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-09-05:55:08\n",
      "INFO:tensorflow:Saving dict for global step 2149: accuracy = 0.66809523, cross_entropy_loss = 0.596536, global_step = 2149, loss = 96.72493\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2149: /tmp/tf_cnn_20181209-0550/model.ckpt-2149\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181209-0550/model.ckpt-2149\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 2149 into /tmp/tf_cnn_20181209-0550/model.ckpt.\n",
      "INFO:tensorflow:loss = 23.596544, step = 2150\n",
      "INFO:tensorflow:global_step/sec: 12.9648\n",
      "INFO:tensorflow:loss = 22.41985, step = 2250 (7.717 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.5474\n",
      "INFO:tensorflow:loss = 24.04851, step = 2350 (7.380 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.3163\n",
      "INFO:tensorflow:loss = 22.298218, step = 2450 (7.510 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2456 into /tmp/tf_cnn_20181209-0550/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.8874824.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-09-05:55:41\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181209-0550/model.ckpt-2456\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-09-05:55:42\n",
      "INFO:tensorflow:Saving dict for global step 2456: accuracy = 0.69238096, cross_entropy_loss = 0.58195966, global_step = 2456, loss = 91.193924\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2456: /tmp/tf_cnn_20181209-0550/model.ckpt-2456\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181209-0550/model.ckpt-2456\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 2456 into /tmp/tf_cnn_20181209-0550/model.ckpt.\n",
      "INFO:tensorflow:loss = 22.347412, step = 2457\n",
      "INFO:tensorflow:global_step/sec: 12.8085\n",
      "INFO:tensorflow:loss = 21.366707, step = 2557 (7.809 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.4552\n",
      "INFO:tensorflow:loss = 22.819645, step = 2657 (7.433 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.3278\n",
      "INFO:tensorflow:loss = 21.078394, step = 2757 (7.502 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2763 into /tmp/tf_cnn_20181209-0550/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.7313247.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-09-05:56:15\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181209-0550/model.ckpt-2763\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-09-05:56:17\n",
      "INFO:tensorflow:Saving dict for global step 2763: accuracy = 0.67, cross_entropy_loss = 0.5970694, global_step = 2763, loss = 86.49376\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2763: /tmp/tf_cnn_20181209-0550/model.ckpt-2763\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181209-0550/model.ckpt-2763\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 2763 into /tmp/tf_cnn_20181209-0550/model.ckpt.\n",
      "INFO:tensorflow:loss = 21.345879, step = 2764\n",
      "INFO:tensorflow:global_step/sec: 12.8408\n",
      "INFO:tensorflow:loss = 20.462154, step = 2864 (7.790 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.3375\n",
      "INFO:tensorflow:loss = 21.593758, step = 2964 (7.498 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.25\n",
      "INFO:tensorflow:loss = 20.041348, step = 3064 (7.547 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3070 into /tmp/tf_cnn_20181209-0550/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.5993056.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-09-05:56:48\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181209-0550/model.ckpt-3070\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-09-05:56:50\n",
      "INFO:tensorflow:Saving dict for global step 3070: accuracy = 0.7133333, cross_entropy_loss = 0.5586186, global_step = 3070, loss = 82.35854\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3070: /tmp/tf_cnn_20181209-0550/model.ckpt-3070\n"
     ]
    }
   ],
   "source": [
    "## Train model and Evaluate on Dev data\n",
    "\n",
    "# Training params, just used in this cell for the input_fn-s\n",
    "train_params = dict(batch_size=32, total_epochs=10, eval_every=1) # start with 2 epochs rather than 20; eval_every=1 (was 2)\n",
    "assert(train_params['total_epochs'] % train_params['eval_every'] == 0)\n",
    "\n",
    "# Construct and train the model, saving checkpoints to the directory above.\n",
    "# Input function for training set batches\n",
    "# Do 'eval_every' epochs at once, followed by evaluating on the dev set.\n",
    "# NOTE: use patch_numpy_io.numpy_input_fn instead of tf.estimator.inputs.numpy_input_fn\n",
    "train_input_fn = patched_numpy_io.numpy_input_fn(\n",
    "                    x={\"ids\": train_x, \"ns\": train_ns}, y=train_y,\n",
    "                    batch_size=train_params['batch_size'], \n",
    "                    num_epochs=train_params['eval_every'], shuffle=True, seed=42\n",
    "                 )\n",
    "\n",
    "# Input function for dev set batches. As above, but:\n",
    "# - Don't randomize order\n",
    "# - Iterate exactly once (one epoch)\n",
    "dev_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": dev_x, \"ns\": dev_ns}, y=dev_y,\n",
    "                    batch_size=128, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "for _ in range(train_params['total_epochs'] // train_params['eval_every']):\n",
    "    # Train for a few epochs, then evaluate on dev\n",
    "    model.train(input_fn=train_input_fn)\n",
    "    eval_metrics = model.evaluate(input_fn=dev_input_fn, name=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-09-05:56:52\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181209-0550/model.ckpt-3070\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-09-05:56:53\n",
      "INFO:tensorflow:Saving dict for global step 3070: accuracy = 0.72333336, cross_entropy_loss = 0.551084, global_step = 3070, loss = 81.44126\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3070: /tmp/tf_cnn_20181209-0550/model.ckpt-3070\n",
      "Accuracy on test set: 72.33%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.72333336,\n",
       " 'cross_entropy_loss': 0.551084,\n",
       " 'loss': 81.44126,\n",
       " 'global_step': 3070}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Evaluate model on (ISOT) Test data\n",
    "\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": test_x, \"ns\": test_ns}, y=test_y,\n",
    "                    batch_size=128, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "eval_metrics = model.evaluate(input_fn=test_input_fn, name=\"test\")\n",
    "\n",
    "print(\"Accuracy on test set: {:.02%}\".format(eval_metrics['accuracy']))\n",
    "eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181209-0550/model.ckpt-3070\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Accuracy on test set: 72.33%\n"
     ]
    }
   ],
   "source": [
    "## We can also evaluate the old-fashioned way, by calling model.predict(...) and working with the predicted labels directly:\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "predictions = list(model.predict(test_input_fn))  # list of dicts\n",
    "y_pred = [p['max'] for p in predictions]\n",
    "acc = accuracy_score(y_pred, test_y)\n",
    "print(\"Accuracy on test set: {:.02%}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy of 70% is better than the baseline NB result of 62%.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create padded ISOT data and apply prediction function to ISOT data.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 831, 2434, 183, 618, 8, 382, 2, 35, 2, 1234, 2, 165, 2859, 363, 18, 2, 425], [6364, 9, 18, 8701, 177, 2, 30, 61, 1364, 329, 8, 14235, 33, 31], [2, 2, 4, 977, 12, 169, 8032, 34, 117, 325, 421, 2, 2, 1120, 3443, 294, 2, 155, 8, 8419, 469, 2, 129, 289, 8421, 5, 254, 145, 374, 1433], [591, 2, 4383, 2, 12000, 2, 431, 1658, 5, 3299, 2, 2], [282, 136, 3248, 3308, 2, 3606, 325, 1466]]\n",
      "\n",
      "[[    2   831  2434   183   618     8   382     2    35     2  1234     2\n",
      "    165  2859   363    18     2   425     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]\n",
      " [ 6364     9    18  8701   177     2    30    61  1364   329     8 14235\n",
      "     33    31     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]]\n",
      "\n",
      "[18 14]\n",
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "## ISOT data padding\n",
    "\n",
    "all_isot_ids=[]\n",
    "for i, tokens in enumerate(isot_title_tokans):  # here, tokens are the words in a single sentence\n",
    "    sent_ids = vocab.words_to_ids(tokens)\n",
    "    all_isot_ids.append(sent_ids)\n",
    "print(all_isot_ids[:5])\n",
    "\n",
    "max_len = 40   # Retain this setting, since it fits the ISOT \"title\" length distribution quite well.\n",
    "isot_x, isot_ns = utils.pad_np_array(all_isot_ids, max_len=max_len)\n",
    "print()\n",
    "print(isot_x[:2])\n",
    "print()\n",
    "print(isot_ns[:2])\n",
    "\n",
    "isot_y = isot_labels\n",
    "print(isot_y[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-09-05:56:57\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_cnn_20181209-0550/model.ckpt-3070\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-09-05:57:19\n",
      "INFO:tensorflow:Saving dict for global step 3070: accuracy = 0.5747027, cross_entropy_loss = 0.7497699, global_step = 3070, loss = 166.81721\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3070: /tmp/tf_cnn_20181209-0550/model.ckpt-3070\n",
      "Accuracy on ISOT set: 57.47%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5747027,\n",
       " 'cross_entropy_loss': 0.7497699,\n",
       " 'loss': 166.81721,\n",
       " 'global_step': 3070}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Evaluate model on LIAR data\n",
    "\n",
    "\n",
    "test_input_fn_isot = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": isot_x, \"ns\": isot_ns}, y=isot_y,\n",
    "                    batch_size=128, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "eval_metrics = model.evaluate(input_fn=test_input_fn_isot, name=\"test\")\n",
    "\n",
    "print(\"Accuracy on ISOT set: {:.02%}\".format(eval_metrics['accuracy']))\n",
    "eval_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction accuracy of 55% for ISOT data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  13,   40,  119, ...,    0,    0,    0],\n",
       "       [  13,   35,   83, ...,    0,    0,    0],\n",
       "       [ 532,  500,   25, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [  13,   14, 2515, ...,    0,    0,    0],\n",
       "       [  13,    4,    9, ...,    0,    0,    0],\n",
       "       [  13, 2594, 5723, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x[y_pred != test_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
