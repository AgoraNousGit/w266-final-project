{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIAR DETECTION GROUP PROJECT\n",
    "\n",
    "Run the cell below to import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.utils import shuffle\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary, tf_embed_viz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "Loading the \"Fake News\" dataset from the Information security and object technology (ISOT) Research lab at the University of Victoria School of Engineering.\n",
    "\n",
    "The ISOT Fake News Dataset is a compilation of several thousands fake news and truthful articles, obtained from different legitimate news sites and sites flagged as unreliable by politifact.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>44898</td>\n",
       "      <td>44898</td>\n",
       "      <td>44898</td>\n",
       "      <td>44898</td>\n",
       "      <td>44898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>38729</td>\n",
       "      <td>38646</td>\n",
       "      <td>8</td>\n",
       "      <td>2397</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Factbox: Trump fills top jobs for his administ...</td>\n",
       "      <td></td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 20, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>14</td>\n",
       "      <td>627</td>\n",
       "      <td>11272</td>\n",
       "      <td>182</td>\n",
       "      <td>23481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title   text  \\\n",
       "count                                               44898  44898   \n",
       "unique                                              38729  38646   \n",
       "top     Factbox: Trump fills top jobs for his administ...          \n",
       "freq                                                   14    627   \n",
       "\n",
       "             subject                date target  \n",
       "count          44898               44898  44898  \n",
       "unique             8                2397      2  \n",
       "top     politicsNews  December 20, 2017       0  \n",
       "freq           11272                 182  23481  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define each downloaded file\n",
    "FAKE_FILENAME = 'Fake.csv'\n",
    "TRUE_FILENAME = 'True.csv'\n",
    "\n",
    "# define the downloaded file path \n",
    "DATAPATH = './datasets/ISOT_FakeNews/'\n",
    "\n",
    "def get_data(filename):\n",
    "    '''Read CSV file into a pandas dataframe'''\n",
    "      \n",
    "    filepath = DATAPATH + filename\n",
    "    return pd.read_csv(filepath, header=0, sep=',', quotechar='\"')\n",
    "\n",
    "\n",
    "fake_data = get_data(FAKE_FILENAME)\n",
    "true_data = get_data(TRUE_FILENAME)\n",
    "\n",
    "\n",
    "\n",
    "# add a label column to the data with the target values\n",
    "fake_data.loc[:,'target'] = '0'\n",
    "true_data['target'] = '1'\n",
    "\n",
    "#append the datasets and shuffle them\n",
    "all_data = true_data.append(fake_data, ignore_index=True)\n",
    "all_data = all_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "all_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "Check for NA values.\n",
    "\n",
    "May not want the dataset to contain the 'subject' since all the true news data comes from \"Reuters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title      0\n",
       "text       0\n",
       "subject    0\n",
       "date       0\n",
       "target     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44898 entries, 0 to 44897\n",
      "Data columns (total 5 columns):\n",
      "title      44898 non-null object\n",
      "text       44898 non-null object\n",
      "subject    44898 non-null object\n",
      "date       44898 non-null object\n",
      "target     44898 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 151.9 MB\n"
     ]
    }
   ],
   "source": [
    "all_data.info(memory_usage='deep', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and Canonicalize Text\n",
    "\n",
    "Need to work on Tokenize and Canonicalizing text. Words like \"Obama's\" need to be corrected. Do we need to mark of sentences within a text? Might want to use some regex code from camron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'abhishek',\n",
       " '.',\n",
       " 'i',\n",
       " 'have',\n",
       " 'no',\n",
       " 'clue',\n",
       " '.',\n",
       " 'learning',\n",
       " 'the',\n",
       " 'back',\n",
       " '-',\n",
       " 'portion',\n",
       " 'that',\n",
       " 'i',\n",
       " 'never',\n",
       " 'cared',\n",
       " 'for',\n",
       " '.',\n",
       " 'obama',\n",
       " \"'\",\n",
       " 's',\n",
       " 'nephew',\n",
       " '.',\n",
       " '<user>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Source:  https://gist.github.com/tokestermw/cb87a97113da12acb388\n",
    "\"\"\"\n",
    "\n",
    "FLAGS = re.MULTILINE | re.DOTALL\n",
    "\n",
    "def hashtag(text):\n",
    "    text = text.group()\n",
    "    hashtag_body = text[1:]\n",
    "    if hashtag_body.isupper():\n",
    "        result = \" {} \".format(hashtag_body.lower())\n",
    "    else:\n",
    "        result = \" \".join([\"<hashtag>\"] + re.split(r\"(?=[A-Z])\", hashtag_body, flags=FLAGS))\n",
    "    return result\n",
    "\n",
    "def allcaps(text):\n",
    "    text = text.group()\n",
    "    return text.lower() + \" <allcaps>\"\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    # Different regex parts for smiley faces\n",
    "    eyes = r\"[8:=;]\"\n",
    "    nose = r\"['`\\-]?\"\n",
    "\n",
    "    # function so code less repetitive\n",
    "    def re_sub(pattern, repl):\n",
    "        return re.sub(pattern, repl, text, flags=FLAGS)\n",
    "\n",
    "    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<url>\")\n",
    "    text = re_sub(r\"@\\w+\", \"<user>\")\n",
    "    text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \"<smile>\")\n",
    "    text = re_sub(r\"{}{}p+\".format(eyes, nose), \"<lolface>\")\n",
    "    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \"<sadface>\")\n",
    "    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \"<neutralface>\")\n",
    "    text = re_sub(r\"/\",\" / \")\n",
    "    text = re_sub(r\"<3\",\"<heart>\")\n",
    "    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<number>\")\n",
    "   # text = re_sub(r\"#\\S+\", hashtag)\n",
    "    text = re_sub(r\"([!?.]){2,}\", r\"\\1 <repeat>\")\n",
    "    text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong>\")\n",
    "    text = re_sub(r\"([A-Z]){2,}\", allcaps)\n",
    "\n",
    "       \n",
    "    output = text.lower().split()\n",
    "    output = list(itertools.chain(*[re.split(r'([^\\w<>])', x) for x in output]))  #Splits punctuation, keeping < and >\n",
    "    return [item for item in output if item != '']  #Removes blank strings from list\n",
    "\n",
    "teststring = \"My name is Abhishek. I have no clue. Learning the back-portion that I never cared for. Obama's nephew. @random\"\n",
    "tokenize(teststring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My</td>\n",
       "      <td>D</td>\n",
       "      <td>0.9984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>name</td>\n",
       "      <td>N</td>\n",
       "      <td>0.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is</td>\n",
       "      <td>V</td>\n",
       "      <td>0.9973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abhishek</td>\n",
       "      <td>^</td>\n",
       "      <td>0.9628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.</td>\n",
       "      <td>,</td>\n",
       "      <td>0.9975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I</td>\n",
       "      <td>O</td>\n",
       "      <td>0.9980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>have</td>\n",
       "      <td>V</td>\n",
       "      <td>0.9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>no</td>\n",
       "      <td>D</td>\n",
       "      <td>0.9911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>clue</td>\n",
       "      <td>N</td>\n",
       "      <td>0.9998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>.</td>\n",
       "      <td>,</td>\n",
       "      <td>0.9985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Learning</td>\n",
       "      <td>V</td>\n",
       "      <td>0.9957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>the</td>\n",
       "      <td>D</td>\n",
       "      <td>0.9960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>back-portion</td>\n",
       "      <td>N</td>\n",
       "      <td>0.8394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>that</td>\n",
       "      <td>P</td>\n",
       "      <td>0.9530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I</td>\n",
       "      <td>O</td>\n",
       "      <td>0.9989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>never</td>\n",
       "      <td>R</td>\n",
       "      <td>0.9922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>cared</td>\n",
       "      <td>V</td>\n",
       "      <td>0.9976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>for</td>\n",
       "      <td>P</td>\n",
       "      <td>0.9806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>.</td>\n",
       "      <td>,</td>\n",
       "      <td>0.9916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Obama's</td>\n",
       "      <td>Z</td>\n",
       "      <td>0.8890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>nephew</td>\n",
       "      <td>N</td>\n",
       "      <td>0.9582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>.</td>\n",
       "      <td>,</td>\n",
       "      <td>0.9976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>@random</td>\n",
       "      <td>@</td>\n",
       "      <td>0.9960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word tag confidence\n",
       "0             My   D     0.9984\n",
       "1           name   N     0.9996\n",
       "2             is   V     0.9973\n",
       "3       Abhishek   ^     0.9628\n",
       "4              .   ,     0.9975\n",
       "5              I   O     0.9980\n",
       "6           have   V     0.9999\n",
       "7             no   D     0.9911\n",
       "8           clue   N     0.9998\n",
       "9              .   ,     0.9985\n",
       "10      Learning   V     0.9957\n",
       "11           the   D     0.9960\n",
       "12  back-portion   N     0.8394\n",
       "13          that   P     0.9530\n",
       "14             I   O     0.9989\n",
       "15         never   R     0.9922\n",
       "16         cared   V     0.9976\n",
       "17           for   P     0.9806\n",
       "18             .   ,     0.9916\n",
       "19       Obama's   Z     0.8890\n",
       "20        nephew   N     0.9582\n",
       "21             .   ,     0.9976\n",
       "22       @random   @     0.9960"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''tokenizer, and part-of-speech tagger from Carnegie Mellon\n",
    "created by Olutobi Owoputi, Brendan O'Connor, Kevin Gimpel, Nathan Schneider, Chris Dyer, Dipanjan Das, Daniel Mills, \n",
    "Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah Smith'''\n",
    "'''RunTagger [options] [ExamplesFilename]\n",
    "  runs the CMU ARK Twitter tagger on tweets from ExamplesFilename, \n",
    "  writing taggings to standard output. Listens on stdin if no input filename.\n",
    "\n",
    "Options:\n",
    "  --model <Filename>        Specify model filename. (Else use built-in.)\n",
    "  --just-tokenize           Only run the tokenizer; no POS tags.\n",
    "  --quiet                   Quiet: no output\n",
    "  --input-format <Format>   Default: auto\n",
    "                            Options: json, text, conll\n",
    "  --output-format <Format>  Default: automatically decide from input format.\n",
    "                            Options: pretsv, conll\n",
    "  --input-field NUM         Default: 1\n",
    "                            Which tab-separated field contains the input\n",
    "                            (1-indexed, like unix 'cut')\n",
    "                            Only for {json, text} input formats.\n",
    "  --word-clusters <File>    Alternate word clusters file (see FeatureExtractor)\n",
    "  --no-confidence           Don't output confidence probabilities\n",
    "  --decoder <Decoder>       Change the decoding algorithm (default: greedy)\n",
    "\n",
    "Tweet-per-line input formats:\n",
    "   json: Every input line has a JSON object containing the tweet,\n",
    "         as per the Streaming API. (The 'text' field is used.)\n",
    "   text: Every input line has the text for one tweet.\n",
    "We actually assume input lines are TSV and the tweet data is one field.\n",
    "(Therefore tab characters are not allowed in tweets.\n",
    "Twitter's own JSON formats guarantee this;\n",
    "if you extract the text yourself, you must remove tabs and newlines.)\n",
    "Tweet-per-line output format is\n",
    "   pretsv: Prepend the tokenization and tagging as new TSV fields, \n",
    "           so the output includes a complete copy of the input.\n",
    "By default, three TSV fields are prepended:\n",
    "   Tokenization \\t POSTags \\t Confidences \\t (original data...)\n",
    "The tokenization and tags are parallel space-separated lists.\n",
    "The 'conll' format is token-per-line, blank spaces separating tweets.'''\n",
    "\n",
    "file = open(\"teststring.txt\", \"w\") \n",
    "file.write(teststring) \n",
    "file.close() \n",
    "\n",
    "#! ./ark-tweet-nlp-0.3.2/runTagger.sh ./ark-tweet-nlp-0.3.2/examples/example_tweets.txt\n",
    "#! ./ark-tweet-nlp-0.3.2/twokenize.sh --output-format pretsv ./ark-tweet-nlp-0.3.2/examples/casual.txt\n",
    "test1 = ! ./ark-tweet-nlp-0.3.2/runTagger.sh --output-format conll teststring.txt\n",
    "test1_list = list([re.split(r'([\\t])',x) for x in test1])\n",
    "test1_list = [[ item for item in word if item != '\\t' ] for word in test1_list]\n",
    "pd_test = pd.DataFrame(test1_list[1:-2], columns = ['word','tag','confidence'] )\n",
    "pd_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>target</th>\n",
       "      <th>text_tokcan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44893</th>\n",
       "      <td>U.N. political affairs chief to visit North Ko...</td>\n",
       "      <td>UNITED NATIONS (Reuters) - The United Nations ...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>December 4, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>[united, &lt;allcaps&gt;, nations, &lt;allcaps&gt;, (, reu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44894</th>\n",
       "      <td>NAILS IT! MIKE ROWE On Why Trump Won…Hillary S...</td>\n",
       "      <td>This is fantastic! Mike Rowe tells a fan why T...</td>\n",
       "      <td>left-news</td>\n",
       "      <td>Nov 11, 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>[this, is, fantastic, !, mike, rowe, tells, a,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44895</th>\n",
       "      <td>Coalition of 13 states to challenge Trump on v...</td>\n",
       "      <td>WASHINGTON (Reuters) - New York State’s attorn...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>June 9, 2017</td>\n",
       "      <td>1</td>\n",
       "      <td>[washington, &lt;allcaps&gt;, (, reuters, ), -, new,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44896</th>\n",
       "      <td>VERY FUNNY VIDEO: SARAH PALIN ADVISES TRUMP TO...</td>\n",
       "      <td>Just released hysterical video of  Sarah Palin...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Aug 14, 2015</td>\n",
       "      <td>0</td>\n",
       "      <td>[just, released, hysterical, video, of, sarah,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44897</th>\n",
       "      <td>Indicted Texas mayor arrested for disrupting m...</td>\n",
       "      <td>SAN ANTONIO (Reuters) - The mayor of the south...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>February 17, 2016</td>\n",
       "      <td>1</td>\n",
       "      <td>[san, &lt;allcaps&gt;, antonio, &lt;allcaps&gt;, (, reuter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "44893  U.N. political affairs chief to visit North Ko...   \n",
       "44894  NAILS IT! MIKE ROWE On Why Trump Won…Hillary S...   \n",
       "44895  Coalition of 13 states to challenge Trump on v...   \n",
       "44896  VERY FUNNY VIDEO: SARAH PALIN ADVISES TRUMP TO...   \n",
       "44897  Indicted Texas mayor arrested for disrupting m...   \n",
       "\n",
       "                                                    text       subject  \\\n",
       "44893  UNITED NATIONS (Reuters) - The United Nations ...     worldnews   \n",
       "44894  This is fantastic! Mike Rowe tells a fan why T...     left-news   \n",
       "44895  WASHINGTON (Reuters) - New York State’s attorn...  politicsNews   \n",
       "44896  Just released hysterical video of  Sarah Palin...      politics   \n",
       "44897  SAN ANTONIO (Reuters) - The mayor of the south...  politicsNews   \n",
       "\n",
       "                     date target  \\\n",
       "44893   December 4, 2017       1   \n",
       "44894        Nov 11, 2016      0   \n",
       "44895       June 9, 2017       1   \n",
       "44896        Aug 14, 2015      0   \n",
       "44897  February 17, 2016       1   \n",
       "\n",
       "                                             text_tokcan  \n",
       "44893  [united, <allcaps>, nations, <allcaps>, (, reu...  \n",
       "44894  [this, is, fantastic, !, mike, rowe, tells, a,...  \n",
       "44895  [washington, <allcaps>, (, reuters, ), -, new,...  \n",
       "44896  [just, released, hysterical, video, of, sarah,...  \n",
       "44897  [san, <allcaps>, antonio, <allcaps>, (, reuter...  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make new column with tokenized, canonicalized text\n",
    "all_data['text_tokcan'] = all_data['text'].apply(tokenize)\n",
    "all_data.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: 26 types\n",
      "26 words\n",
      "wordset:  ['<s>', '</s>', '<unk>', '.', 'i', 'my', 'name', 'is', 'abhishek', 'have', 'no', 'clue', 'learning', 'the', 'back', '-', 'portion', 'that', 'never', 'cared', 'for', 'obama', \"'\", 's', 'nephew', '<user>']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_vocab(corpus, V=None, **kw):\n",
    "    if isinstance(corpus, list):\n",
    "        token_feed = (utils.canonicalize_word(w) for w in corpus)\n",
    "        vocab = vocabulary.Vocabulary(token_feed, size=V, **kw)\n",
    "    print(\"Vocabulary: {:,} types\".format(vocab.size))\n",
    "    return vocab\n",
    "\n",
    "\n",
    "#utils.canonicalize_word(teststring.split())\n",
    "vocab=build_vocab(tokenize(teststring))\n",
    "print(\"{:,} words\".format(vocab.size))\n",
    "print(\"wordset: \",vocab.ordered_words())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Dev/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split fractions add up to 1.0\n",
      "training set:  (31428, 5)\n",
      "dev set:  (6735, 5)\n",
      "test set:  (6735, 5)\n"
     ]
    }
   ],
   "source": [
    "#train/dev/train split\n",
    "#train_dev_split = 0.8\n",
    "\n",
    "train_fract = 0.70\n",
    "dev_fract = 0.15\n",
    "test_fract = 0.15\n",
    "\n",
    "if (train_fract+dev_fract+test_fract) == 1.0:\n",
    "    print('Split fractions add up to 1.0')\n",
    "else:\n",
    "    print('SPLIT FRACTIONS DO NOT ADD UP TO 1.0; PLEASE TRY AGAIN.............')\n",
    "\n",
    "#train_data = all_data[:int(len(all_data)*train_dev_split)].reset_index(drop=True)\n",
    "#dev_data = all_data[int(len(all_data)*train_dev_split):].reset_index(drop=True)\n",
    "\n",
    "train_set = all_data[ :int(len(all_data)*train_fract)].reset_index(drop=True)\n",
    "dev_set = all_data[int(len(all_data)*(train_fract)) : int(len(all_data)*(train_fract+dev_fract))].reset_index(drop=True)\n",
    "test_set = all_data[int(len(all_data)*(train_fract+dev_fract)) : ].reset_index(drop=True)\n",
    "\n",
    "print('training set: ',train_set.shape)\n",
    "print('dev set: ',dev_set.shape)\n",
    "print('test set: ',test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRUMP WINS! Supreme Court Rules On Travel Ban ...</td>\n",
       "      <td>This is gonna be a tough pill for the left to ...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Jun 26, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. State Department says 'very concerned' ab...</td>\n",
       "      <td>WASHINGTON (Reuters) - The U.S. State Departme...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>October 16, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Head of Germany's FDP offers Macron 'bitterswe...</td>\n",
       "      <td>BERLIN (Reuters) - A leading candidate to be G...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>October 19, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pro-Damascus alliance declares Syria offensive...</td>\n",
       "      <td>BEIRUT (Reuters) - A military alliance fightin...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>September 16, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump’s Labor Pick Belonged To Group That FOR...</td>\n",
       "      <td>Andrew Puzder, Trump s nominee for Secretary o...</td>\n",
       "      <td>News</td>\n",
       "      <td>January 25, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  TRUMP WINS! Supreme Court Rules On Travel Ban ...   \n",
       "1  U.S. State Department says 'very concerned' ab...   \n",
       "2  Head of Germany's FDP offers Macron 'bitterswe...   \n",
       "3  Pro-Damascus alliance declares Syria offensive...   \n",
       "4   Trump’s Labor Pick Belonged To Group That FOR...   \n",
       "\n",
       "                                                text    subject  \\\n",
       "0  This is gonna be a tough pill for the left to ...   politics   \n",
       "1  WASHINGTON (Reuters) - The U.S. State Departme...  worldnews   \n",
       "2  BERLIN (Reuters) - A leading candidate to be G...  worldnews   \n",
       "3  BEIRUT (Reuters) - A military alliance fightin...  worldnews   \n",
       "4  Andrew Puzder, Trump s nominee for Secretary o...       News   \n",
       "\n",
       "                  date target  \n",
       "0         Jun 26, 2017      0  \n",
       "1    October 16, 2017       1  \n",
       "2    October 19, 2017       1  \n",
       "3  September 16, 2017       1  \n",
       "4     January 25, 2017      0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sandbox\n",
    "\n",
    "delete eveything below when notebook complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NEW DELHI (Reuters) - Donald Trump sympathizes with India in its recent escalation of tensions with Pakistan and supports skilled immigration, an adviser said on Friday, portraying the U.S. presidential hopeful as a friend of India and Indian Americans. Trump, a real estate billionaire, has earned a reputation of hostility toward minorities with proposals such as “extreme vetting” of potential immigrants and building a wall along the Mexican border to stop illegal immigration.  The Republican nominee has proposed a ban on immigration from countries where vetting would be difficult, such as nations faced with Islamic militancy. Some Indian officials worry the United States could become more isolationist under Trump, leaving allies like New Delhi without the support it has enjoyed under President Barack Obama against China’s growing regional influence.      Shalabh Kumar, a Chicago-based businessman of Indian origin tasked by the Trump campaign with reaching out to Asian-Americans, said India and the Indian-American community had nothing to fear from a Trump presidency.    There may be only 4 million Indian Americans but the potential to develop the relationship with their country of origin, a land of 1.3 billion, was huge, Kumar told a news conference at a hotel in New Delhi that drew a curious audience of about 30 journalists and several TV cameras. “Our overarching goal is for the 21st century to be an Indian-American century - instead of a Sino-American century,” said Kumar, wearing a dark suit and gold-rimmed pilot glasses, his white hair swept back.     “Mr Trump is a businessman - he has no bone in his body, not a drop of blood that is anti-immigrant,” Kumar said. “He wants to have legal, skills-based immigration.” The Punjabi-born businessman, who emigrated in 1969 and was converted to the Republican cause by Ronald Reagan, also said Trump’s determination to keep Islamic militants out of the United States played well with his community.  Around four in five Indians profess the Hindu faith, while 14 percent are Muslims. Hindu nationalist groups, including many Indian Americans, have applauded Trump’s anti-Muslim comments. “There is a war declared on the free world by Islamic terrorists,” Kumar said. “We are very happy that India under Prime Minister Narendra Modi has taken a very firm stand.” Kumar said the Republican Hindu Coalition, a group that he set up last year, would hold a cultural event and charity fundraiser in New Jersey on Oct. 15 at which Trump will deliver a keynote address. Half of the gate receipts would go to victims of terrorism, in particular Hindu refugees from Kashmir and Bangladesh. '"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))\n",
    "#df2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'))\n",
    "\n",
    "\n",
    "\n",
    "all_data.iloc[1]['text']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Detected',\n",
       " ' ',\n",
       " 'text',\n",
       " ' ',\n",
       " 'input',\n",
       " ' ',\n",
       " 'format',\n",
       " 'My',\n",
       " '\\t',\n",
       " 'D',\n",
       " '\\t',\n",
       " '0',\n",
       " '.',\n",
       " '9984',\n",
       " 'name',\n",
       " '\\t',\n",
       " 'N',\n",
       " '\\t',\n",
       " '0',\n",
       " '.',\n",
       " '9996',\n",
       " 'is',\n",
       " '\\t',\n",
       " 'V',\n",
       " '\\t',\n",
       " '0',\n",
       " '.',\n",
       " '9973',\n",
       " 'Abhishek',\n",
       " '\\t',\n",
       " '',\n",
       " '^',\n",
       " '',\n",
       " '\\t',\n",
       " '0',\n",
       " '.',\n",
       " '9628',\n",
       " '',\n",
       " '.',\n",
       " '',\n",
       " '\\t',\n",
       " '',\n",
       " ',',\n",
       " '',\n",
       " '\\t',\n",
       " '0',\n",
       " '.',\n",
       " '9975',\n",
       " 'I',\n",
       " '\\t',\n",
       " 'O',\n",
       " '\\t',\n",
       " '0',\n",
       " '.',\n",
       " '9980',\n",
       " 'have',\n",
       " '\\t',\n",
       " 'V',\n",
       " '\\t',\n",
       " '0',\n",
       " '.',\n",
       " '9999',\n",
       " 'no',\n",
       " '\\t',\n",
       " 'D',\n",
       " '\\t',\n",
       " '0',\n",
       " '.',\n",
       " '9911',\n",
       " 'clue',\n",
       " '\\t',\n",
       " 'N',\n",
       " '\\t',\n",
       " '0',\n",
       " '.',\n",
       " '9998',\n",
       " '',\n",
       " '.',\n",
       " '',\n",
       " '\\t',\n",
       " '',\n",
       " ',',\n",
       " '',\n",
       " '\\t',\n",
       " '0',\n",
       " '.',\n",
       " '9985',\n",
       " 'Learning',\n",
       " '\\t',\n",
       " 'V',\n",
       " '\\t',\n",
       " '0',\n",
       " '.',\n",
       " '9957',\n",
       " 'the',\n",
       " '\\t',\n",
       " 'D',\n",
       " '\\t',\n",
       " '0',\n",
       " '.',\n",
       " '9960',\n",
       " 'back',\n",
       " '-',\n",
       " 'portion',\n",
       " '\\t',\n",
       " 'N',\n",
       " '\\t',\n",
       " '0',\n",
       " '.',\n",
       " '8394',\n",
       " 'that',\n",
       " '\\t',\n",
       " 'P',\n",
       " '\\t',\n",
       " '0',\n",
       " '.',\n",
       " '9530',\n",
       " 'I',\n",
       " '\\t',\n",
       " 'O',\n",
       " '\\t',\n",
       " '0',\n",
       " '.',\n",
       " '9989',\n",
       " 'never',\n",
       " '\\t',\n",
       " 'R',\n",
       " '\\t',\n",
       " '0',\n",
       " '.',\n",
       " '9922',\n",
       " 'cared',\n",
       " '\\t',\n",
       " 'V',\n",
       " '\\t',\n",
       " '0',\n",
       " '.',\n",
       " '9976',\n",
       " 'for',\n",
       " '\\t',\n",
       " 'P',\n",
       " '\\t',\n",
       " '0',\n",
       " '.',\n",
       " '9806',\n",
       " '',\n",
       " '.',\n",
       " '',\n",
       " '\\t',\n",
       " '',\n",
       " ',',\n",
       " '',\n",
       " '\\t',\n",
       " '0',\n",
       " '.',\n",
       " '9916',\n",
       " 'Obama',\n",
       " \"'\",\n",
       " 's',\n",
       " '\\t',\n",
       " 'Z',\n",
       " '\\t',\n",
       " '0',\n",
       " '.',\n",
       " '8890',\n",
       " 'nephew',\n",
       " '\\t',\n",
       " 'N',\n",
       " '\\t',\n",
       " '0',\n",
       " '.',\n",
       " '9582',\n",
       " '',\n",
       " '.',\n",
       " '',\n",
       " '\\t',\n",
       " '',\n",
       " ',',\n",
       " '',\n",
       " '\\t',\n",
       " '0',\n",
       " '.',\n",
       " '9976',\n",
       " '',\n",
       " '@',\n",
       " 'random',\n",
       " '\\t',\n",
       " '',\n",
       " '@',\n",
       " '',\n",
       " '\\t',\n",
       " '0',\n",
       " '.',\n",
       " '9960',\n",
       " '',\n",
       " 'Tokenized',\n",
       " ' ',\n",
       " 'and',\n",
       " ' ',\n",
       " 'tagged',\n",
       " ' ',\n",
       " '1',\n",
       " ' ',\n",
       " 'tweets',\n",
       " ' ',\n",
       " '',\n",
       " '(',\n",
       " '23',\n",
       " ' ',\n",
       " 'tokens',\n",
       " ')',\n",
       " '',\n",
       " ' ',\n",
       " 'in',\n",
       " ' ',\n",
       " '0',\n",
       " '.',\n",
       " '7',\n",
       " ' ',\n",
       " 'seconds',\n",
       " ':',\n",
       " '',\n",
       " ' ',\n",
       " '1',\n",
       " '.',\n",
       " '5',\n",
       " ' ',\n",
       " 'tweets',\n",
       " '/',\n",
       " 'sec',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " '34',\n",
       " '.',\n",
       " '5',\n",
       " ' ',\n",
       " 'tokens',\n",
       " '/',\n",
       " 'sec']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#re.split(r'([^\\w<>])', teststring)\n",
    "list(itertools.chain(*[re.split(r'([^\\w<>])', x) for x in test1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['My', 'D', '0.9984'],\n",
       " ['name', 'N', '0.9996'],\n",
       " ['is', 'V', '0.9973'],\n",
       " ['Abhishek', '^', '0.9628'],\n",
       " ['.', ',', '0.9975'],\n",
       " ['I', 'O', '0.9980'],\n",
       " ['have', 'V', '0.9999'],\n",
       " ['no', 'D', '0.9911'],\n",
       " ['clue', 'N', '0.9998'],\n",
       " ['.', ',', '0.9985'],\n",
       " ['Learning', 'V', '0.9957'],\n",
       " ['the', 'D', '0.9960'],\n",
       " ['back-portion', 'N', '0.8394'],\n",
       " ['that', 'P', '0.9530'],\n",
       " ['I', 'O', '0.9989'],\n",
       " ['never', 'R', '0.9922'],\n",
       " ['cared', 'V', '0.9976'],\n",
       " ['for', 'P', '0.9806'],\n",
       " ['.', ',', '0.9916'],\n",
       " [\"Obama's\", 'Z', '0.8890'],\n",
       " ['nephew', 'N', '0.9582'],\n",
       " ['.', ',', '0.9976'],\n",
       " ['@random', '@', '0.9960']]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1_list = list([re.split(r'([\\t])',x) for x in test1])\n",
    "test1_list = [[ item for item in word if item != '\\t' ] for word in test1_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D',\n",
       " 'N',\n",
       " 'V',\n",
       " '^',\n",
       " ',',\n",
       " 'O',\n",
       " 'V',\n",
       " 'D',\n",
       " 'N',\n",
       " ',',\n",
       " 'V',\n",
       " 'D',\n",
       " 'N',\n",
       " 'P',\n",
       " 'O',\n",
       " 'R',\n",
       " 'V',\n",
       " 'P',\n",
       " ',',\n",
       " 'Z',\n",
       " 'N',\n",
       " ',',\n",
       " '@']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pdtest2 = pd.DataFrame(test1_list[1:-2], columns = ['word','tag','confidence'] )\n",
    "pd_test['tag'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
