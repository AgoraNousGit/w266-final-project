{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIAR DETECTION GROUP PROJECT\n",
    "\n",
    "Using char_wb to do character n-grams only within words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "from functools import reduce\n",
    "\n",
    "#import unittest\n",
    "from IPython.display import display, HTML\n",
    "#from sklearn.utils import shuffle\n",
    "# NLTK for NLP utils and corpora\n",
    "#import nltk\n",
    "from collections import defaultdict\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import tensorflow as tf\n",
    "import pickle\n",
    "import dill\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary, tf_embed_viz\n",
    "import timeit  #For timing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from project_files import pdio # for saving and loading dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "Loading data from Pre-Processing step Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23258 entries, 0 to 23257\n",
      "Data columns (total 6 columns):\n",
      "target            23258 non-null int64\n",
      "title             23258 non-null object\n",
      "title_tokcan      23258 non-null object\n",
      "title_POS         23258 non-null object\n",
      "binary_target     23258 non-null int64\n",
      "embedded_title    23258 non-null object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 19.1 MB\n"
     ]
    }
   ],
   "source": [
    "#Read pkl file from part 1\n",
    "# all_data = pd.read_pickle('parsed_data/df_alldata1.pkl')\n",
    "all_data = pd.read_pickle('parsed_data/df_liarpolitifact_data_embed.pkl')\n",
    "all_data.info(memory_usage='deep', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>title</th>\n",
       "      <th>title_tokcan</th>\n",
       "      <th>title_POS</th>\n",
       "      <th>binary_target</th>\n",
       "      <th>embedded_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Says 31 percent of Texas physicians accept all...</td>\n",
       "      <td>[says, &lt;number&gt;, percent, of, texas, physician...</td>\n",
       "      <td>[V, $, N, P, ^, N, V, D, A, ^, N, ,, R, P, $, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.11797, 0.21126, 0.29075, -0.021211, 0.7819...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>''Both Democrats and Republicans are advocatin...</td>\n",
       "      <td>['', both, democrats, and, republicans, are, a...</td>\n",
       "      <td>[,, D, N, &amp;, N, V, V, P, D, N, P, N, N, V, P, ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[[0.0028594, 0.19457, -0.19449, -0.037583, 0.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>A Republican-led softening of firearms trainin...</td>\n",
       "      <td>[a, republican-led, softening, of, firearms, t...</td>\n",
       "      <td>[D, A, N, P, N, N, N, V, D, A, N, V, V, V, P, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.21705, 0.46515, -0.46757, 0.10082, 1.0135,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>The first tweet was sent from Austin.</td>\n",
       "      <td>[the, first, tweet, was, sent, from, austin, .]</td>\n",
       "      <td>[D, A, N, V, V, P, ^, ,]</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Georgia has the countrys second highest number...</td>\n",
       "      <td>[georgia, has, the, countrys, second, highest,...</td>\n",
       "      <td>[^, V, D, N, A, A, N, P, A, N, N, N, ,]</td>\n",
       "      <td>-1</td>\n",
       "      <td>[[-1.3427, 0.4592, 0.19281, 0.71305, -0.5934, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                              title  \\\n",
       "0       1  Says 31 percent of Texas physicians accept all...   \n",
       "1       2  ''Both Democrats and Republicans are advocatin...   \n",
       "2       0  A Republican-led softening of firearms trainin...   \n",
       "3       5              The first tweet was sent from Austin.   \n",
       "4       2  Georgia has the countrys second highest number...   \n",
       "\n",
       "                                        title_tokcan  \\\n",
       "0  [says, <number>, percent, of, texas, physician...   \n",
       "1  ['', both, democrats, and, republicans, are, a...   \n",
       "2  [a, republican-led, softening, of, firearms, t...   \n",
       "3    [the, first, tweet, was, sent, from, austin, .]   \n",
       "4  [georgia, has, the, countrys, second, highest,...   \n",
       "\n",
       "                                           title_POS  binary_target  \\\n",
       "0  [V, $, N, P, ^, N, V, D, A, ^, N, ,, R, P, $, ...              1   \n",
       "1  [,, D, N, &, N, V, V, P, D, N, P, N, N, V, P, ...             -1   \n",
       "2  [D, A, N, P, N, N, N, V, D, A, N, V, V, V, P, ...              1   \n",
       "3                           [D, A, N, V, V, P, ^, ,]              0   \n",
       "4            [^, V, D, N, A, A, N, P, A, N, N, N, ,]             -1   \n",
       "\n",
       "                                      embedded_title  \n",
       "0  [[0.11797, 0.21126, 0.29075, -0.021211, 0.7819...  \n",
       "1  [[0.0028594, 0.19457, -0.19449, -0.037583, 0.9...  \n",
       "2  [[0.21705, 0.46515, -0.46757, 0.10082, 1.0135,...  \n",
       "3  [[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -...  \n",
       "4  [[-1.3427, 0.4592, 0.19281, 0.71305, -0.5934, ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char pre-process\n",
    "Make it easier to select different sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select column to train on \n",
    "# mini_data = all_data[:10]\n",
    "\n",
    "tok_char_n = lambda a, ngram_len: [a[i:i+ngram_len] for i in range(len(a))][:-ngram_len]\n",
    "\n",
    "n_gram_len = 2\n",
    "field_name='title'\n",
    "new_field_name = field_name+'_pad'\n",
    "tok_field_name = field_name+'_tok'\n",
    "tok_int_field_name = field_name+'_tok_int'\n",
    "\n",
    "# find 95% quantile\n",
    "text_lens=[len(i) for i in all_data[field_name]]\n",
    "pad_len = int(np.percentile(text_lens, 95))\n",
    "\n",
    "# truncate/pad new field to 95th percentile\n",
    "all_data[new_field_name] = [(\" \".join(i.lower().split()) + pad_len * ' ')[:pad_len] for i in all_data[field_name]]\n",
    "\n",
    "# tokenize\n",
    "all_data[tok_field_name] = [tok_char_n(i,n_gram_len) for i in all_data[new_field_name]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define helper functions\n",
    "Helper functions for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulled from a StackOverflow post\n",
    "\n",
    "def train_validate_test_split(df, train_percent=.6, validate_percent=.2, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df.index)\n",
    "    train_end = int(train_percent * m)\n",
    "    validate_end = int(validate_percent * m) + train_end\n",
    "    train = df.ix[perm[:train_end]]\n",
    "    validate = df.ix[perm[train_end:validate_end]]\n",
    "    test = df.ix[perm[validate_end:]]\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process character n-grams\n",
    "Create character n-grams for each of the news entries. This gives me the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1601\n"
     ]
    }
   ],
   "source": [
    "# for ngram_len in range(start_ngram,start_ngram):\n",
    "# vec = []\n",
    "# # ngram = []\n",
    "# # start_ngram = 2\n",
    "# # vocab = []\n",
    "# i = ngram_len-2\n",
    "vec=CountVectorizer(analyzer='char_wb', binary=True, ngram_range=(n_gram_len, n_gram_len))\n",
    "ngram = vec.fit_transform(all_data[field_name])\n",
    "vocab = vec.vocabulary_\n",
    "vocab['  '] = len(vocab)\n",
    "vocab['oov'] = len(vocab)\n",
    "default = len(vocab)-1\n",
    "\n",
    "tok = lambda voc, toks: [voc.get(t,default) for t in toks]\n",
    "\n",
    "all_data[tok_int_field_name] = [tok(vocab, j) for j in all_data[tok_field_name]]\n",
    "\n",
    "all_data[tok_int_field_name][:10]\n",
    "print(default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and run Keras LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  if __name__ == '__main__':\n",
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:11: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:  (16280, 190)\n",
      "y_train:  (16280, 1)\n",
      "x_test:  (3488, 190)\n",
      "y_test:  (3488, 1)\n",
      "Epoch 1/2\n",
      "16280/16280 [==============================] - 1s 78us/step - loss: 13.5510 - acc: 0.3562\n",
      "Epoch 2/2\n",
      "16280/16280 [==============================] - 1s 55us/step - loss: 13.5510 - acc: 0.3562\n",
      "3488/3488 [==============================] - 0s 42us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 64)                12224     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 14,337\n",
      "Trainable params: 14,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "all_data_train, all_data_dev, all_data_test = train_validate_test_split(all_data, train_percent = .7, validate_percent=.15)\n",
    "\n",
    "x_train = np.array(list(all_data_train[tok_int_field_name]))\n",
    "print(\"x_train: \", x_train.shape)\n",
    "y_train = np.reshape(list(all_data_train['binary_target']),[-1,1])\n",
    "print(\"y_train: \", y_train.shape)\n",
    "x_test = np.array(list(all_data_dev[tok_int_field_name]))\n",
    "print(\"x_test: \", x_test.shape)\n",
    "y_test = np.reshape(list(all_data_dev['binary_target']),[-1,1])\n",
    "print(\"y_test: \", y_test.shape)\n",
    "\n",
    "max_features = len(vocab)\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(Embedding(max_features, output_dim=256))\n",
    "# model.add(LSTM(128))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=2)\n",
    "score = model.evaluate(x_test, y_test, batch_size=32)\n",
    "\n",
    "\n",
    "# define model\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(75, input_shape=(X.shape[1], X.shape[2])))\n",
    "# model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list(all_data_dev[tok_int_field_name][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3488/3488 [==============================] - 0s 36us/step\n"
     ]
    }
   ],
   "source": [
    "# x_train = np.array(list(all_data_train[tok_int_field_name]))\n",
    "# print(\"x_train: \", x_train.shape)\n",
    "# y_train = np.array(list(all_data_train['target']))\n",
    "# print(\"y_train: \", y_train.shape)\n",
    "# x_test = np.array(list(all_data_dev[tok_int_field_name]))\n",
    "# print(\"x_test: \", x_test.shape)\n",
    "# y_test = np.reshape(list(all_data_dev['target']),[1,-1])\n",
    "# print(\"y_test: \", y_test.shape)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# isot score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13.565653651132497, 0.3540711009174312]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1601"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated output channel (0-based index): All\n",
      "Building gradient functions\n",
      "Progress: 100.0%\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IntegratedGradients.IntegratedGradients import *\n",
    "\n",
    "ig = integrated_gradients(model)\n",
    "ig.explain(x_test[256])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
